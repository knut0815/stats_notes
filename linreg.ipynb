{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Consider the data set $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$, where $x$ is the independent variable and $y$ is the dependent variable. We will model this data set as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "What are the values of $\\alpha$ and $\\beta$ that provide the best fit to the data set? To answer this question, we minimize the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha, \\beta) &= \\frac{1}{2m}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "                 &= \\frac{1}{2m}\\sum_{i=1}^N (\\alpha + \\beta x^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "$J(\\alpha, \\beta)$ is called the *cost function*, or *objective function*. Let's minimize the cost function with respect to $\\alpha$ and $\\beta$. We will denote the optimal values by $\\hat{\\beta}$ and $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial\\alpha} = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)}) = 0 \\\\\n",
    "\\frac{\\partial J}{\\partial\\beta}  = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)})x^{(i)} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)} ,\\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y^{(i)} ,\\qquad\n",
    "S_x^2   = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 ,\\qquad\n",
    "C_{xy}  = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "These quantities should be familiar: $\\bar{x}$ is the sample mean of $x$, $\\bar{y}$ is the sample mean of $y$, $S_x^2$ is the (biased) sample variance of $x$, and $C_{xy}$ is the (biased) sample covariance of $x$ and $y$. Using these definitions, it can be shown after straightforward algebra that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{C_{xy}}{S_x^2}, \\qquad\n",
    "\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data using the model $f(x)=0.3 + 2x$ and Gaussian noise with $\\sigma=0.5$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105f2c610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x = np.linspace(0, 1, m)\n",
    "y = np.random.normal(loc=2.0*x+0.3, scale=0.5, size=m)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run regression using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   106.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 24 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>2.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:50:47</td>     <th>  Log-Likelihood:    </th> <td> -84.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   173.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   178.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3065</td> <td>    0.113</td> <td>    2.710</td> <td> 0.008</td> <td>    0.082</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0140</td> <td>    0.195</td> <td>   10.306</td> <td> 0.000</td> <td>    1.626</td> <td>    2.402</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.753</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.252</td> <th>  Jarque-Bera (JB):  </th> <td>   1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.035</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    4.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.520\n",
       "Model:                            OLS   Adj. R-squared:                  0.515\n",
       "Method:                 Least Squares   F-statistic:                     106.2\n",
       "Date:                Sun, 24 Feb 2019   Prob (F-statistic):           2.63e-17\n",
       "Time:                        17:50:47   Log-Likelihood:                -84.642\n",
       "No. Observations:                 100   AIC:                             173.3\n",
       "Df Residuals:                      98   BIC:                             178.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3065      0.113      2.710      0.008       0.082       0.531\n",
       "x1             2.0140      0.195     10.306      0.000       1.626       2.402\n",
       "==============================================================================\n",
       "Omnibus:                        2.753   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                1.746\n",
       "Skew:                           0.035   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.356   Cond. No.                         4.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(len(x)), x])\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's see if the formulas we derived for $\\hat{\\beta}$ and $\\hat{\\alpha}$ match the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha = 0.306537621742\n",
      "Reproduced  Alpha = 0.306537621742\n",
      "statsmodels Beta  = 2.01403383001\n",
      "Reproduced  Beta  = 2.01403383001\n",
      "Alpha = 0.306537621742\n"
     ]
    }
   ],
   "source": [
    "beta = np.cov(x, y, bias=True)[0, 1] / np.var(x)\n",
    "alpha = np.mean(y) - beta * np.mean(x)\n",
    "print(\"statsmodels Alpha = {}\".format(model.params[0]))\n",
    "print(\"Reproduced  Alpha = {}\".format(alpha))\n",
    "print(\"statsmodels Beta  = {}\".format(model.params[1]))\n",
    "print(\"Reproduced  Beta  = {}\".format(beta))\n",
    "print(\"Alpha = {}\".format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$ are quite close to the actual model parameters $\\beta=2$ and $\\alpha=0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "R-squared (usually denoted $R^2$) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and $f^{(i)} = f(x^{(i)})$. R-squared compares the error in prediction $y^{(i)} - f^{(i)}$ to that of the trivial model $f=\\bar{y}$. The smaller the error of prediction, the closer $R^2$ is to $1$. In that sense, it measures the proportion of variance in the dependent variable $y$ that is predictable from the independent variable $x$ using the model $f$. Note, however, that this is an *in-sample* measure of goodness-of-fit, if the same dataset is used to fit the parameters of the model *and* compute $R^2$.\n",
    "\n",
    "Let's confirm $R^2$ for our linear regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.520089566727\n",
      "Reproduced  R-squared = 0.520089566727\n"
     ]
    }
   ],
   "source": [
    "f = alpha + beta * x\n",
    "e = f - y\n",
    "y_c = y - y.mean()\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y_c, y_c)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model $f$ is linear regression with an intercept term, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! Let's prove this.\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "S_y^2   = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2 ,\\qquad\n",
    "r_{xy}  = \\frac{C_{xy}}{S_xS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where $S_y$ is the (biased) sample variance of $y$ and $r_{xy}$ is the sample correlation coefficient of $x$ and $y$. Using this, we can express $\\hat{\\beta}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{r_{xy}S_y}{S_x}.\n",
    "\\end{equation}\n",
    "\n",
    "Also note that:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} - \\bar{y} = \\hat{\\alpha} + \\hat{\\beta}x^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x}),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$. The average of squared residuals can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - y^{(i)})^2\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m[(f^{(i)} - \\bar{y}) - (y^{(i)} - \\bar{y}) ]^2 \\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})^2 + \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\frac{2}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\frac{\\hat{\\beta}^2}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})^2 + S_y^2 - \\frac{\\hat{\\beta}}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\hat{\\beta}^2 S_x^2 + S_y^2 - 2\\hat{\\beta}C_{xy} \\\\\n",
    "&= S_y^2(1 - r_{xy}^2),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$, and the last one from substituting the value of $\\hat{\\beta}$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2} \\\\\n",
    "    &= 1 - \\frac{S_y^2(1 - r_{xy}^2)}{S_y^2} \\\\\n",
    "    &= r_{xy}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We proved that $R^2$ is given by the square of the sample correlation between $x$ and $y$, and not $f$ and $y$ as originally promised! First, let's numerically check what we just proved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's do the extra work and show that $r_{fy}^2 = r_{xy}^2$.\n",
    "\n",
    "To do this, we'll need one more result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m f^{(i)}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)})\n",
    "    = \\hat{\\alpha} + \\hat{\\beta}\\bar{x}\n",
    "    = \\bar{y},\n",
    "\\end{equation}\n",
    "where the last line follows from $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}$.\n",
    "\n",
    "Now we're ready:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{fy}\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{y})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sqrt{\\hat{\\beta}^2\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}}{|\\hat{\\beta}|}r_{xy},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\bar{f}=\\bar{y}$ and the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$. There we have it:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = r_{fy}^2 = r_{xy}^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "In most realistic cases, we cannot hope to explain all variability in $y$ using only $x$. This is also true for models which are more complex than linear regression. We should always expect a certain degree of randomness that our models cannot account for. However, we can hope to design models that make the correct predictions *on average*. This suggests a probabilistic approach for modelling. We can think about $y$ as a random variable with a certain probability distribution, whose mean is given by the model $f(x)$.\n",
    "\n",
    "For simplicity, let's assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing $(x^{(i)}, y^{(i)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P^{(i)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we're assuming all observations are independent, the probability of observing the whole dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\Pi_{i=1}^m P^{(i)} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\alpha, \\beta)}{\\sigma^2}\\right)},\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is the cost function defined earlier. This is interesting! The probability of observing the data set can be expressed in terms of the cost function. In fact, the parameters $\\alpha$ and $\\beta$ that minimize $J$ also maximize $P$. Picking the parameters of a model to maximize the probability of observing the dataset is called *maximum likelihood estimation*.\n",
    "\n",
    "*Log-likelihood* is given by $\\log P$, which in our case is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\alpha, \\beta) = -\\frac{m}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m(y^{(i)} - \\alpha - \\beta x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the model summary above from `statsmodels`, a number is quoted for Log-Likelihood. To reproduce it, let's compute $\\log P(\\hat{\\alpha}, \\hat{\\beta})$. One thing, though, is that $\\log P$ depends on $\\sigma$, which `statsmodels` doesn't take as input. It can, however, esimate it from the sample standard deviation of the prediction errors $y^{(1)} - f^{(1)}, \\dots, y^{(m)} - f^{(m)}$. Let's give that a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -84.6424357588\n",
      "Reproduced  Log-Likelihood = -84.6424357588\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(len(x)/2.0)*np.log(2*np.pi*np.var(e)) - np.dot(y - f, y - f)/(2*np.var(e))\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "Given the probabilistic view, we now see that the optimal estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$ should be regarded as random variables, and not confused with the \"real\" underlying parameters $\\alpha$ and $\\beta$. A different dataset, for instance, would result in different values for $\\hat{\\alpha}$ and $\\hat{\\beta}$. Therefore, it's important to study their variability.\n",
    "\n",
    "Let's first check their means. Note that by assumption\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "and as a result\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\bar{y} \\rangle = \\frac{1}{m} \\sum_{i=1}^m \\langle y^{(i)} \\rangle = \\frac{1}{m} \\sum_{i=1}^m (\\alpha + \\beta x^{(i)}) = \\alpha + \\beta \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Combining these:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle(y^{(i)} - \\bar{y})\\rangle = \\beta (x^{(i)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these results, we can compute the mean of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta} \\rangle\n",
    "= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})\\langle(y^{(i)} - \\bar{y})\\rangle\n",
    "= \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x}) = \\beta,\n",
    "\\end{equation}\n",
    "\n",
    "and also that of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\alpha} \\rangle = \\langle \\bar{y} \\rangle - \\langle \\hat{\\beta} \\rangle \\bar{x} = \\alpha + \\beta \\bar{x} - \\beta \\bar{x} = \\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "So the mean of $\\hat{\\beta}$ and $\\hat{\\alpha}$ are $\\beta$ and $\\alpha$, respectively. Good, but no terribly surprising!\n",
    "\n",
    "Let's compute the variance of $\\hat{\\beta}$. First, we will express $\\hat{\\beta} - \\beta$ in a convenient way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) - \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})[(y^{(i)} - \\beta x^{(i)} - \\alpha) - (\\bar{y} - \\beta \\bar{x} - \\alpha)] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle) - \\frac{\\bar{y} - \\beta \\bar{x} - \\alpha}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "        &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle)\n",
    "\\end{align*}\n",
    "\n",
    "where the first equality follows from the definition of $S_x^2$, the second from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, and the third from $\\sum_{i=1}^m (x^{(i)} - \\bar{x})=0$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\\\\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\sigma^2\\delta_{ij} \\\\\n",
    "&= \\frac{\\sigma^2}{m^2S_x^4}\\sum_{i}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "&= \\frac{\\sigma^2}{mS_x^2},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from the fact that $y^{(1)}, \\dots, y^{(m)}$ are (by assumption) independent and normally distributed with variance $\\sigma^2$.\n",
    "\n",
    "Let's check this against `statsmodels`. The estimated value of $\\sigma^2$ (since the real value is unknown to `statsmodels`), is stored in the `scale` attribute of `model`. Using that, we can reproduce the standard error of $\\hat{\\beta}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta std err = 0.195431582311\n",
      "Reproduced  Beta std err = 0.195431582311\n"
     ]
    }
   ],
   "source": [
    "beta_se = np.sqrt(model.scale / (m * np.var(x)))\n",
    "print(\"statsmodels Beta std err = {}\".format(model.bse[1]))\n",
    "print(\"Reproduced  Beta std err = {}\".format(beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute $\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle$. First, note that we can express $\\hat{\\alpha} - \\alpha$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha\n",
    "    = \\bar{y} - (\\hat{\\beta} - \\beta)\\bar{x} - \\beta\\bar{x} - \\alpha\n",
    "    = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "To compute the variance of $\\hat{\\alpha} - \\alpha$, we will need the covariance between $\\hat{\\beta} - \\beta$ and $\\bar{y} - \\langle \\bar{y} \\rangle$. We can express $\\bar{y} - \\langle \\bar{y} \\rangle$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\bar{y} - \\beta \\bar{x} - \\alpha = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\beta x^{(j)} - \\alpha) = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\left\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\right\\rangle \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\langle(\\bar{y} - \\langle \\bar{y} \\rangle)^2\\right\\rangle\n",
    "=\\frac{1}{m^2}\\sum_{k,l=1}^m \\left\\langle(y^{(k)} - \\langle y^{(k)} \\rangle)(y^{(l)} - \\langle y^{(l)} \\rangle)\\right\\rangle\n",
    "= \\frac{1}{m^2}\\sum_{k,l=1}^m \\sigma^2\\delta_{kl}\n",
    "= \\frac{\\sigma^2}{m}.\n",
    "\\end{equation}\n",
    "This is exactly what we should've expected without doing any work, since $\\bar{y} - \\langle \\bar{y} \\rangle \\sim \\mathcal{N}(0, \\sigma^2/m)$ follows from the fact that $\\bar{y}$ is an average of independent and identical normally distributed random variables.\n",
    "\n",
    "Now we can compute the variance of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle(\\hat{\\alpha} - \\alpha)^2\\rangle =\n",
    "\\left\\langle (\\bar{y} - \\langle \\bar{y} \\rangle)^2 \\right\\rangle\n",
    "+ \\bar{x}^2\\left\\langle (\\hat{\\beta} - \\beta)^2 \\right\\rangle\n",
    "= \\frac{\\sigma^2}{m} + \\bar{x}^2\\frac{\\sigma^2}{mS_x^2}\n",
    "= \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha std err = 0.113117048297\n",
      "Reproduced  Alpha std err = 0.113117048297\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(model.scale / m * (1 + np.mean(x)**2/np.var(x)))\n",
    "print(\"statsmodels Alpha std err = {}\".format(model.bse[0]))\n",
    "print(\"Reproduced  Alpha std err = {}\".format(alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation above also makes it easy to derive the covariance between $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)(\\hat{\\beta} - \\beta) \\rangle\n",
    "= \\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle - \\bar{x} \\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "= 0 - \\bar{x} \\frac{\\sigma^2}{mS_x^2}\n",
    "= -\\frac{\\sigma^2\\bar{x}}{mS_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Again, we get the same number as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha-beta covariance = -0.0190967516823\n",
      "Reproduced  alpha-beta covariance = -0.0190967516823\n"
     ]
    }
   ],
   "source": [
    "cov_params = - model.scale * np.mean(x)/(m*np.var(x))\n",
    "print(\"statsmodels alpha-beta covariance = {}\".format(model.cov_params()[0, 1]))\n",
    "print(\"Reproduced  alpha-beta covariance = {}\".format(cov_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we glossed over was how `statsmodels` estimates $\\sigma^2$. Let's think about this a little. Since, by assumption, $y{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)$, it makes sense to use the sample variance of the prediction error $e_i=y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)}=y^{(i)} - f^{(i)}$ as an estimate. We can express $e_i$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "e_i &= y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\hat{\\alpha} - \\alpha) - (\\hat{\\beta} - \\beta)x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\sum_{j=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)\\left[\\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}) \\right] \\\\\n",
    "    &= \\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, the third from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$, the fourth from our earlier expressions for $\\bar{y} - \\langle \\bar{y} \\rangle$ and $\\hat{\\beta} - \\beta$, and in the last equality we have introduced the matrix $A$ whose elements are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} \\equiv \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "$A$ has a few interesting properties. First, it's symmetric: $A_{ij} = A_{ij}$. Second, it's equal to its square: $A^2=A$. (This can be shown by carrying out the matrix multiplication between $A$ and itself. It's a bit tedious and not terribly interesting, so I'll leave it out.) Using these results, we can compute the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m} e_i^2\n",
    "    &= \\sum_{i=1}^{m}\\left[\\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle)\\right]^2 \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\left[\\sum_{i=1}^{m}A_{ij}A_{ik}\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle) A_{jk},\n",
    "\\end{align*}\n",
    "\n",
    "where the last equality follows from the fact that $A$ is symmetric and $A^2=A$. Taking the expectation value of both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle\\sum_{i=1}^{m} e_i^2 \\right\\rangle\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\left\\langle(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(A),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{tr}(A)=\\sum_{i=1}^{m}A_{ii}$ denotes the trace of $A$. It's easy to show that $\\text{tr}(A)=m-2$, so the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m-2}\\sum_{i=1}^{m}e_i^2 = \\frac{1}{m-2}\\sum_{i=1}^m(y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.324709077426\n",
      "Reproduced  estimate of sigma^2 = 0.324709077426\n"
     ]
    }
   ],
   "source": [
    "var_est = np.var(e)*m/(m-2)\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(var_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "We've worked out the variance (and covariance) of the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. It would also be nice to have confidence intervals for them, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta_L, \\beta_U)$ contains $\\beta$. We've done this sort of analysis for the sample mean when discussing the [t-distribution](https://github.com/siavashaslanbeigi/stats_notes/blob/master/t.ipynb). Can we do something similar for $\\hat{\\alpha}$ and $\\hat{\\beta}$? Yes, but it's a bit more involved than dealing with the sample mean.\n",
    "\n",
    "\n",
    "Above we showed that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2/(m S_x^2))$, or equivalently $\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma \\sim \\mathcal{N}(0, 1)$. This means $\\beta_L=\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x}$ and $\\beta_U=\\hat{\\beta}+z\\frac{\\sigma}{\\sqrt{m} S_x}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x} \\le \\beta \\le \\hat{\\beta}+z \\frac{\\sigma}{\\sqrt{m} S_x})\n",
    "= P(-z \\le \\frac{\\hat{\\beta} - \\beta}{\\sigma/(\\sqrt{m} S_x)} \\le z )\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\hat{\\sigma}^2 / (m S_x^2)}}\n",
    "    = \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)}{\\sigma}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\sum_{i=1}^{m}e_i^2.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. In what follows, we will show that $V$ has a chi-squared distribution with $m-2$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "We will make use (one of the formulations of) [Cochran's theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem), which states the following: suppose $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an $m$-dimensional multivariate normal random vector, and that $B^{(1)}, \\dots, B^{(k)}$ are symmetric $m \\times m$ matrices which satisfy the following properties:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{k}B^{(i)} = I, \\qquad\n",
    "\\sum_{i=1}^{k}r_i = m,\n",
    "\\end{equation}\n",
    "where $I$ is the $m \\times m$ identity matrix and $r_i$ denotes the rank of $B_i$. Then, the random variable $Q^{(i)}=Y^TB^{(i)}Y$ is distributed as $\\sigma^2 \\chi^2_{r_i}$, where $\\chi^2_{r_i}$ is the chi-squared distribution with $r_i$ degrees of freedom, and $Q^{(i)}$ and $Q^{(j)}$ are independent for all $i\\neq j$.\n",
    "\n",
    "\n",
    "It's not immediately obvious how this theorem will help us arrive at confidence intervals, but it will. First, note that if we define $Y$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i \\equiv y^{(i)} - \\langle y^{(i)} \\rangle.\n",
    "\\end{equation}\n",
    "then $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$. With this definition, we can rewrite the sum of squared errors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}e_i^2 = Y^TAY.\n",
    "\\end{equation}\n",
    "\n",
    "Now this looks like a $Q$ matrix from Cochran's theorem. What is the rank of $A$? Remember that the rank of a matrix is the dimension of the vector space spanned by its column vectors. It can be shown that this is equal to the number of non-zero eigenvalues. Because $A^2=A$, eigenvalues of $A$ are either $1$ or $0$. To see why, let $v$ denote an eigenvector of $A$ with eigenvalue $\\lambda$: $Av=\\lambda v$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda v = Av = A^2v=A(Av)=A(\\lambda v)=\\lambda Av=\\lambda^2v,\n",
    "\\end{equation}\n",
    "\n",
    "which implies $\\lambda^2=\\lambda$, which has the solutions $\\lambda=0$ and $\\lambda=1$. Therefore, for $A$, the number of non-zero eigenvalues is simply the sum of its eigenvalues, which is also its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Rank}(A) = \\text{tr}(A) = m - 2.\n",
    "\\end{equation}\n",
    "Let's summarize what we have so far\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(1)} \\equiv \\sum_{i=1}^{m}e_i^2 = Y^TB^{(1)}Y, \\qquad\n",
    "B^{(1)}_{ij} = \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}), \\qquad\n",
    "r_1 = m - 2.\n",
    "\\end{equation}\n",
    "\n",
    "Next we'll turn our attention to $\\hat{\\beta} - \\beta$. We had previous shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} - \\beta = \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(2)} \\equiv m S_x^2(\\hat{\\beta} - \\beta)^2 = Y^TB^{(2)}Y, \\qquad\n",
    "B^{(2)}_{ij} = \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Again, it can be checked that $B^{(2)}$ is equal to its own square, so its rank is given by its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "r_2 = \\text{Rank}(B^{(2)}) = \\text{tr}(B^{(2)}) = \\sum_{i=1}^{m} B^{(2)}_{ii} = \\frac{1}{m S_x^2}\\sum_{i=1}^{m} (x^{(i)} - \\bar{x})^2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, as we showed in previous section\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(3)} \\equiv m (\\bar{y} - \\langle \\bar{y} \\rangle)^2 = Y^TB^{(3)}Y, \\qquad\n",
    "B^{(3)}_{ij} = \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Since all columns of $B^{(3)}$ are the same, its rank is $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "It's now easy to check that $B^{(1)}$, $B^{(2)}$, and $B^{(3)}$ satisfy the conditions of Cochran's theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(1)}_{ij} + B^{(2)}_{ij} + B^{(3)}_{ij} = \\delta_{ij}, \\qquad\n",
    "r_1 + r_2 + r_3 = m.\n",
    "\\end{equation}\n",
    "\n",
    "Cochran's theorem then implies the following:\n",
    "\n",
    "* $\\sum_{i=1}^{m}e_i^2 \\sim \\mathcal \\sigma^2 \\chi^2_{m-2}$.\n",
    "* $\\sum_{i=1}^{m}e_i^2$, $(\\hat{\\beta} - \\beta)^2$, and $(\\bar{y} - \\langle \\bar{y} \\rangle)^2$ are mutually independent, which implies $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are mutually independent.\n",
    "\n",
    "This in turn implies that $V$ has a $\\chi^2_{m-2}$ distribution and that $Z$ and $V$ are independent. Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}} = \\frac{\\hat{\\beta} - \\beta}{s_{\\hat{\\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "has the t-distribution with $m-2$ degrees of freedom, where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\beta}}^2 = \\frac{\\hat{\\sigma}^2}{m S_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally: $\\beta_L=\\hat{\\beta}-ts_{\\hat{\\beta}}$ and $\\beta_U=\\hat{\\beta}+ts_{\\hat{\\beta}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta 95% confidence interval = (1.62620621534, 2.40186144467)\n",
      "Reproduced  beta 95% confidence interval = (1.62620621534, 2.40186144467)\n"
     ]
    }
   ],
   "source": [
    "t = stats.t.ppf(1 - 0.05/2., m - 2)\n",
    "beta_se = np.sqrt(var_est / (m * np.var(x)))\n",
    "print(\"statsmodels beta 95% confidence interval = ({0}, {1})\".format(model.conf_int()[1, 0], model.conf_int()[1, 1]))\n",
    "print(\"Reproduced  beta 95% confidence interval = ({0}, {1})\".format(beta - t*beta_se, beta + t*beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the confidence interval for $\\hat{\\alpha}$. Earlier we had shown that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} \\sim \\mathcal{N}\\left(\\alpha, \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As before, we divide $\\hat{\\alpha} - \\alpha$ by its standard deviation and replace $\\sigma$ with $\\hat{\\sigma}$:\n",
    "\\begin{equation}\n",
    "T_{\\alpha}\n",
    "    \\equiv \\frac{\\hat{\\alpha} - \\alpha}{s_{\\hat{\\alpha}}}\n",
    "    = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2)}}\n",
    "    = \\frac{\\sqrt{m}(\\hat{\\alpha} - \\alpha)/\\sqrt{\\sigma^2(1+\\bar{x}^2/S_x^2)}}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z_{\\alpha}}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\alpha}} \\equiv \\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2), \\qquad\n",
    "Z_{\\alpha} = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that $Z_{\\alpha} \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi^2_{m-2}$. If $Z_{\\alpha}$ and $V$ are independent, then $T_{\\alpha}$ also has a t-distribution with $m-2$ degrees of freedom. We had shown earlier that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are independent, it follows that $\\hat{\\alpha} - \\alpha$ and $\\sum_{i=1}^{m}e_i^2$, and in turn $Z_{\\alpha}$ and $V$, are independent. As a result, $\\alpha_L=\\hat{\\beta}-ts_{\\hat{\\alpha}}$ and $\\alpha_U=\\hat{\\alpha}+ts_{\\hat{\\alpha}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n",
      "Reproduced  alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(var_est / m * (1 + np.mean(x)**2/np.var(x)))\n",
    "print(\"statsmodels alpha 95% confidence interval = ({0}, {1})\".format(model.conf_int()[0, 0], model.conf_int()[0, 1]))\n",
    "print(\"Reproduced  alpha 95% confidence interval = ({0}, {1})\".format(alpha - t*alpha_se, alpha + t*alpha_se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
