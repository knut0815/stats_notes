{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Consider the data set $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$, where $x$ is the independent variable and $y$ is the dependent variable. We will model this data set as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "What are the values of $\\alpha$ and $\\beta$ that provide the best fit to the data set? To answer this question, we minimize the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha, \\beta) &= \\frac{1}{2m}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "                 &= \\frac{1}{2m}\\sum_{i=1}^N (\\alpha + \\beta x^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "$J(\\alpha, \\beta)$ is called the *cost function*, or *objective function*. Let's minimize the cost function with respect to $\\alpha$ and $\\beta$. We will denote the optimal values by $\\hat{\\beta}$ and $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial\\alpha} = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)}) = 0 \\\\\n",
    "\\frac{\\partial J}{\\partial\\beta}  = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)})x^{(i)} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)} ,\\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y^{(i)} ,\\qquad\n",
    "S_x^2   = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 ,\\qquad\n",
    "C_{xy}  = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "These quantities should be familiar: $\\bar{x}$ is the sample mean of $x$, $\\bar{y}$ is the sample mean of $y$, $S_x^2$ is the (biased) sample variance of $x$, and $C_{xy}$ is the (biased) sample covariance of $x$ and $y$. Using these definitions, it can be shown after straightforward algebra that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{C_{xy}}{S_x^2}, \\qquad\n",
    "\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data using the model $f(x)=0.3 + 2x$ and Gaussian noise with $\\sigma=0.5$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHQhJREFUeJzt3X+wZ3V93/Hni2WV62hZ4m4jXFiXNoQJI5U1d1CHmdaiVsQMSxErdpJqhnRHI220hukmmbHW/sFaprEh2NiNMoKTKlYN2ZbtMEZwSKhQLrIgC6HZoAl7oWEFF+OwEsB3//h+L16+fH+c7z2fc87nnPN6zNzZ74+z3/M53++97+/nvD/v8/koIjAzs345pukGmJlZ/Rz8zcx6yMHfzKyHHPzNzHrIwd/MrIcc/M3MesjB38yshxz8zcx6yMHfzKyHjm26AZNs3rw5tm3b1nQzzMxa5a677vpeRGyZtV22wX/btm0sLy833Qwzs1aR9JdFtnPax8yshxz8zcx6yMHfzKyHHPzNzHrIwd/MrIcc/M3MeijbUk8zs6rdcPcKV970II8cOcpJmxa4/G2nc+H2xaabVQsHfzPrpRvuXuE3vvptjj7zHAArR47yG1/9NkAvvgCc9jGzXrrypgefD/yrjj7zHFfe9GBDLaqXg7+Z9dIjR47O9XjXOPibWS+dtGlhrse7pnTwl3ScpP8j6R5JByT9+zHbvFTS9ZIOSrpD0ray+zUzK+Pyt53OwsYNL3hsYeMGLn/b6Q21qF4pev5PA+dGxGuBs4DzJL1hZJtLge9HxM8AnwQ+kWC/ZmbrduH2Ra646EwWNy0gYHHTAldcdGYvBnshQbVPRATww+HdjcOfGNlsB/Cx4e0vA1dL0vD/mpk14sLti70J9qOS5PwlbZC0H3gM+FpE3DGyySLwMEBEPAs8Cbwyxb7NzGx+SYJ/RDwXEWcBJwNnS3rNel5H0k5Jy5KWDx8+nKJpZmY2RtJqn4g4AtwCnDfy1ApwCoCkY4HjgcfH/P89EbEUEUtbtsxciMbMzNYpRbXPFkmbhrcXgLcCfzay2V7gvcPbFwM3O99vZtacFNM7nAhcK2kDgy+TL0XE/5T0cWA5IvYCnwU+L+kg8ARwSYL9mpnZOqWo9rkX2D7m8Y+uuf0j4F1l92VmZmn4Cl8zsx5y8Dcz6yEHfzOzHvJ8/mbWOn1ehCUVB38za5VcFmFp+xeQg7+Ztcq0RVjqCr7TvoBW25j7l4KDv5m1Sg6LsEz6AvrY3gM8/eyPGz8rKcIDvmbWKnUswnLD3Sucs/tmTt11I+fsvpkb7l55wfOTvmiOHH2mNUtDOvibWatUvQjLakpn5chRgp/03td+Acz7RZPj0pAO/mbWKlUvwlJkYfdJX0AnvGzj2NfMcWlI5/zNrHWqXISlyJjC6r5HB3aBFwwEQ75LQzr4m5mtcdKmBVbGfAGM9t6nfQG52sfMrGUuf9vppXrvbVka0sHfzGyNSSmdNgT0eTj4m5mNaEvvvQwHfzPLVtunUMhZimUcT5F0i6T7JR2Q9GtjtnmTpCcl7R/+fHTca5mZrSpSb2/rl6Ln/yzwkYj4lqRXAHdJ+lpE3D+y3Z9ExC8k2J+Z9UAOc/h0Wemef0Q8GhHfGt7+G+ABwJ+MmZWSwxw+XZY05y9pG4P1fO8Y8/QbJd0DPAL8ekQcSLlvM+uWovX2bZTDWEay6R0kvRz4CvChiPjByNPfAl4dEa8Ffhe4YcJr7JS0LGn58OHDqZpmZi1U9Rw+TcllLCNJ8Je0kUHg/4OI+Oro8xHxg4j44fD2PmCjpM1jttsTEUsRsbRly5YUTTObadYMjtaMqufwaUqRuYPqUDrtI0nAZ4EHIuK3J2zzKuCvIyIknc3gS+fxsvs2K6voqlA5nKb3URfr7XMZy0iR8z8H+CXg25L2Dx/7TWArQER8GrgY+ICkZ4GjwCUREQn2bVZKkYqSXJYNtPHa9sWcy1hG6eAfEX8KaMY2VwNXl92XWWpFemEuOcxXG7+Yy84dlIqv8LVeK9ILy+U0PUdFet1V9szb+MWcy9xBDv7Wa0V6YXWdprctfVGk1111z7ytX8w5jGV4JS/rtSIVJXWUHFZV/ldlJVORqpWqK1vqWM+3q9zzt96b1Qur4zS9ivRFDr3uqnvmueTP28jB36yAqk/TqwiSVefDi6TDqk6Z5ZI/byMHf7MMVBEkc+h119EzrzN/3rZxmWmc8zdLpEx+fdy4ghikatabq686H15kvKRLV+nmMi1DKsr1WqulpaVYXl5uuhlmhYzm12HQw50n0K32KleOHEXA2r/MeV8rVZvsJ87ZffPYs7PFTQvctuvcBlo0nqS7ImJp1nbu+ZslkKKq5cLti9y261wWNy0w2iVbT4VMl3rddZh15tbWstJJnPM3SyBlYEj5WjnUk7dBkcqoXKZlSMU9f7MEUubXXbtevyJnbl2bYtrB3yyBlIGha0GmDYqcbXUtjea0j1kCKevNXbtev6IpnSrSaE2Vj7rax8x6r6nKqCr262ofM7OCmkrpNLmql9M+ZmY0UxnVZPmog79Z5lLlhLs0NUFu1vveNlk+WjrtI+kUSbdIul/SAUm/NmYbSbpK0kFJ90p6Xdn9mvVBqikFujY1QU7KvLdNVnalyPk/C3wkIs4A3gB8UNIZI9u8HTht+LMT+L0E+zXrvFQ54SZzy11X5r1tsnw0xRq+jwKPDm//jaQHgEXg/jWb7QCuGy7afrukTZJOHP5fM5sgVU64a1MT5KTse9vUVdhJc/6StgHbgTtGnloEHl5z/9DwsRcEf0k7GZwZsHXr1pRNM8vCvLnhVDnhsq/j8YLJ2jrtQ7JST0kvB74CfCgifrCe14iIPRGxFBFLW7ZsSdU0syysJzc8LSc8zxTSZXLLHi+Yrq1XZCcJ/pI2Mgj8fxARXx2zyQpwypr7Jw8fM+uN9eSGJ+WEgbkCcpncsscLpmvrtA+lr/CVJOBa4ImI+NCEbd4BXAacD7weuCoizp72ur7C17rm1F03vmiq5lWLc6ZS6pxbflK7BXxn9zuS7svKK3qFb4qc/znALwHflrR/+NhvAlsBIuLTwD4Ggf8g8BTwywn2a9Yqk3LDMP/i6nUO4LY1p23Tpaj2+VMGnYBp2wTwwbL7MmuzcevZrjXP4up1BuQ61uFNxQPTxXluH7OarM0NT1K0517nIGNbctoemJ6PZ/U0a0CKnL17uS/UljV2q1Znzt/M5pQilVL1xUFVf7mkfn1fyDYfB3+zCaoMfrkv2FJkTdvcXt8D0/Nx8G8Jn+LXq+rgt/o6uX6G02r7U7S5itdv08B0Dhz8M7Ya8FeOHEXwfK11FYHIXqjq4Je7qlMoVbx+7mdTuXHwz9Roz3N0WL5PgagJbcsfpz4zrDqFUtXr53Y2lfMZu0s9MzWu5zkq10DUBZOCUI754ypKHKsuJW3rfDjzyL301ME/U0UCe46BqCvaFJzWM/fOrEnhqq7tr/vagXkmwUsl9zmRnPbJ1LSpACDfQNQVbcofz5uiKjqYXXUKpa4UTR2D9+Pknjp08M/UuMqF1UHfeScBs/XJLX88ybz58yYHs5vIgTd1vLmXnjr4Z6pNPU9r1rwljk31SPvWA8+99NTBP2Nt6Xlas+btKDTVIy3bA1/vWUNTxzv6uRy/sBEJPnz9fq686cHGO3MO/mYdME9HoakeaZkeeJmzhiZ74KufS1NnPdO42sesZ0YrbTYtbOS4jcfw4ev3V1oJU6Z8tkzlTA6zkuZY+eOev1kPNdEjLdMDL5u3bzqFmmPlT6o1fK+R9Jik+yY8/yZJT0raP/z5aIr9mlk5dfZIy/TAm7zoLsU1AjleNJiq5/854Grguinb/ElE/EKi/ZlZAnX3SNfbA28qb5/qzCjHyp8kwT8ibpW0LcVrWRo5zylSp3nfh769b7nUos9639dT+pzis0x1jUCOpdt15vzfKOke4BHg1yPiwOgGknYCOwG2bt1aY9O6JcfKgrXqCrDzvg+5v29VyKFHWsUVx9NeE4oH4ZRnRk2PO4yqq9rnW8CrI+K1wO8CN4zbKCL2RMRSRCxt2bKlpqZ1T46VBavqnOxq3vch5/etKl2thJn0mh/be2Cu378cc/Wp1NLzj4gfrLm9T9J/kbQ5Ir5Xx/77JsfKglV1Xmo/7/tQ1fuWeyqp6R5pFe/7pP975OgzL3ps2u9fDmdGVaml5y/pVZI0vH32cL+P17HvPsq5t1L0D72JCosq3rfcp/XNQRXv+7z/d9LvZQ5nRlVJVer5BeCbwOmSDkm6VNL7Jb1/uMnFwH3DnP9VwCURMbo+iSWS83TERf7QUwXMce+Dhq837gulivetj6mkeVXxvk96zRNetnHs9tO+LC7cvshtu87lO7vfwW27zu1E4Id01T7vmfH81QxKQa0G0yoLmk5BFDmNrqLCoshSmFVUZOScgstFFe/7pNcEOpvGmZdy7YAvLS3F8vJy083olNEKCBj84td9GjvrC+jUXTe+aNlKGPTav7P7Heva5zm7bx5b0ri4aYHbdp27rtfMeb9VaroDUVbb2z+LpLsiYmnWdp7eoUdyWZR81gBjFbXnntY3jS6UwzY9wJ0LT+xWoyaWklurzsHWMqrIATc1CN61AUOPYXSHe/41yaHHVKRHnUM7q8gB5zCtbxd4DKM7HPxrkkPKpc7B1rJSB8wcL69vo1ymg7DyHPzHqGJAKIceU5EAmEM7pynz2XSpB96UaR2Irg+kdo2D/4iq0h659JiaGGxNJeVn40C1PkVLKNs4ENw3HvAdUdWAVs4XXq2VcztTfTa+6raccRc9eSC4fdzzH1FV2iNlzrnKXmvOufFUn00u4xpdknu60F7MwX9ElWmPFDnnOqpxcs2Np/psHKjSyzldaOM57TMi57QH9LvOOtVnk/PEd7M0fQ3GJLn/3diLOfiPyP2inD73WlN9Nm0NVDmPVeT+d2Mv5rl9SmiiYqSLc8U0oY3VPn357Nv42eTEc/tUrKkrYV1nnUau4xrT1HHW1/TvUA5XmPeF0z7r1FTufdLpNZBtSsDSqHqsIoe0Up/HtOrmnv86NZl7H9drPWf3za0vX2y615m7qucnyqEEts9jWnVLtZLXNZIek3TfhOcl6SpJByXdK+l1KfbbpNwqRtr+R5NDrzN3VQ+q5vA7lNvfVZelSvt8DjhvyvNvB04b/uwEfi/RfhuTW8VI2/9ofLpfTJVLCubwO5Tb31WXJQn+EXEr8MSUTXYA18XA7cAmSSem2HdTcitta/sfTQ69zpRyrcefJoffodz+rrqsrpz/IvDwmvuHho89unYjSTsZnBmwdevWmpq2fjlVjOQ8LUMRKa8QbXrsoK0VK7n8DuX0d9VlWQ34RsQeYA8M6vwbbk7rtPGPZjVQjy6wDuvrdeYQeHMYOF2vNv4O2frUFfxXgFPW3D95+Jj12GigDnj+C2Bxnb3OHAJv0ymsps98rB3qCv57gcskfRF4PfBkRDw64/9Yx40L1KuBf71XrDYdeKHZSc5yOPOxdkhV6vkF4JvA6ZIOSbpU0vslvX+4yT7gIeAg8PvAr6bYr7VzYHFVFYG67xUrrpqyopL0/CPiPTOeD+CDKfZlP9H2Xl4VPeQmF2pf1eTAaQ5nPtYOWQ34VqWrOdAc8ttlVBGo+16x4nn1rajOB/+2946naXsvr6pAXWfgza1jkcOZj7VD54N/23vH09Tdy5sU6MoEwDaXFubYscjlzMfy1/ng3/be8TR19vImBbrlv3yCr9y1klUArEuuHYs2f6FafTo/pXMO1R9VqfNS+EmB7gt3PNzb6pIudyys+zrf8+96DrSuXt6kgPbchJXg+hAAPbg6WW5jIfZine/5e6KoNCYFtA3SXNun1PQ1DjlMhJYjT8/dDp3v+YNzoClMOoN6588vviDnv/p41QEwh8FWD66Ol+tYiL1QL4K/lTct0C29+qdqD4BNBhinNKbzWEg7OPhbYZPOoJo4s2oqwORwxpE7j4W0Q+dz/tZ+43L7TVVxee6c2TwW0g7u+Vst1psqmdTTbmqswSmN2TwW0g4O/la5MqmSST3tW/7sMFdcdGbtAcYpjWJcZJE/B3+rXJnB2Wk97SYCTNevG7H+cM7fKlcmVZLbFdq+bsS6wj1/q1yZVEmOPW2nNKwLUq3kdZ6kByUdlLRrzPPvk3RY0v7hz6+k2K+1Q5nqD/e0zapRuucvaQPwKeCtwCHgTkl7I+L+kU2vj4jLyu6vLF+gM16V70vZ6g/3tM3SS5H2ORs4GBEPAQwXad8BjAb/xjVxgU4bvmzqeF8cwM3ykiLtswg8vOb+oeFjo94p6V5JX5Z0yrgXkrRT0rKk5cOHDydo2gvVfYFOWya4Ws/70vSkamZWTl3VPv8D2BYR/wD4GnDtuI0iYk9ELEXE0pYtW5I3ou4LdNpyNei870tbvtTMbLIUwX8FWNuTP3n42PMi4vGIeHp49zPAzyfY79zqLhtsy9Wg874vbflSM7PJUgT/O4HTJJ0q6SXAJcDetRtIOnHN3QuABxLsd251zzmyni+bJtIp874vbflSM7PJSg/4RsSzki4DbgI2ANdExAFJHweWI2Iv8K8lXQA8CzwBvK/sftej7jlHitaorw4Krxw5ioDVtbHqmjFy3vfFUxyYtZ9iwjJ8TVtaWorl5eWmm1HarGqf0UqbcRY3LXDbrnPraG4h49q8sHGD6+/NMiDprohYmrWdr/Ct2KwSx3H581G5pVM8a6NZ+zn4N6zM/DZNct2+Wbt5YreGzQrsTc9jY2bd5J5/w8YNCq8O+i4mSqe04SrjsvpwjGYpOfg3ZG2wOn5hI8dtPIYjTz2TPHD1Yc3ZPhyjWWoO/nNI1bscDVZHjj7DwsYNfPLdZyUPVmUWUmmLPhyjWWrO+ReUckqDOq+Q7cMFWX04RrPUHPwLShmw6wxWua2EVYU+HKNZag7+Bc0TsGdN0VBnsKp7Sosm9OEYzVJz8C+oaMAukh6qM1hNWwmrK9Mye7Uvs/l5eoeCik5pcM7um8fOezM6RUPTpYlFj6fpdprZfDy9Q2JFpzQomh5q+grZIhUyLqE06y4H/6EiPdwiAbstM14W+ZJyCaVZdznnT9oyzrYMPhYZwyhbldSVMQWzLnLwJ00Z52qg+/D1+3npscdwwss2Zj34WORLqkxVkpd6NMubgz9perhrA92Ro8/wo2d+zCfffRa37To3u8APxSpkypzFeKlHs7wlyflLOg/4HQYreX0mInaPPP9S4DoGa/c+Drw7Ir6bYt8plM3TtzU3PmsMo8y8/b7q1ixvpYO/pA3Ap4C3AoeAOyXtjYj712x2KfD9iPgZSZcAnwDeXXbfqRRdbnGSLge69VYltWXg26yvUqR9zgYORsRDEfG3wBeBHSPb7ACuHd7+MvBmSUqw7yTKXiTk6QVerC0D32Z9lSLtswg8vOb+IeD1k7YZLvj+JPBK4HtrN5K0E9gJsHXr1gRNK65M3X3ZM4cu8lKPZnnLqs4/IvYAe2BwhW/DzSnMgW68pi9kM7PJUgT/FeCUNfdPHj42bptDko4Fjmcw8NsZDnRm1iYpcv53AqdJOlXSS4BLgL0j2+wF3ju8fTFwc+Q6qZCZWQ+U7vkPc/iXATcxKPW8JiIOSPo4sBwRe4HPAp+XdBB4gsEXhJmZNSRJzj8i9gH7Rh776JrbPwLelWJfZmZWnq/wNTPrIQd/M7MecvA3M+shB38zsx7K6iKvlLz8oJnZZJ0M/l5+0Mxsuk6mfTyXvJnZdJ0M/l2eYtnMLIVOBn9PsWxmNl0ng7/nkjczm66TA76eYtnMbLpOBn8oNsWyy0HNrK86G/xncTmomfVZJ3P+Rbgc1Mz6rLfB3+WgZtZnvU37nLRpgZUxgd7loOV4HMWsHUr1/CX9lKSvSfrz4b8nTNjuOUn7hz+jSzw2wuWg6a2Oo6wcOUrwk3GUG+4eXdLZzJpWNu2zC/h6RJwGfH14f5yjEXHW8OeCkvtM4sLti1xx0ZksblpAwOKmBa646Ez3UkvwOIpZe5RN++wA3jS8fS3wDeDflnzN2hQpB7XiPI5i1h5le/4/HRGPDm//P+CnJ2x3nKRlSbdLurDkPi1TnlbDrD1mBn9JfyzpvjE/O9ZuFxEBxISXeXVELAH/HPjPkv7+hH3tHH5JLB8+fHjeY7GGeRzFrD1mpn0i4i2TnpP015JOjIhHJZ0IPDbhNVaG/z4k6RvAduAvxmy3B9gDsLS0NOmLxDLlaTXM2qNszn8v8F5g9/DfPxrdYFgB9FREPC1pM3AO8B9L7tcy5XEUs3Yom/PfDbxV0p8DbxneR9KSpM8Mt/k5YFnSPcAtwO6IuL/kfs3MrIRSPf+IeBx485jHl4FfGd7+38CZZfZjZmZp9XZ6BzOzPnPwNzPrIQd/M7MecvA3M+shB38zsx5y8Dcz66HezudflOenN7MucvCfwuv8mllXOe0zheenN7OucvCfwvPTm1lXOfhP4fnpzayrHPyn8Pz0ZtZVHvCdwvPTm1lXOfjP4PnpzayLnPYxM+shB38zsx4qFfwlvUvSAUk/lrQ0ZbvzJD0o6aCkXWX2aWZm5ZXt+d8HXATcOmkDSRuATwFvB84A3iPpjJL7NTOzEsou4/gAgKRpm50NHIyIh4bbfhHYAXgdXzOzhtSR818EHl5z/9DwMTMza8jMnr+kPwZeNeap34qIP0rZGEk7gZ3Duz+UlGISnc3A9xK8Tlv4eLvNx9tdqY711UU2mhn8I+ItJRuyApyy5v7Jw8fG7WsPsKfk/l5A0nJETByM7hofb7f5eLur7mOtI+1zJ3CapFMlvQS4BNhbw37NzGyCsqWe/1TSIeCNwI2Sbho+fpKkfQAR8SxwGXAT8ADwpYg4UK7ZZmZWRtlqnz8E/nDM448A56+5vw/YV2ZfJSRNI7WAj7fbfLzdVeuxKiLq3J+ZmWXA0zuYmfVQZ4L/rCkkJL1U0vXD5++QtK3+VqZT4Hj/jaT7Jd0r6euSCpV/5aroFCGS3ikppk03krsixyrpnw0/3wOS/lvdbUypwO/yVkm3SLp7+Pt8/rjXaQtJ10h6TNJ9E56XpKuG78e9kl5XSUMiovU/wAbgL4C/B7wEuAc4Y2SbXwU+Pbx9CXB90+2u+Hj/MfCy4e0PdP14h9u9gsFUI7cDS023u8LP9jTgbuCE4f2/23S7Kz7ePcAHhrfPAL7bdLtLHvM/BF4H3Dfh+fOB/wUIeANwRxXt6ErP//kpJCLib4HVKSTW2gFcO7z9ZeDNmjEvRcZmHm9E3BIRTw3v3s7g+oq2KvL5AvwH4BPAj+psXGJFjvVfAp+KiO8DRMRjNbcxpSLHG8DfGd4+HnikxvYlFxG3Ak9M2WQHcF0M3A5sknRi6nZ0JfgXmULi+W1iUH76JPDKWlqX3rxTZlzKoCfRVjOPd3hqfEpE3FhnwypQ5LP9WeBnJd0m6XZJ59XWuvSKHO/HgF8clpXvA/5VPU1rTC1T4nglr46T9IvAEvCPmm5LVSQdA/w28L6Gm1KXYxmkft7E4IzuVklnRsSRRltVnfcAn4uI/yTpjcDnJb0mIn7cdMParCs9/yJTSDy/jaRjGZw+Pl5L69IrNGWGpLcAvwVcEBFP19S2Ksw63lcArwG+Iem7DPKke1s66Fvksz0E7I2IZyLiO8D/ZfBl0EZFjvdS4EsAEfFN4DgG8+B0VeEpccroSvAvMoXEXuC9w9sXAzfHcHSlhWYer6TtwH9lEPjbnBOGGccbEU9GxOaI2BYR2xiMcVwQEcvNNLeUIr/LNzDo9SNpM4M00EN1NjKhIsf7V8CbAST9HIPgf7jWVtZrL/AvhlU/bwCejIhHU++kE2mfiHhW0uoUEhuAayLigKSPA8sRsRf4LIPTxYMMBlsuaa7F5RQ83iuBlwP/fTiu/VcRcUFjjS6h4PF2QsFjvQn4J5LuB54DLo+IVp7FFjzejwC/L+nDDAZ/39fijhuSvsDgy3vzcBzj3wEbASLi0wzGNc4HDgJPAb9cSTta/B6amdk6dSXtY2Zmc3DwNzPrIQd/M7MecvA3M+shB38zsx5y8Dcz6yEHfzOzHnLwNzProf8PCnoRWfzkjZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11580c290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x = np.linspace(0, 1, m)\n",
    "y = np.random.normal(loc=2.0*x+0.3, scale=0.5, size=m)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run regression using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   106.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:44:13</td>     <th>  Log-Likelihood:    </th> <td> -84.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   173.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   178.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3065</td> <td>    0.113</td> <td>    2.710</td> <td> 0.008</td> <td>    0.082</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0140</td> <td>    0.195</td> <td>   10.306</td> <td> 0.000</td> <td>    1.626</td> <td>    2.402</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.753</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.252</td> <th>  Jarque-Bera (JB):  </th> <td>   1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.035</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    4.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.520\n",
       "Model:                            OLS   Adj. R-squared:                  0.515\n",
       "Method:                 Least Squares   F-statistic:                     106.2\n",
       "Date:                Sat, 09 Mar 2019   Prob (F-statistic):           2.63e-17\n",
       "Time:                        12:44:13   Log-Likelihood:                -84.642\n",
       "No. Observations:                 100   AIC:                             173.3\n",
       "Df Residuals:                      98   BIC:                             178.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3065      0.113      2.710      0.008       0.082       0.531\n",
       "x1             2.0140      0.195     10.306      0.000       1.626       2.402\n",
       "==============================================================================\n",
       "Omnibus:                        2.753   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                1.746\n",
       "Skew:                           0.035   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.356   Cond. No.                         4.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression.linear_model.OLS(y, np.column_stack([np.ones(len(x)), x])).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's see if the formulas we derived for $\\hat{\\beta}$ and $\\hat{\\alpha}$ match the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha = 0.306537621742\n",
      "Reproduced  Alpha = 0.306537621742\n",
      "\n",
      "statsmodels Beta  = 2.01403383001\n",
      "Reproduced  Beta  = 2.01403383001\n"
     ]
    }
   ],
   "source": [
    "xmean = np.mean(x)\n",
    "xvar = np.var(x)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "beta = np.cov(x, y, bias=True)[0, 1] / xvar\n",
    "alpha = ymean - beta * xmean\n",
    "print(\"statsmodels Alpha = {}\".format(model.params[0]))\n",
    "print(\"Reproduced  Alpha = {}\".format(alpha))\n",
    "print(\"\")\n",
    "print(\"statsmodels Beta  = {}\".format(model.params[1]))\n",
    "print(\"Reproduced  Beta  = {}\".format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$ are quite close to the actual model parameters $\\beta=2$ and $\\alpha=0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "R-squared (usually denoted $R^2$) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and $f^{(i)} = f(x^{(i)})$. R-squared compares the error in prediction $y^{(i)} - f^{(i)}$ to that of the trivial model $f=\\bar{y}$. The smaller the error of prediction, the closer $R^2$ is to $1$. In that sense, it measures the proportion of variance in the dependent variable $y$ that is predictable from the independent variable $x$ using the model $f$. Note, however, that this is an *in-sample* measure of goodness-of-fit, if the same dataset is used to fit the parameters of the model *and* compute $R^2$.\n",
    "\n",
    "Let's confirm $R^2$ for our linear regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.520089566727\n",
      "Reproduced  R-squared = 0.520089566727\n"
     ]
    }
   ],
   "source": [
    "f = alpha + beta * x\n",
    "e = f - y\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model $f$ is linear regression with an intercept term, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! Let's prove this.\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "S_y^2   = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2 ,\\qquad\n",
    "r_{xy}  = \\frac{C_{xy}}{S_xS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where $S_y$ is the (biased) sample variance of $y$ and $r_{xy}$ is the sample correlation coefficient of $x$ and $y$. Using this, we can express $\\hat{\\beta}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{r_{xy}S_y}{S_x}.\n",
    "\\end{equation}\n",
    "\n",
    "Also note that:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} - \\bar{y} = \\hat{\\alpha} + \\hat{\\beta}x^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x}),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$. The average of squared residuals can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - y^{(i)})^2\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m[(f^{(i)} - \\bar{y}) - (y^{(i)} - \\bar{y}) ]^2 \\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})^2 + \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\frac{2}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\frac{\\hat{\\beta}^2}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})^2 + S_y^2 - \\frac{\\hat{\\beta}}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\hat{\\beta}^2 S_x^2 + S_y^2 - 2\\hat{\\beta}C_{xy} \\\\\n",
    "&= S_y^2(1 - r_{xy}^2),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$, and the last one from substituting the value of $\\hat{\\beta}$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2} \\\\\n",
    "    &= 1 - \\frac{S_y^2(1 - r_{xy}^2)}{S_y^2} \\\\\n",
    "    &= r_{xy}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We proved that $R^2$ is given by the square of the sample correlation between $x$ and $y$, and not $f$ and $y$ as originally promised! First, let's numerically check what we just proved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's do the extra work and show that $r_{fy}^2 = r_{xy}^2$.\n",
    "\n",
    "To do this, we'll need one more result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m f^{(i)}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)})\n",
    "    = \\hat{\\alpha} + \\hat{\\beta}\\bar{x}\n",
    "    = \\bar{y},\n",
    "\\end{equation}\n",
    "where the last line follows from $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}$.\n",
    "\n",
    "Now we're ready:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{fy}\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{y})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sqrt{\\hat{\\beta}^2\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}}{|\\hat{\\beta}|}r_{xy},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\bar{f}=\\bar{y}$ and the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$. There we have it:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = r_{fy}^2 = r_{xy}^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "In most realistic cases, we cannot hope to explain all variability in $y$ using only $x$. This is also true for models which are more complex than linear regression. We should always expect a certain degree of randomness that our models cannot account for. However, we can hope to design models that make the correct predictions *on average*. This suggests a probabilistic approach for modelling. We can think about $y$ as a random variable with a certain probability distribution, whose mean is given by the model $f(x)$.\n",
    "\n",
    "For simplicity, let's assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing $(x^{(i)}, y^{(i)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P^{(i)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we're assuming all observations are independent, the probability of observing the whole dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\Pi_{i=1}^m P^{(i)} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\alpha, \\beta)}{\\sigma^2}\\right)},\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is the cost function defined earlier. This is interesting! The probability of observing the data set can be expressed in terms of the cost function. In fact, the parameters $\\alpha$ and $\\beta$ that minimize $J$ also maximize $P$. Picking the parameters of a model to maximize the probability of observing the dataset is called *maximum likelihood estimation*.\n",
    "\n",
    "*Log-likelihood* is given by $\\log P$, which in our case is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\alpha, \\beta, \\sigma^2) = -\\frac{m}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m(y^{(i)} - \\alpha - \\beta x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the model summary above from `statsmodels`, a number is quoted for Log-Likelihood. To reproduce it, let's compute $\\log P(\\hat{\\alpha}, \\hat{\\beta}, \\sigma^2)$. One thing, though, is that $\\log P$ depends on $\\sigma$, which `statsmodels` doesn't take as input. It can, however, esimate it from the sample standard deviation of the prediction errors $y^{(1)} - f^{(1)}, \\dots, y^{(m)} - f^{(m)}$. Let's give that a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -84.6424357588\n",
      "Reproduced  Log-Likelihood = -84.6424357588\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(len(x)/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "Given the probabilistic view, we now see that the optimal estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$ should be regarded as random variables, and not confused with the \"real\" underlying parameters $\\alpha$ and $\\beta$. A different dataset, for instance, would result in different values for $\\hat{\\alpha}$ and $\\hat{\\beta}$. Therefore, it's important to study their variability.\n",
    "\n",
    "Let's first check their means. Note that by assumption\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "and as a result\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\bar{y} \\rangle = \\frac{1}{m} \\sum_{i=1}^m \\langle y^{(i)} \\rangle = \\frac{1}{m} \\sum_{i=1}^m (\\alpha + \\beta x^{(i)}) = \\alpha + \\beta \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Combining these:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle(y^{(i)} - \\bar{y})\\rangle = \\beta (x^{(i)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these results, we can compute the mean of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta} \\rangle\n",
    "= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})\\langle(y^{(i)} - \\bar{y})\\rangle\n",
    "= \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x}) = \\beta,\n",
    "\\end{equation}\n",
    "\n",
    "and also that of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\alpha} \\rangle = \\langle \\bar{y} \\rangle - \\langle \\hat{\\beta} \\rangle \\bar{x} = \\alpha + \\beta \\bar{x} - \\beta \\bar{x} = \\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "So the mean of $\\hat{\\beta}$ and $\\hat{\\alpha}$ are $\\beta$ and $\\alpha$, respectively. Good, but no terribly surprising!\n",
    "\n",
    "Let's compute the variance of $\\hat{\\beta}$. First, we will express $\\hat{\\beta} - \\beta$ in a convenient way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) - \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})[(y^{(i)} - \\beta x^{(i)} - \\alpha) - (\\bar{y} - \\beta \\bar{x} - \\alpha)] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle) - \\frac{\\bar{y} - \\beta \\bar{x} - \\alpha}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "        &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle)\n",
    "\\end{align*}\n",
    "\n",
    "where the first equality follows from the definition of $S_x^2$, the second from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, and the third from $\\sum_{i=1}^m (x^{(i)} - \\bar{x})=0$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\\\\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\sigma^2\\delta_{ij} \\\\\n",
    "&= \\frac{\\sigma^2}{m^2S_x^4}\\sum_{i}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "&= \\frac{\\sigma^2}{mS_x^2},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from the fact that $y^{(1)}, \\dots, y^{(m)}$ are (by assumption) independent and normally distributed with variance $\\sigma^2$.\n",
    "\n",
    "Let's check this against `statsmodels`. The estimated value of $\\sigma^2$ (since the real value is unknown to `statsmodels`), is stored in the `scale` attribute of `model`. Using that, we can reproduce the standard error of $\\hat{\\beta}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta std err = 0.195431582311\n",
      "Reproduced  Beta std err = 0.195431582311\n"
     ]
    }
   ],
   "source": [
    "beta_se = np.sqrt(model.scale / (m * xvar))\n",
    "print(\"statsmodels Beta std err = {}\".format(model.bse[1]))\n",
    "print(\"Reproduced  Beta std err = {}\".format(beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute $\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle$. First, note that we can express $\\hat{\\alpha} - \\alpha$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha\n",
    "    = \\bar{y} - (\\hat{\\beta} - \\beta)\\bar{x} - \\beta\\bar{x} - \\alpha\n",
    "    = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "To compute the variance of $\\hat{\\alpha} - \\alpha$, we will need the covariance between $\\hat{\\beta} - \\beta$ and $\\bar{y} - \\langle \\bar{y} \\rangle$. We can express $\\bar{y} - \\langle \\bar{y} \\rangle$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\bar{y} - \\beta \\bar{x} - \\alpha = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\beta x^{(j)} - \\alpha) = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\left\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\right\\rangle \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\langle(\\bar{y} - \\langle \\bar{y} \\rangle)^2\\right\\rangle\n",
    "=\\frac{1}{m^2}\\sum_{k,l=1}^m \\left\\langle(y^{(k)} - \\langle y^{(k)} \\rangle)(y^{(l)} - \\langle y^{(l)} \\rangle)\\right\\rangle\n",
    "= \\frac{1}{m^2}\\sum_{k,l=1}^m \\sigma^2\\delta_{kl}\n",
    "= \\frac{\\sigma^2}{m}.\n",
    "\\end{equation}\n",
    "This is exactly what we should've expected without doing any work, since $\\bar{y} - \\langle \\bar{y} \\rangle \\sim \\mathcal{N}(0, \\sigma^2/m)$ follows from the fact that $\\bar{y}$ is an average of independent and identical normally distributed random variables.\n",
    "\n",
    "Now we can compute the variance of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle =\n",
    "\\left\\langle (\\bar{y} - \\langle \\bar{y} \\rangle)^2 \\right\\rangle + \\bar{x}^2 \\left\\langle (\\hat{\\beta} - \\beta)^2 \\right\\rangle\n",
    "= \\frac{\\sigma^2}{m} + \\bar{x}^2\\frac{\\sigma^2}{mS_x^2}\n",
    "= \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha std err = 0.113117048297\n",
      "Reproduced  Alpha std err = 0.113117048297\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(model.scale / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels Alpha std err = {}\".format(model.bse[0]))\n",
    "print(\"Reproduced  Alpha std err = {}\".format(alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation above also makes it easy to derive the covariance between $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)(\\hat{\\beta} - \\beta) \\rangle\n",
    "= \\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle - \\bar{x} \\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "= 0 - \\bar{x} \\frac{\\sigma^2}{mS_x^2}\n",
    "= -\\frac{\\sigma^2\\bar{x}}{mS_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Again, we get the same number as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha-beta covariance = -0.0190967516823\n",
      "Reproduced  alpha-beta covariance = -0.0190967516823\n"
     ]
    }
   ],
   "source": [
    "cov_params = - model.scale * xmean/(m*xvar)\n",
    "print(\"statsmodels alpha-beta covariance = {}\".format(model.cov_params()[0, 1]))\n",
    "print(\"Reproduced  alpha-beta covariance = {}\".format(cov_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we glossed over was how `statsmodels` estimates $\\sigma^2$. Let's think about this a little. Since, by assumption, $y{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)$, it makes sense to use the sample variance of the prediction error $e_i=y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)}=y^{(i)} - f^{(i)}$ as an estimate. We can express $e_i$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "e_i &= y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\hat{\\alpha} - \\alpha) - (\\hat{\\beta} - \\beta)x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\sum_{j=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)\\left[\\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}) \\right] \\\\\n",
    "    &= \\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, the third from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$, the fourth from our earlier expressions for $\\bar{y} - \\langle \\bar{y} \\rangle$ and $\\hat{\\beta} - \\beta$, and in the last equality we have introduced the matrix $A$ whose elements are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} \\equiv \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "$A$ has a few interesting properties. First, it's symmetric: $A_{ij} = A_{ij}$. Second, it's equal to its square: $A^2=A$. (This can be shown by carrying out the matrix multiplication between $A$ and itself. It's a bit tedious and not terribly interesting, so I'll leave it out.) Using these results, we can compute the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m} e_i^2\n",
    "    &= \\sum_{i=1}^{m}\\left[\\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle)\\right]^2 \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\left[\\sum_{i=1}^{m}A_{ij}A_{ik}\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle) A_{jk},\n",
    "\\end{align*}\n",
    "\n",
    "where the last equality follows from the fact that $A$ is symmetric and $A^2=A$. Taking the expectation value of both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle\\sum_{i=1}^{m} e_i^2 \\right\\rangle\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\left\\langle(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(A),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{tr}(A)=\\sum_{i=1}^{m}A_{ii}$ denotes the trace of $A$. It's easy to show that $\\text{tr}(A)=m-2$, so the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m-2}\\sum_{i=1}^{m}e_i^2 = \\frac{1}{m-2}\\sum_{i=1}^m(y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.324709077426\n",
      "Reproduced  estimate of sigma^2 = 0.324709077426\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m-2)\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "We've worked out the variance (and covariance) of the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. It would also be nice to have confidence intervals for them, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta_L, \\beta_U)$ contains $\\beta$. We've done this sort of analysis for the sample mean when discussing the [t-distribution](https://github.com/siavashaslanbeigi/stats_notes/blob/master/t.ipynb). Can we do something similar for $\\hat{\\alpha}$ and $\\hat{\\beta}$? Yes, but it's a bit more involved than dealing with the sample mean.\n",
    "\n",
    "\n",
    "Above we showed that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2/(m S_x^2))$, or equivalently $\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma \\sim \\mathcal{N}(0, 1)$. This means $\\beta_L=\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x}$ and $\\beta_U=\\hat{\\beta}+z\\frac{\\sigma}{\\sqrt{m} S_x}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x} \\le \\beta \\le \\hat{\\beta}+z \\frac{\\sigma}{\\sqrt{m} S_x})\n",
    "= P(-z \\le \\frac{\\hat{\\beta} - \\beta}{\\sigma/(\\sqrt{m} S_x)} \\le z )\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\hat{\\sigma}^2 / (m S_x^2)}}\n",
    "    = \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)}{\\sigma}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\sum_{i=1}^{m}e_i^2.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. In what follows, we will show that $V$ has a chi-squared distribution with $m-2$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "We will make use (one of the formulations of) [Cochran's theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem), which states the following: suppose $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an $m$-dimensional multivariate normal random vector, and that $B^{(1)}, \\dots, B^{(k)}$ are symmetric $m \\times m$ matrices which satisfy the following properties:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{k}B^{(i)} = I, \\qquad\n",
    "\\sum_{i=1}^{k}r_i = m,\n",
    "\\end{equation}\n",
    "where $I$ is the $m \\times m$ identity matrix and $r_i$ denotes the rank of $B_i$. Then, the random variable $Q^{(i)}=Y^TB^{(i)}Y$ is distributed as $\\sigma^2 \\chi^2_{r_i}$, where $\\chi^2_{r_i}$ is the chi-squared distribution with $r_i$ degrees of freedom, and $Q^{(i)}$ and $Q^{(j)}$ are independent for all $i\\neq j$.\n",
    "\n",
    "\n",
    "It's not immediately obvious how this theorem will help us arrive at confidence intervals, but it will. First, note that if we define $Y$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i \\equiv y^{(i)} - \\langle y^{(i)} \\rangle.\n",
    "\\end{equation}\n",
    "then $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$. With this definition, we can rewrite the sum of squared errors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}e_i^2 = Y^TAY.\n",
    "\\end{equation}\n",
    "\n",
    "Now this looks like a $Q$ matrix from Cochran's theorem. What is the rank of $A$? Remember that the rank of a matrix is the dimension of the vector space spanned by its column vectors. It can be shown that this is equal to the number of non-zero eigenvalues. Because $A^2=A$, eigenvalues of $A$ are either $1$ or $0$. To see why, let $v$ denote an eigenvector of $A$ with eigenvalue $\\lambda$: $Av=\\lambda v$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda v = Av = A^2v=A(Av)=A(\\lambda v)=\\lambda Av=\\lambda^2v,\n",
    "\\end{equation}\n",
    "\n",
    "which implies $\\lambda^2=\\lambda$, which has the solutions $\\lambda=0$ and $\\lambda=1$. Therefore, for $A$, the number of non-zero eigenvalues is simply the sum of its eigenvalues, which is also its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Rank}(A) = \\text{tr}(A) = m - 2.\n",
    "\\end{equation}\n",
    "Let's summarize what we have so far\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(1)} \\equiv \\sum_{i=1}^{m}e_i^2 = Y^TB^{(1)}Y, \\qquad\n",
    "B^{(1)}_{ij} = \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}), \\qquad\n",
    "r_1 = m - 2.\n",
    "\\end{equation}\n",
    "\n",
    "Next we'll turn our attention to $\\hat{\\beta} - \\beta$. We had previous shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} - \\beta = \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(2)} \\equiv m S_x^2(\\hat{\\beta} - \\beta)^2 = Y^TB^{(2)}Y, \\qquad\n",
    "B^{(2)}_{ij} = \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Again, it can be checked that $B^{(2)}$ is equal to its own square, so its rank is given by its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "r_2 = \\text{Rank}(B^{(2)}) = \\text{tr}(B^{(2)}) = \\sum_{i=1}^{m} B^{(2)}_{ii} = \\frac{1}{m S_x^2}\\sum_{i=1}^{m} (x^{(i)} - \\bar{x})^2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, as we showed in previous section\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(3)} \\equiv m (\\bar{y} - \\langle \\bar{y} \\rangle)^2 = Y^TB^{(3)}Y, \\qquad\n",
    "B^{(3)}_{ij} = \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Since all columns of $B^{(3)}$ are the same, its rank is $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "It's now easy to check that $B^{(1)}$, $B^{(2)}$, and $B^{(3)}$ satisfy the conditions of Cochran's theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(1)}_{ij} + B^{(2)}_{ij} + B^{(3)}_{ij} = \\delta_{ij}, \\qquad\n",
    "r_1 + r_2 + r_3 = m.\n",
    "\\end{equation}\n",
    "\n",
    "Cochran's theorem then implies the following:\n",
    "\n",
    "* $\\sum_{i=1}^{m}e_i^2 \\sim \\sigma^2 \\chi^2_{m-2}$.\n",
    "* $\\sum_{i=1}^{m}e_i^2$, $(\\hat{\\beta} - \\beta)^2$, and $(\\bar{y} - \\langle \\bar{y} \\rangle)^2$ are mutually independent, which implies $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are mutually independent.\n",
    "\n",
    "This in turn implies that $V$ has a $\\chi^2_{m-2}$ distribution and that $Z$ and $V$ are independent. Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}} = \\frac{\\hat{\\beta} - \\beta}{s_{\\hat{\\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "has the t-distribution with $m-2$ degrees of freedom, where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\beta}}^2 = \\frac{\\hat{\\sigma}^2}{m S_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally: $\\beta_L=\\hat{\\beta}-ts_{\\hat{\\beta}}$ and $\\beta_U=\\hat{\\beta}+ts_{\\hat{\\beta}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta 95% confidence interval = (1.62620621534, 2.40186144467)\n",
      "Reproduced  beta 95% confidence interval = (1.62620621534, 2.40186144467)\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - 2)\n",
    "# Reproduced confidence interval for beta\n",
    "beta_se = np.sqrt(yvar / (m * np.var(x)))\n",
    "print(\"statsmodels beta 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[1, 0], statsmodels_ci[1, 1]))\n",
    "print(\"Reproduced  beta 95% confidence interval = ({0}, {1})\".format(beta - t*beta_se, beta + t*beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the confidence interval for $\\hat{\\alpha}$. Earlier we had shown that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} \\sim \\mathcal{N}\\left(\\alpha, \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As before, we divide $\\hat{\\alpha} - \\alpha$ by its standard deviation and replace $\\sigma$ with $\\hat{\\sigma}$:\n",
    "\\begin{equation}\n",
    "T_{\\alpha}\n",
    "    \\equiv \\frac{\\hat{\\alpha} - \\alpha}{s_{\\hat{\\alpha}}}\n",
    "    = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2)}}\n",
    "    = \\frac{\\sqrt{m}(\\hat{\\alpha} - \\alpha)/\\sqrt{\\sigma^2(1+\\bar{x}^2/S_x^2)}}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z_{\\alpha}}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\alpha}} \\equiv \\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2), \\qquad\n",
    "Z_{\\alpha} = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that $Z_{\\alpha} \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi^2_{m-2}$. If $Z_{\\alpha}$ and $V$ are independent, then $T_{\\alpha}$ also has a t-distribution with $m-2$ degrees of freedom. We had shown earlier that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are independent, it follows that $\\hat{\\alpha} - \\alpha$ and $\\sum_{i=1}^{m}e_i^2$, and in turn $Z_{\\alpha}$ and $V$, are independent. As a result, $\\alpha_L=\\hat{\\alpha}-ts_{\\hat{\\alpha}}$ and $\\alpha_U=\\hat{\\alpha}+ts_{\\hat{\\alpha}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n",
      "Reproduced  alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(yvar / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels alpha 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[0, 0], statsmodels_ci[0, 1]))\n",
    "print(\"Reproduced  alpha 95% confidence interval = ({0}, {1})\".format(alpha - t*alpha_se, alpha + t*alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using $\\hat{\\alpha}$ and $\\hat{\\beta}$, so our predictions are also subject to statistical noise. Let's compute the variance of $\\hat{f}=\\hat{\\alpha} + \\hat{\\beta} x$. First, its mean is what we expect it to be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{f} \\rangle = \\langle\\hat{\\alpha}\\rangle + \\langle\\hat{\\beta}\\rangle x = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "As a result:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f} - \\langle \\hat{f} \\rangle\n",
    "    &= (\\hat{\\alpha} - \\alpha) + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}  + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) + (\\hat{\\beta} - \\beta)(x - \\bar{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$. Since $\\bar{y}$ and $\\hat{\\beta}$ are independent, the variance of $\\hat{f}$ is given simply by:\n",
    "\n",
    "The variance is given by:\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{f} - \\langle \\hat{f} \\rangle)^2 \\right\\rangle\n",
    "    &= \\langle (\\bar{y} - \\langle \\bar{y}\\rangle)^2 + (x - \\bar{x})^2 \\langle(\\hat{\\beta} - \\beta)^2 \\rangle \\\\\n",
    "    &= \\frac{\\sigma^2}{m} + (x - \\bar{x})^2 \\frac{\\sigma^2}{m S_x^2} \\\\\n",
    "    &= \\frac{\\sigma^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "The same argument we used for deriving the confidence interval of $\\hat{\\alpha}$ can be used for $\\hat{f}$: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom, and:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\frac{\\hat{\\sigma}^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can use `seaborn.regplot` to plot the confidence interval for $f$. Below we also show the $95\\%$ confidence interval computed using the formula we just derived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158fe690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard error of f\n",
    "x_axis = np.linspace(0, 1, 1000)\n",
    "f_se = np.sqrt(yvar/m*(1 + (x_axis - xmean)**2/xvar))\n",
    "f_l = alpha + beta * x_axis - t * f_se\n",
    "f_u = alpha + beta * x_axis + t * f_se\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_axis, f_l, 'r--')\n",
    "plt.plot(x_axis, f_u, 'r--', label='Reproduced 95% confidence interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Plot using seaborn\n",
    "seaborn.regplot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the quantities $t$ and $P>|t|$ which are quoted for both $\\hat{\\alpha}$ and $\\hat{\\beta}$ in the regression summary above. Consider the following null hypothesis: $\\beta=0$. In other words, we assume that the true population value of $\\beta$ is zero. Given this assumption, we can compute a concrete $t$-value for $\\hat{\\beta}$: $t=\\hat{\\beta}/s_{\\hat{\\beta}}$. Futhermore, we can calculate the probability of observing a value greater than $|t|$, given the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta t-value = 10.3055698889\n",
      "Reproduced  beta t-value = 10.3055698889\n",
      "\n",
      "statsmodels beta p-value = 2.63092577535e-17\n",
      "Reproduced  beta p-value = 2.63092577535e-17\n"
     ]
    }
   ],
   "source": [
    "tval_beta = beta/beta_se\n",
    "pval_beta = 2*stats.t.cdf(-abs(tval_beta), m - 2)\n",
    "\n",
    "print(\"statsmodels beta t-value = {0}\".format(model.tvalues[1]))\n",
    "print(\"Reproduced  beta t-value = {0}\".format(tval_beta))\n",
    "print(\"\")\n",
    "print(\"statsmodels beta p-value = {0}\".format(model.pvalues[1]))\n",
    "print(\"Reproduced  beta p-value = {0}\".format(pval_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for $\\hat{\\alpha}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta t-value = 2.70991531654\n",
      "Reproduced  beta t-value = 2.70991531654\n",
      "\n",
      "statsmodels beta p-value = 0.00794562387443\n",
      "Reproduced  beta p-value = 0.00794562387443\n"
     ]
    }
   ],
   "source": [
    "tval_alpha = alpha/alpha_se\n",
    "pval_alpha = 2*stats.t.cdf(-abs(tval_alpha), m - 2)\n",
    "\n",
    "print(\"statsmodels beta t-value = {0}\".format(model.tvalues[0]))\n",
    "print(\"Reproduced  beta t-value = {0}\".format(tval_alpha))\n",
    "print(\"\")\n",
    "print(\"statsmodels beta p-value = {0}\".format(model.pvalues[0]))\n",
    "print(\"Reproduced  beta p-value = {0}\".format(pval_alpha))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
