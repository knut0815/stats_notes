{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Consider the data set $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$, where $x$ is the independent variable and $y$ is the dependent variable. We will model this data set as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "What are the values of $\\alpha$ and $\\beta$ that provide the best fit to the data set? To answer this question, we minimize the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha, \\beta) &= \\frac{1}{2m}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "                 &= \\frac{1}{2m}\\sum_{i=1}^N (\\alpha + \\beta x^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "$J(\\alpha, \\beta)$ is called the *cost function*, or *objective function*. Let's minimize the cost function with respect to $\\alpha$ and $\\beta$. We will denote the optimal values by $\\hat{\\beta}$ and $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial\\alpha} = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)}) = 0 \\\\\n",
    "\\frac{\\partial J}{\\partial\\beta}  = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)})x^{(i)} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)} ,\\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y^{(i)} ,\\qquad\n",
    "S_x^2   = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 ,\\qquad\n",
    "C_{xy}  = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "These quantities should be familiar: $\\bar{x}$ is the sample mean of $x$, $\\bar{y}$ is the sample mean of $y$, $S_x^2$ is the (biased) sample variance of $x$, and $C_{xy}$ is the (biased) sample covariance of $x$ and $y$. Using these definitions, it can be shown after straightforward algebra that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{C_{xy}}{S_x^2}, \\qquad\n",
    "\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data using the model $f(x)=0.3 + 2x$ and Gaussian noise with $\\sigma=0.5$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHQhJREFUeJzt3X+wZ3V93/Hni2WV62hZ4m4jXFiXNoQJI5U1d1CHmdaiVsQMSxErdpJqhnRHI220hukmmbHW/sFaprEh2NiNMoKTKlYN2ZbtMEZwSKhQLrIgC6HZoAl7oWEFF+OwEsB3//h+L16+fH+c7z2fc87nnPN6zNzZ74+z3/M53++97+/nvD/v8/koIjAzs345pukGmJlZ/Rz8zcx6yMHfzKyHHPzNzHrIwd/MrIcc/M3MesjB38yshxz8zcx6yMHfzKyHjm26AZNs3rw5tm3b1nQzzMxa5a677vpeRGyZtV22wX/btm0sLy833Qwzs1aR9JdFtnPax8yshxz8zcx6yMHfzKyHHPzNzHrIwd/MrIcc/M3MeijbUk8zs6rdcPcKV970II8cOcpJmxa4/G2nc+H2xaabVQsHfzPrpRvuXuE3vvptjj7zHAArR47yG1/9NkAvvgCc9jGzXrrypgefD/yrjj7zHFfe9GBDLaqXg7+Z9dIjR47O9XjXOPibWS+dtGlhrse7pnTwl3ScpP8j6R5JByT9+zHbvFTS9ZIOSrpD0ray+zUzK+Pyt53OwsYNL3hsYeMGLn/b6Q21qF4pev5PA+dGxGuBs4DzJL1hZJtLge9HxM8AnwQ+kWC/ZmbrduH2Ra646EwWNy0gYHHTAldcdGYvBnshQbVPRATww+HdjcOfGNlsB/Cx4e0vA1dL0vD/mpk14sLti70J9qOS5PwlbZC0H3gM+FpE3DGyySLwMEBEPAs8Cbwyxb7NzGx+SYJ/RDwXEWcBJwNnS3rNel5H0k5Jy5KWDx8+nKJpZmY2RtJqn4g4AtwCnDfy1ApwCoCkY4HjgcfH/P89EbEUEUtbtsxciMbMzNYpRbXPFkmbhrcXgLcCfzay2V7gvcPbFwM3O99vZtacFNM7nAhcK2kDgy+TL0XE/5T0cWA5IvYCnwU+L+kg8ARwSYL9mpnZOqWo9rkX2D7m8Y+uuf0j4F1l92VmZmn4Cl8zsx5y8Dcz6yEHfzOzHvJ8/mbWOn1ehCUVB38za5VcFmFp+xeQg7+Ztcq0RVjqCr7TvoBW25j7l4KDv5m1Sg6LsEz6AvrY3gM8/eyPGz8rKcIDvmbWKnUswnLD3Sucs/tmTt11I+fsvpkb7l55wfOTvmiOHH2mNUtDOvibWatUvQjLakpn5chRgp/03td+Acz7RZPj0pAO/mbWKlUvwlJkYfdJX0AnvGzj2NfMcWlI5/zNrHWqXISlyJjC6r5HB3aBFwwEQ75LQzr4m5mtcdKmBVbGfAGM9t6nfQG52sfMrGUuf9vppXrvbVka0sHfzGyNSSmdNgT0eTj4m5mNaEvvvQwHfzPLVtunUMhZimUcT5F0i6T7JR2Q9GtjtnmTpCcl7R/+fHTca5mZrSpSb2/rl6Ln/yzwkYj4lqRXAHdJ+lpE3D+y3Z9ExC8k2J+Z9UAOc/h0Wemef0Q8GhHfGt7+G+ABwJ+MmZWSwxw+XZY05y9pG4P1fO8Y8/QbJd0DPAL8ekQcSLlvM+uWovX2bZTDWEay6R0kvRz4CvChiPjByNPfAl4dEa8Ffhe4YcJr7JS0LGn58OHDqZpmZi1U9Rw+TcllLCNJ8Je0kUHg/4OI+Oro8xHxg4j44fD2PmCjpM1jttsTEUsRsbRly5YUTTObadYMjtaMqufwaUqRuYPqUDrtI0nAZ4EHIuK3J2zzKuCvIyIknc3gS+fxsvs2K6voqlA5nKb3URfr7XMZy0iR8z8H+CXg25L2Dx/7TWArQER8GrgY+ICkZ4GjwCUREQn2bVZKkYqSXJYNtPHa9sWcy1hG6eAfEX8KaMY2VwNXl92XWWpFemEuOcxXG7+Yy84dlIqv8LVeK9ILy+U0PUdFet1V9szb+MWcy9xBDv7Wa0V6YXWdprctfVGk1111z7ytX8w5jGV4JS/rtSIVJXWUHFZV/ldlJVORqpWqK1vqWM+3q9zzt96b1Qur4zS9ivRFDr3uqnvmueTP28jB36yAqk/TqwiSVefDi6TDqk6Z5ZI/byMHf7MMVBEkc+h119EzrzN/3rZxmWmc8zdLpEx+fdy4ghikatabq686H15kvKRLV+nmMi1DKsr1WqulpaVYXl5uuhlmhYzm12HQw50n0K32KleOHEXA2r/MeV8rVZvsJ87ZffPYs7PFTQvctuvcBlo0nqS7ImJp1nbu+ZslkKKq5cLti9y261wWNy0w2iVbT4VMl3rddZh15tbWstJJnPM3SyBlYEj5WjnUk7dBkcqoXKZlSMU9f7MEUubXXbtevyJnbl2bYtrB3yyBlIGha0GmDYqcbXUtjea0j1kCKevNXbtev6IpnSrSaE2Vj7rax8x6r6nKqCr262ofM7OCmkrpNLmql9M+ZmY0UxnVZPmog79Z5lLlhLs0NUFu1vveNlk+WjrtI+kUSbdIul/SAUm/NmYbSbpK0kFJ90p6Xdn9mvVBqikFujY1QU7KvLdNVnalyPk/C3wkIs4A3gB8UNIZI9u8HTht+LMT+L0E+zXrvFQ54SZzy11X5r1tsnw0xRq+jwKPDm//jaQHgEXg/jWb7QCuGy7afrukTZJOHP5fM5sgVU64a1MT5KTse9vUVdhJc/6StgHbgTtGnloEHl5z/9DwsRcEf0k7GZwZsHXr1pRNM8vCvLnhVDnhsq/j8YLJ2jrtQ7JST0kvB74CfCgifrCe14iIPRGxFBFLW7ZsSdU0syysJzc8LSc8zxTSZXLLHi+Yrq1XZCcJ/pI2Mgj8fxARXx2zyQpwypr7Jw8fM+uN9eSGJ+WEgbkCcpncsscLpmvrtA+lr/CVJOBa4ImI+NCEbd4BXAacD7weuCoizp72ur7C17rm1F03vmiq5lWLc6ZS6pxbflK7BXxn9zuS7svKK3qFb4qc/znALwHflrR/+NhvAlsBIuLTwD4Ggf8g8BTwywn2a9Yqk3LDMP/i6nUO4LY1p23Tpaj2+VMGnYBp2wTwwbL7MmuzcevZrjXP4up1BuQ61uFNxQPTxXluH7OarM0NT1K0517nIGNbctoemJ6PZ/U0a0CKnL17uS/UljV2q1Znzt/M5pQilVL1xUFVf7mkfn1fyDYfB3+zCaoMfrkv2FJkTdvcXt8D0/Nx8G8Jn+LXq+rgt/o6uX6G02r7U7S5itdv08B0Dhz8M7Ya8FeOHEXwfK11FYHIXqjq4Je7qlMoVbx+7mdTuXHwz9Roz3N0WL5PgagJbcsfpz4zrDqFUtXr53Y2lfMZu0s9MzWu5zkq10DUBZOCUI754ypKHKsuJW3rfDjzyL301ME/U0UCe46BqCvaFJzWM/fOrEnhqq7tr/vagXkmwUsl9zmRnPbJ1LSpACDfQNQVbcofz5uiKjqYXXUKpa4UTR2D9+Pknjp08M/UuMqF1UHfeScBs/XJLX88ybz58yYHs5vIgTd1vLmXnjr4Z6pNPU9r1rwljk31SPvWA8+99NTBP2Nt6Xlas+btKDTVIy3bA1/vWUNTxzv6uRy/sBEJPnz9fq686cHGO3MO/mYdME9HoakeaZkeeJmzhiZ74KufS1NnPdO42sesZ0YrbTYtbOS4jcfw4ev3V1oJU6Z8tkzlTA6zkuZY+eOev1kPNdEjLdMDL5u3bzqFmmPlT6o1fK+R9Jik+yY8/yZJT0raP/z5aIr9mlk5dfZIy/TAm7zoLsU1AjleNJiq5/854Grguinb/ElE/EKi/ZlZAnX3SNfbA28qb5/qzCjHyp8kwT8ibpW0LcVrWRo5zylSp3nfh769b7nUos9639dT+pzis0x1jUCOpdt15vzfKOke4BHg1yPiwOgGknYCOwG2bt1aY9O6JcfKgrXqCrDzvg+5v29VyKFHWsUVx9NeE4oH4ZRnRk2PO4yqq9rnW8CrI+K1wO8CN4zbKCL2RMRSRCxt2bKlpqZ1T46VBavqnOxq3vch5/etKl2thJn0mh/be2Cu378cc/Wp1NLzj4gfrLm9T9J/kbQ5Ir5Xx/77JsfKglV1Xmo/7/tQ1fuWeyqp6R5pFe/7pP975OgzL3ps2u9fDmdGVaml5y/pVZI0vH32cL+P17HvPsq5t1L0D72JCosq3rfcp/XNQRXv+7z/d9LvZQ5nRlVJVer5BeCbwOmSDkm6VNL7Jb1/uMnFwH3DnP9VwCURMbo+iSWS83TERf7QUwXMce+Dhq837gulivetj6mkeVXxvk96zRNetnHs9tO+LC7cvshtu87lO7vfwW27zu1E4Id01T7vmfH81QxKQa0G0yoLmk5BFDmNrqLCoshSmFVUZOScgstFFe/7pNcEOpvGmZdy7YAvLS3F8vJy083olNEKCBj84td9GjvrC+jUXTe+aNlKGPTav7P7Heva5zm7bx5b0ri4aYHbdp27rtfMeb9VaroDUVbb2z+LpLsiYmnWdp7eoUdyWZR81gBjFbXnntY3jS6UwzY9wJ0LT+xWoyaWklurzsHWMqrIATc1CN61AUOPYXSHe/41yaHHVKRHnUM7q8gB5zCtbxd4DKM7HPxrkkPKpc7B1rJSB8wcL69vo1ymg7DyHPzHqGJAKIceU5EAmEM7pynz2XSpB96UaR2Irg+kdo2D/4iq0h659JiaGGxNJeVn40C1PkVLKNs4ENw3HvAdUdWAVs4XXq2VcztTfTa+6raccRc9eSC4fdzzH1FV2iNlzrnKXmvOufFUn00u4xpdknu60F7MwX9ElWmPFDnnOqpxcs2Np/psHKjSyzldaOM57TMi57QH9LvOOtVnk/PEd7M0fQ3GJLn/3diLOfiPyP2inD73WlN9Nm0NVDmPVeT+d2Mv5rl9SmiiYqSLc8U0oY3VPn357Nv42eTEc/tUrKkrYV1nnUau4xrT1HHW1/TvUA5XmPeF0z7r1FTufdLpNZBtSsDSqHqsIoe0Up/HtOrmnv86NZl7H9drPWf3za0vX2y615m7qucnyqEEts9jWnVLtZLXNZIek3TfhOcl6SpJByXdK+l1KfbbpNwqRtr+R5NDrzN3VQ+q5vA7lNvfVZelSvt8DjhvyvNvB04b/uwEfi/RfhuTW8VI2/9ofLpfTJVLCubwO5Tb31WXJQn+EXEr8MSUTXYA18XA7cAmSSem2HdTcitta/sfTQ69zpRyrcefJoffodz+rrqsrpz/IvDwmvuHho89unYjSTsZnBmwdevWmpq2fjlVjOQ8LUMRKa8QbXrsoK0VK7n8DuX0d9VlWQ34RsQeYA8M6vwbbk7rtPGPZjVQjy6wDuvrdeYQeHMYOF2vNv4O2frUFfxXgFPW3D95+Jj12GigDnj+C2Bxnb3OHAJv0ymsps98rB3qCv57gcskfRF4PfBkRDw64/9Yx40L1KuBf71XrDYdeKHZSc5yOPOxdkhV6vkF4JvA6ZIOSbpU0vslvX+4yT7gIeAg8PvAr6bYr7VzYHFVFYG67xUrrpqyopL0/CPiPTOeD+CDKfZlP9H2Xl4VPeQmF2pf1eTAaQ5nPtYOWQ34VqWrOdAc8ttlVBGo+16x4nn1rajOB/+2946naXsvr6pAXWfgza1jkcOZj7VD54N/23vH09Tdy5sU6MoEwDaXFubYscjlzMfy1/ng3/be8TR19vImBbrlv3yCr9y1klUArEuuHYs2f6FafTo/pXMO1R9VqfNS+EmB7gt3PNzb6pIudyys+zrf8+96DrSuXt6kgPbchJXg+hAAPbg6WW5jIfZine/5e6KoNCYFtA3SXNun1PQ1DjlMhJYjT8/dDp3v+YNzoClMOoN6588vviDnv/p41QEwh8FWD66Ol+tYiL1QL4K/lTct0C29+qdqD4BNBhinNKbzWEg7OPhbYZPOoJo4s2oqwORwxpE7j4W0Q+dz/tZ+43L7TVVxee6c2TwW0g7u+Vst1psqmdTTbmqswSmN2TwW0g4O/la5MqmSST3tW/7sMFdcdGbtAcYpjWJcZJE/B3+rXJnB2Wk97SYCTNevG7H+cM7fKlcmVZLbFdq+bsS6wj1/q1yZVEmOPW2nNKwLUq3kdZ6kByUdlLRrzPPvk3RY0v7hz6+k2K+1Q5nqD/e0zapRuucvaQPwKeCtwCHgTkl7I+L+kU2vj4jLyu6vLF+gM16V70vZ6g/3tM3SS5H2ORs4GBEPAQwXad8BjAb/xjVxgU4bvmzqeF8cwM3ykiLtswg8vOb+oeFjo94p6V5JX5Z0yrgXkrRT0rKk5cOHDydo2gvVfYFOWya4Ws/70vSkamZWTl3VPv8D2BYR/wD4GnDtuI0iYk9ELEXE0pYtW5I3ou4LdNpyNei870tbvtTMbLIUwX8FWNuTP3n42PMi4vGIeHp49zPAzyfY79zqLhtsy9Wg874vbflSM7PJUgT/O4HTJJ0q6SXAJcDetRtIOnHN3QuABxLsd251zzmyni+bJtIp874vbflSM7PJSg/4RsSzki4DbgI2ANdExAFJHweWI2Iv8K8lXQA8CzwBvK/sftej7jlHitaorw4Krxw5ioDVtbHqmjFy3vfFUxyYtZ9iwjJ8TVtaWorl5eWmm1HarGqf0UqbcRY3LXDbrnPraG4h49q8sHGD6+/NMiDprohYmrWdr/Ct2KwSx3H581G5pVM8a6NZ+zn4N6zM/DZNct2+Wbt5YreGzQrsTc9jY2bd5J5/w8YNCq8O+i4mSqe04SrjsvpwjGYpOfg3ZG2wOn5hI8dtPIYjTz2TPHD1Yc3ZPhyjWWoO/nNI1bscDVZHjj7DwsYNfPLdZyUPVmUWUmmLPhyjWWrO+ReUckqDOq+Q7cMFWX04RrPUHPwLShmw6wxWua2EVYU+HKNZag7+Bc0TsGdN0VBnsKp7Sosm9OEYzVJz8C+oaMAukh6qM1hNWwmrK9Mye7Uvs/l5eoeCik5pcM7um8fOezM6RUPTpYlFj6fpdprZfDy9Q2JFpzQomh5q+grZIhUyLqE06y4H/6EiPdwiAbstM14W+ZJyCaVZdznnT9oyzrYMPhYZwyhbldSVMQWzLnLwJ00Z52qg+/D1+3npscdwwss2Zj34WORLqkxVkpd6NMubgz9perhrA92Ro8/wo2d+zCfffRa37To3u8APxSpkypzFeKlHs7wlyflLOg/4HQYreX0mInaPPP9S4DoGa/c+Drw7Ir6bYt8plM3TtzU3PmsMo8y8/b7q1ixvpYO/pA3Ap4C3AoeAOyXtjYj712x2KfD9iPgZSZcAnwDeXXbfqRRdbnGSLge69VYltWXg26yvUqR9zgYORsRDEfG3wBeBHSPb7ACuHd7+MvBmSUqw7yTKXiTk6QVerC0D32Z9lSLtswg8vOb+IeD1k7YZLvj+JPBK4HtrN5K0E9gJsHXr1gRNK65M3X3ZM4cu8lKPZnnLqs4/IvYAe2BwhW/DzSnMgW68pi9kM7PJUgT/FeCUNfdPHj42bptDko4Fjmcw8NsZDnRm1iYpcv53AqdJOlXSS4BLgL0j2+wF3ju8fTFwc+Q6qZCZWQ+U7vkPc/iXATcxKPW8JiIOSPo4sBwRe4HPAp+XdBB4gsEXhJmZNSRJzj8i9gH7Rh776JrbPwLelWJfZmZWnq/wNTPrIQd/M7MecvA3M+shB38zsx7K6iKvlLz8oJnZZJ0M/l5+0Mxsuk6mfTyXvJnZdJ0M/l2eYtnMLIVOBn9PsWxmNl0ng7/nkjczm66TA76eYtnMbLpOBn8oNsWyy0HNrK86G/xncTmomfVZJ3P+Rbgc1Mz6rLfB3+WgZtZnvU37nLRpgZUxgd7loOV4HMWsHUr1/CX9lKSvSfrz4b8nTNjuOUn7hz+jSzw2wuWg6a2Oo6wcOUrwk3GUG+4eXdLZzJpWNu2zC/h6RJwGfH14f5yjEXHW8OeCkvtM4sLti1xx0ZksblpAwOKmBa646Ez3UkvwOIpZe5RN++wA3jS8fS3wDeDflnzN2hQpB7XiPI5i1h5le/4/HRGPDm//P+CnJ2x3nKRlSbdLurDkPi1TnlbDrD1mBn9JfyzpvjE/O9ZuFxEBxISXeXVELAH/HPjPkv7+hH3tHH5JLB8+fHjeY7GGeRzFrD1mpn0i4i2TnpP015JOjIhHJZ0IPDbhNVaG/z4k6RvAduAvxmy3B9gDsLS0NOmLxDLlaTXM2qNszn8v8F5g9/DfPxrdYFgB9FREPC1pM3AO8B9L7tcy5XEUs3Yom/PfDbxV0p8DbxneR9KSpM8Mt/k5YFnSPcAtwO6IuL/kfs3MrIRSPf+IeBx485jHl4FfGd7+38CZZfZjZmZp9XZ6BzOzPnPwNzPrIQd/M7MecvA3M+shB38zsx5y8Dcz66HezudflOenN7MucvCfwuv8mllXOe0zheenN7OucvCfwvPTm1lXOfhP4fnpzayrHPyn8Pz0ZtZVHvCdwvPTm1lXOfjP4PnpzayLnPYxM+shB38zsx4qFfwlvUvSAUk/lrQ0ZbvzJD0o6aCkXWX2aWZm5ZXt+d8HXATcOmkDSRuATwFvB84A3iPpjJL7NTOzEsou4/gAgKRpm50NHIyIh4bbfhHYAXgdXzOzhtSR818EHl5z/9DwMTMza8jMnr+kPwZeNeap34qIP0rZGEk7gZ3Duz+UlGISnc3A9xK8Tlv4eLvNx9tdqY711UU2mhn8I+ItJRuyApyy5v7Jw8fG7WsPsKfk/l5A0nJETByM7hofb7f5eLur7mOtI+1zJ3CapFMlvQS4BNhbw37NzGyCsqWe/1TSIeCNwI2Sbho+fpKkfQAR8SxwGXAT8ADwpYg4UK7ZZmZWRtlqnz8E/nDM448A56+5vw/YV2ZfJSRNI7WAj7fbfLzdVeuxKiLq3J+ZmWXA0zuYmfVQZ4L/rCkkJL1U0vXD5++QtK3+VqZT4Hj/jaT7Jd0r6euSCpV/5aroFCGS3ikppk03krsixyrpnw0/3wOS/lvdbUypwO/yVkm3SLp7+Pt8/rjXaQtJ10h6TNJ9E56XpKuG78e9kl5XSUMiovU/wAbgL4C/B7wEuAc4Y2SbXwU+Pbx9CXB90+2u+Hj/MfCy4e0PdP14h9u9gsFUI7cDS023u8LP9jTgbuCE4f2/23S7Kz7ePcAHhrfPAL7bdLtLHvM/BF4H3Dfh+fOB/wUIeANwRxXt6ErP//kpJCLib4HVKSTW2gFcO7z9ZeDNmjEvRcZmHm9E3BIRTw3v3s7g+oq2KvL5AvwH4BPAj+psXGJFjvVfAp+KiO8DRMRjNbcxpSLHG8DfGd4+HnikxvYlFxG3Ak9M2WQHcF0M3A5sknRi6nZ0JfgXmULi+W1iUH76JPDKWlqX3rxTZlzKoCfRVjOPd3hqfEpE3FhnwypQ5LP9WeBnJd0m6XZJ59XWuvSKHO/HgF8clpXvA/5VPU1rTC1T4nglr46T9IvAEvCPmm5LVSQdA/w28L6Gm1KXYxmkft7E4IzuVklnRsSRRltVnfcAn4uI/yTpjcDnJb0mIn7cdMParCs9/yJTSDy/jaRjGZw+Pl5L69IrNGWGpLcAvwVcEBFP19S2Ksw63lcArwG+Iem7DPKke1s66Fvksz0E7I2IZyLiO8D/ZfBl0EZFjvdS4EsAEfFN4DgG8+B0VeEpccroSvAvMoXEXuC9w9sXAzfHcHSlhWYer6TtwH9lEPjbnBOGGccbEU9GxOaI2BYR2xiMcVwQEcvNNLeUIr/LNzDo9SNpM4M00EN1NjKhIsf7V8CbAST9HIPgf7jWVtZrL/AvhlU/bwCejIhHU++kE2mfiHhW0uoUEhuAayLigKSPA8sRsRf4LIPTxYMMBlsuaa7F5RQ83iuBlwP/fTiu/VcRcUFjjS6h4PF2QsFjvQn4J5LuB54DLo+IVp7FFjzejwC/L+nDDAZ/39fijhuSvsDgy3vzcBzj3wEbASLi0wzGNc4HDgJPAb9cSTta/B6amdk6dSXtY2Zmc3DwNzPrIQd/M7MecvA3M+shB38zsx5y8Dcz6yEHfzOzHnLwNzProf8PCnoRWfzkjZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a88290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x = np.linspace(0, 1, m)\n",
    "y = np.random.normal(loc=2.0*x+0.3, scale=0.5, size=m)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run regression using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   106.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 24 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>2.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:56:10</td>     <th>  Log-Likelihood:    </th> <td> -84.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   173.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   178.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3065</td> <td>    0.113</td> <td>    2.710</td> <td> 0.008</td> <td>    0.082</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0140</td> <td>    0.195</td> <td>   10.306</td> <td> 0.000</td> <td>    1.626</td> <td>    2.402</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.753</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.252</td> <th>  Jarque-Bera (JB):  </th> <td>   1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.035</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    4.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.520\n",
       "Model:                            OLS   Adj. R-squared:                  0.515\n",
       "Method:                 Least Squares   F-statistic:                     106.2\n",
       "Date:                Sun, 24 Feb 2019   Prob (F-statistic):           2.63e-17\n",
       "Time:                        18:56:10   Log-Likelihood:                -84.642\n",
       "No. Observations:                 100   AIC:                             173.3\n",
       "Df Residuals:                      98   BIC:                             178.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3065      0.113      2.710      0.008       0.082       0.531\n",
       "x1             2.0140      0.195     10.306      0.000       1.626       2.402\n",
       "==============================================================================\n",
       "Omnibus:                        2.753   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                1.746\n",
       "Skew:                           0.035   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.356   Cond. No.                         4.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(len(x)), x])\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's see if the formulas we derived for $\\hat{\\beta}$ and $\\hat{\\alpha}$ match the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha = 0.306537621742\n",
      "Reproduced  Alpha = 0.306537621742\n",
      "statsmodels Beta  = 2.01403383001\n",
      "Reproduced  Beta  = 2.01403383001\n",
      "Alpha = 0.306537621742\n"
     ]
    }
   ],
   "source": [
    "beta = np.cov(x, y, bias=True)[0, 1] / np.var(x)\n",
    "alpha = np.mean(y) - beta * np.mean(x)\n",
    "print(\"statsmodels Alpha = {}\".format(model.params[0]))\n",
    "print(\"Reproduced  Alpha = {}\".format(alpha))\n",
    "print(\"statsmodels Beta  = {}\".format(model.params[1]))\n",
    "print(\"Reproduced  Beta  = {}\".format(beta))\n",
    "print(\"Alpha = {}\".format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$ are quite close to the actual model parameters $\\beta=2$ and $\\alpha=0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "R-squared (usually denoted $R^2$) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and $f^{(i)} = f(x^{(i)})$. R-squared compares the error in prediction $y^{(i)} - f^{(i)}$ to that of the trivial model $f=\\bar{y}$. The smaller the error of prediction, the closer $R^2$ is to $1$. In that sense, it measures the proportion of variance in the dependent variable $y$ that is predictable from the independent variable $x$ using the model $f$. Note, however, that this is an *in-sample* measure of goodness-of-fit, if the same dataset is used to fit the parameters of the model *and* compute $R^2$.\n",
    "\n",
    "Let's confirm $R^2$ for our linear regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.520089566727\n",
      "Reproduced  R-squared = 0.520089566727\n"
     ]
    }
   ],
   "source": [
    "f = alpha + beta * x\n",
    "e = f - y\n",
    "y_c = y - y.mean()\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y_c, y_c)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model $f$ is linear regression with an intercept term, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! Let's prove this.\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "S_y^2   = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2 ,\\qquad\n",
    "r_{xy}  = \\frac{C_{xy}}{S_xS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where $S_y$ is the (biased) sample variance of $y$ and $r_{xy}$ is the sample correlation coefficient of $x$ and $y$. Using this, we can express $\\hat{\\beta}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{r_{xy}S_y}{S_x}.\n",
    "\\end{equation}\n",
    "\n",
    "Also note that:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} - \\bar{y} = \\hat{\\alpha} + \\hat{\\beta}x^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x}),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$. The average of squared residuals can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - y^{(i)})^2\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m[(f^{(i)} - \\bar{y}) - (y^{(i)} - \\bar{y}) ]^2 \\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})^2 + \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\frac{2}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\frac{\\hat{\\beta}^2}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})^2 + S_y^2 - \\frac{\\hat{\\beta}}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\hat{\\beta}^2 S_x^2 + S_y^2 - 2\\hat{\\beta}C_{xy} \\\\\n",
    "&= S_y^2(1 - r_{xy}^2),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$, and the last one from substituting the value of $\\hat{\\beta}$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2} \\\\\n",
    "    &= 1 - \\frac{S_y^2(1 - r_{xy}^2)}{S_y^2} \\\\\n",
    "    &= r_{xy}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We proved that $R^2$ is given by the square of the sample correlation between $x$ and $y$, and not $f$ and $y$ as originally promised! First, let's numerically check what we just proved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's do the extra work and show that $r_{fy}^2 = r_{xy}^2$.\n",
    "\n",
    "To do this, we'll need one more result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m f^{(i)}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)})\n",
    "    = \\hat{\\alpha} + \\hat{\\beta}\\bar{x}\n",
    "    = \\bar{y},\n",
    "\\end{equation}\n",
    "where the last line follows from $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}$.\n",
    "\n",
    "Now we're ready:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{fy}\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{y})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sqrt{\\hat{\\beta}^2\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}}{|\\hat{\\beta}|}r_{xy},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\bar{f}=\\bar{y}$ and the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$. There we have it:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = r_{fy}^2 = r_{xy}^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "In most realistic cases, we cannot hope to explain all variability in $y$ using only $x$. This is also true for models which are more complex than linear regression. We should always expect a certain degree of randomness that our models cannot account for. However, we can hope to design models that make the correct predictions *on average*. This suggests a probabilistic approach for modelling. We can think about $y$ as a random variable with a certain probability distribution, whose mean is given by the model $f(x)$.\n",
    "\n",
    "For simplicity, let's assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing $(x^{(i)}, y^{(i)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P^{(i)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we're assuming all observations are independent, the probability of observing the whole dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\Pi_{i=1}^m P^{(i)} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\alpha, \\beta)}{\\sigma^2}\\right)},\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is the cost function defined earlier. This is interesting! The probability of observing the data set can be expressed in terms of the cost function. In fact, the parameters $\\alpha$ and $\\beta$ that minimize $J$ also maximize $P$. Picking the parameters of a model to maximize the probability of observing the dataset is called *maximum likelihood estimation*.\n",
    "\n",
    "*Log-likelihood* is given by $\\log P$, which in our case is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\alpha, \\beta) = -\\frac{m}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m(y^{(i)} - \\alpha - \\beta x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the model summary above from `statsmodels`, a number is quoted for Log-Likelihood. To reproduce it, let's compute $\\log P(\\hat{\\alpha}, \\hat{\\beta})$. One thing, though, is that $\\log P$ depends on $\\sigma$, which `statsmodels` doesn't take as input. It can, however, esimate it from the sample standard deviation of the prediction errors $y^{(1)} - f^{(1)}, \\dots, y^{(m)} - f^{(m)}$. Let's give that a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -84.6424357588\n",
      "Reproduced  Log-Likelihood = -84.6424357588\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(len(x)/2.0)*np.log(2*np.pi*np.var(e)) - np.dot(y - f, y - f)/(2*np.var(e))\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "Given the probabilistic view, we now see that the optimal estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$ should be regarded as random variables, and not confused with the \"real\" underlying parameters $\\alpha$ and $\\beta$. A different dataset, for instance, would result in different values for $\\hat{\\alpha}$ and $\\hat{\\beta}$. Therefore, it's important to study their variability.\n",
    "\n",
    "Let's first check their means. Note that by assumption\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "and as a result\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\bar{y} \\rangle = \\frac{1}{m} \\sum_{i=1}^m \\langle y^{(i)} \\rangle = \\frac{1}{m} \\sum_{i=1}^m (\\alpha + \\beta x^{(i)}) = \\alpha + \\beta \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Combining these:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle(y^{(i)} - \\bar{y})\\rangle = \\beta (x^{(i)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these results, we can compute the mean of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta} \\rangle\n",
    "= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})\\langle(y^{(i)} - \\bar{y})\\rangle\n",
    "= \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x}) = \\beta,\n",
    "\\end{equation}\n",
    "\n",
    "and also that of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\alpha} \\rangle = \\langle \\bar{y} \\rangle - \\langle \\hat{\\beta} \\rangle \\bar{x} = \\alpha + \\beta \\bar{x} - \\beta \\bar{x} = \\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "So the mean of $\\hat{\\beta}$ and $\\hat{\\alpha}$ are $\\beta$ and $\\alpha$, respectively. Good, but no terribly surprising!\n",
    "\n",
    "Let's compute the variance of $\\hat{\\beta}$. First, we will express $\\hat{\\beta} - \\beta$ in a convenient way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) - \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})[(y^{(i)} - \\beta x^{(i)} - \\alpha) - (\\bar{y} - \\beta \\bar{x} - \\alpha)] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle) - \\frac{\\bar{y} - \\beta \\bar{x} - \\alpha}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "        &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle)\n",
    "\\end{align*}\n",
    "\n",
    "where the first equality follows from the definition of $S_x^2$, the second from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, and the third from $\\sum_{i=1}^m (x^{(i)} - \\bar{x})=0$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\\\\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\sigma^2\\delta_{ij} \\\\\n",
    "&= \\frac{\\sigma^2}{m^2S_x^4}\\sum_{i}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "&= \\frac{\\sigma^2}{mS_x^2},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from the fact that $y^{(1)}, \\dots, y^{(m)}$ are (by assumption) independent and normally distributed with variance $\\sigma^2$.\n",
    "\n",
    "Let's check this against `statsmodels`. The estimated value of $\\sigma^2$ (since the real value is unknown to `statsmodels`), is stored in the `scale` attribute of `model`. Using that, we can reproduce the standard error of $\\hat{\\beta}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta std err = 0.195431582311\n",
      "Reproduced  Beta std err = 0.195431582311\n"
     ]
    }
   ],
   "source": [
    "beta_se = np.sqrt(model.scale / (m * np.var(x)))\n",
    "print(\"statsmodels Beta std err = {}\".format(model.bse[1]))\n",
    "print(\"Reproduced  Beta std err = {}\".format(beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute $\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle$. First, note that we can express $\\hat{\\alpha} - \\alpha$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha\n",
    "    = \\bar{y} - (\\hat{\\beta} - \\beta)\\bar{x} - \\beta\\bar{x} - \\alpha\n",
    "    = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "To compute the variance of $\\hat{\\alpha} - \\alpha$, we will need the covariance between $\\hat{\\beta} - \\beta$ and $\\bar{y} - \\langle \\bar{y} \\rangle$. We can express $\\bar{y} - \\langle \\bar{y} \\rangle$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\bar{y} - \\beta \\bar{x} - \\alpha = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\beta x^{(j)} - \\alpha) = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\left\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\right\\rangle \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\langle(\\bar{y} - \\langle \\bar{y} \\rangle)^2\\right\\rangle\n",
    "=\\frac{1}{m^2}\\sum_{k,l=1}^m \\left\\langle(y^{(k)} - \\langle y^{(k)} \\rangle)(y^{(l)} - \\langle y^{(l)} \\rangle)\\right\\rangle\n",
    "= \\frac{1}{m^2}\\sum_{k,l=1}^m \\sigma^2\\delta_{kl}\n",
    "= \\frac{\\sigma^2}{m}.\n",
    "\\end{equation}\n",
    "This is exactly what we should've expected without doing any work, since $\\bar{y} - \\langle \\bar{y} \\rangle \\sim \\mathcal{N}(0, \\sigma^2/m)$ follows from the fact that $\\bar{y}$ is an average of independent and identical normally distributed random variables.\n",
    "\n",
    "Now we can compute the variance of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle =\n",
    "\\left\\langle (\\bar{y} - \\langle \\bar{y} \\rangle)^2 \\right\\rangle + \\bar{x}^2 \\left\\langle (\\hat{\\beta} - \\beta)^2 \\right\\rangle\n",
    "= \\frac{\\sigma^2}{m} + \\bar{x}^2\\frac{\\sigma^2}{mS_x^2}\n",
    "= \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha std err = 0.113117048297\n",
      "Reproduced  Alpha std err = 0.113117048297\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(model.scale / m * (1 + np.mean(x)**2/np.var(x)))\n",
    "print(\"statsmodels Alpha std err = {}\".format(model.bse[0]))\n",
    "print(\"Reproduced  Alpha std err = {}\".format(alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation above also makes it easy to derive the covariance between $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)(\\hat{\\beta} - \\beta) \\rangle\n",
    "= \\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle - \\bar{x} \\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "= 0 - \\bar{x} \\frac{\\sigma^2}{mS_x^2}\n",
    "= -\\frac{\\sigma^2\\bar{x}}{mS_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Again, we get the same number as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha-beta covariance = -0.0190967516823\n",
      "Reproduced  alpha-beta covariance = -0.0190967516823\n"
     ]
    }
   ],
   "source": [
    "cov_params = - model.scale * np.mean(x)/(m*np.var(x))\n",
    "print(\"statsmodels alpha-beta covariance = {}\".format(model.cov_params()[0, 1]))\n",
    "print(\"Reproduced  alpha-beta covariance = {}\".format(cov_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we glossed over was how `statsmodels` estimates $\\sigma^2$. Let's think about this a little. Since, by assumption, $y{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)$, it makes sense to use the sample variance of the prediction error $e_i=y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)}=y^{(i)} - f^{(i)}$ as an estimate. We can express $e_i$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "e_i &= y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\hat{\\alpha} - \\alpha) - (\\hat{\\beta} - \\beta)x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\sum_{j=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)\\left[\\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}) \\right] \\\\\n",
    "    &= \\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, the third from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$, the fourth from our earlier expressions for $\\bar{y} - \\langle \\bar{y} \\rangle$ and $\\hat{\\beta} - \\beta$, and in the last equality we have introduced the matrix $A$ whose elements are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} \\equiv \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "$A$ has a few interesting properties. First, it's symmetric: $A_{ij} = A_{ij}$. Second, it's equal to its square: $A^2=A$. (This can be shown by carrying out the matrix multiplication between $A$ and itself. It's a bit tedious and not terribly interesting, so I'll leave it out.) Using these results, we can compute the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m} e_i^2\n",
    "    &= \\sum_{i=1}^{m}\\left[\\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle)\\right]^2 \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\left[\\sum_{i=1}^{m}A_{ij}A_{ik}\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle) A_{jk},\n",
    "\\end{align*}\n",
    "\n",
    "where the last equality follows from the fact that $A$ is symmetric and $A^2=A$. Taking the expectation value of both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle\\sum_{i=1}^{m} e_i^2 \\right\\rangle\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\left\\langle(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(A),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{tr}(A)=\\sum_{i=1}^{m}A_{ii}$ denotes the trace of $A$. It's easy to show that $\\text{tr}(A)=m-2$, so the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m-2}\\sum_{i=1}^{m}e_i^2 = \\frac{1}{m-2}\\sum_{i=1}^m(y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.324709077426\n",
      "Reproduced  estimate of sigma^2 = 0.324709077426\n"
     ]
    }
   ],
   "source": [
    "var_est = np.var(e)*m/(m-2)\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(var_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "We've worked out the variance (and covariance) of the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. It would also be nice to have confidence intervals for them, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta_L, \\beta_U)$ contains $\\beta$. We've done this sort of analysis for the sample mean when discussing the [t-distribution](https://github.com/siavashaslanbeigi/stats_notes/blob/master/t.ipynb). Can we do something similar for $\\hat{\\alpha}$ and $\\hat{\\beta}$? Yes, but it's a bit more involved than dealing with the sample mean.\n",
    "\n",
    "\n",
    "Above we showed that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2/(m S_x^2))$, or equivalently $\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma \\sim \\mathcal{N}(0, 1)$. This means $\\beta_L=\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x}$ and $\\beta_U=\\hat{\\beta}+z\\frac{\\sigma}{\\sqrt{m} S_x}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x} \\le \\beta \\le \\hat{\\beta}+z \\frac{\\sigma}{\\sqrt{m} S_x})\n",
    "= P(-z \\le \\frac{\\hat{\\beta} - \\beta}{\\sigma/(\\sqrt{m} S_x)} \\le z )\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\hat{\\sigma}^2 / (m S_x^2)}}\n",
    "    = \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)}{\\sigma}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\sum_{i=1}^{m}e_i^2.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. In what follows, we will show that $V$ has a chi-squared distribution with $m-2$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "We will make use (one of the formulations of) [Cochran's theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem), which states the following: suppose $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an $m$-dimensional multivariate normal random vector, and that $B^{(1)}, \\dots, B^{(k)}$ are symmetric $m \\times m$ matrices which satisfy the following properties:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{k}B^{(i)} = I, \\qquad\n",
    "\\sum_{i=1}^{k}r_i = m,\n",
    "\\end{equation}\n",
    "where $I$ is the $m \\times m$ identity matrix and $r_i$ denotes the rank of $B_i$. Then, the random variable $Q^{(i)}=Y^TB^{(i)}Y$ is distributed as $\\sigma^2 \\chi^2_{r_i}$, where $\\chi^2_{r_i}$ is the chi-squared distribution with $r_i$ degrees of freedom, and $Q^{(i)}$ and $Q^{(j)}$ are independent for all $i\\neq j$.\n",
    "\n",
    "\n",
    "It's not immediately obvious how this theorem will help us arrive at confidence intervals, but it will. First, note that if we define $Y$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i \\equiv y^{(i)} - \\langle y^{(i)} \\rangle.\n",
    "\\end{equation}\n",
    "then $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$. With this definition, we can rewrite the sum of squared errors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}e_i^2 = Y^TAY.\n",
    "\\end{equation}\n",
    "\n",
    "Now this looks like a $Q$ matrix from Cochran's theorem. What is the rank of $A$? Remember that the rank of a matrix is the dimension of the vector space spanned by its column vectors. It can be shown that this is equal to the number of non-zero eigenvalues. Because $A^2=A$, eigenvalues of $A$ are either $1$ or $0$. To see why, let $v$ denote an eigenvector of $A$ with eigenvalue $\\lambda$: $Av=\\lambda v$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda v = Av = A^2v=A(Av)=A(\\lambda v)=\\lambda Av=\\lambda^2v,\n",
    "\\end{equation}\n",
    "\n",
    "which implies $\\lambda^2=\\lambda$, which has the solutions $\\lambda=0$ and $\\lambda=1$. Therefore, for $A$, the number of non-zero eigenvalues is simply the sum of its eigenvalues, which is also its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Rank}(A) = \\text{tr}(A) = m - 2.\n",
    "\\end{equation}\n",
    "Let's summarize what we have so far\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(1)} \\equiv \\sum_{i=1}^{m}e_i^2 = Y^TB^{(1)}Y, \\qquad\n",
    "B^{(1)}_{ij} = \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}), \\qquad\n",
    "r_1 = m - 2.\n",
    "\\end{equation}\n",
    "\n",
    "Next we'll turn our attention to $\\hat{\\beta} - \\beta$. We had previous shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} - \\beta = \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(2)} \\equiv m S_x^2(\\hat{\\beta} - \\beta)^2 = Y^TB^{(2)}Y, \\qquad\n",
    "B^{(2)}_{ij} = \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Again, it can be checked that $B^{(2)}$ is equal to its own square, so its rank is given by its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "r_2 = \\text{Rank}(B^{(2)}) = \\text{tr}(B^{(2)}) = \\sum_{i=1}^{m} B^{(2)}_{ii} = \\frac{1}{m S_x^2}\\sum_{i=1}^{m} (x^{(i)} - \\bar{x})^2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, as we showed in previous section\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(3)} \\equiv m (\\bar{y} - \\langle \\bar{y} \\rangle)^2 = Y^TB^{(3)}Y, \\qquad\n",
    "B^{(3)}_{ij} = \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Since all columns of $B^{(3)}$ are the same, its rank is $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "It's now easy to check that $B^{(1)}$, $B^{(2)}$, and $B^{(3)}$ satisfy the conditions of Cochran's theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(1)}_{ij} + B^{(2)}_{ij} + B^{(3)}_{ij} = \\delta_{ij}, \\qquad\n",
    "r_1 + r_2 + r_3 = m.\n",
    "\\end{equation}\n",
    "\n",
    "Cochran's theorem then implies the following:\n",
    "\n",
    "* $\\sum_{i=1}^{m}e_i^2 \\sim \\sigma^2 \\chi^2_{m-2}$.\n",
    "* $\\sum_{i=1}^{m}e_i^2$, $(\\hat{\\beta} - \\beta)^2$, and $(\\bar{y} - \\langle \\bar{y} \\rangle)^2$ are mutually independent, which implies $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are mutually independent.\n",
    "\n",
    "This in turn implies that $V$ has a $\\chi^2_{m-2}$ distribution and that $Z$ and $V$ are independent. Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}} = \\frac{\\hat{\\beta} - \\beta}{s_{\\hat{\\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "has the t-distribution with $m-2$ degrees of freedom, where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\beta}}^2 = \\frac{\\hat{\\sigma}^2}{m S_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally: $\\beta_L=\\hat{\\beta}-ts_{\\hat{\\beta}}$ and $\\beta_U=\\hat{\\beta}+ts_{\\hat{\\beta}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta 95% confidence interval = (1.62620621534, 2.40186144467)\n",
      "Reproduced  beta 95% confidence interval = (1.62620621534, 2.40186144467)\n"
     ]
    }
   ],
   "source": [
    "t = stats.t.ppf(1 - 0.05/2., m - 2)\n",
    "beta_se = np.sqrt(var_est / (m * np.var(x)))\n",
    "print(\"statsmodels beta 95% confidence interval = ({0}, {1})\".format(model.conf_int()[1, 0], model.conf_int()[1, 1]))\n",
    "print(\"Reproduced  beta 95% confidence interval = ({0}, {1})\".format(beta - t*beta_se, beta + t*beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the confidence interval for $\\hat{\\alpha}$. Earlier we had shown that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} \\sim \\mathcal{N}\\left(\\alpha, \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As before, we divide $\\hat{\\alpha} - \\alpha$ by its standard deviation and replace $\\sigma$ with $\\hat{\\sigma}$:\n",
    "\\begin{equation}\n",
    "T_{\\alpha}\n",
    "    \\equiv \\frac{\\hat{\\alpha} - \\alpha}{s_{\\hat{\\alpha}}}\n",
    "    = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2)}}\n",
    "    = \\frac{\\sqrt{m}(\\hat{\\alpha} - \\alpha)/\\sqrt{\\sigma^2(1+\\bar{x}^2/S_x^2)}}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z_{\\alpha}}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\alpha}} \\equiv \\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2), \\qquad\n",
    "Z_{\\alpha} = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that $Z_{\\alpha} \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi^2_{m-2}$. If $Z_{\\alpha}$ and $V$ are independent, then $T_{\\alpha}$ also has a t-distribution with $m-2$ degrees of freedom. We had shown earlier that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are independent, it follows that $\\hat{\\alpha} - \\alpha$ and $\\sum_{i=1}^{m}e_i^2$, and in turn $Z_{\\alpha}$ and $V$, are independent. As a result, $\\alpha_L=\\hat{\\alpha}-ts_{\\hat{\\alpha}}$ and $\\alpha_U=\\hat{\\alpha}+ts_{\\hat{\\alpha}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n",
      "Reproduced  alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(var_est / m * (1 + np.mean(x)**2/np.var(x)))\n",
    "print(\"statsmodels alpha 95% confidence interval = ({0}, {1})\".format(model.conf_int()[0, 0], model.conf_int()[0, 1]))\n",
    "print(\"Reproduced  alpha 95% confidence interval = ({0}, {1})\".format(alpha - t*alpha_se, alpha + t*alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using $\\hat{\\alpha}$ and $\\hat{\\beta}$, so our predictions are also subject to statistical noise. Let's compute the variance of $\\hat{f}=\\hat{\\alpha} + \\hat{\\beta} x$. First, its mean is what we expect it to be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{f} \\rangle = \\langle\\hat{\\alpha}\\rangle + \\langle\\hat{\\beta}\\rangle x = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "As a result:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f} - \\langle \\hat{f} \\rangle\n",
    "    &= (\\hat{\\alpha} - \\alpha) + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}  + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) + (\\hat{\\beta} - \\beta)(x - \\bar{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$. Since $\\bar{y}$ and $\\hat{\\beta}$ are independent, the variance of $\\hat{f}$ is given simply by:\n",
    "\n",
    "The variance is given by:\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{f} - \\langle \\hat{f} \\rangle)^2 \\right\\rangle\n",
    "    &= \\langle (\\bar{y} - \\langle \\bar{y}\\rangle)^2 + (x - \\bar{x})^2 \\langle(\\hat{\\beta} - \\beta)^2 \\rangle \\\\\n",
    "    &= \\frac{\\sigma^2}{m} + (x - \\bar{x})^2 \\frac{\\sigma^2}{m S_x^2} \\\\\n",
    "    &= \\frac{\\sigma^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "The same argument we used for deriving the confidence interval of $\\hat{\\alpha}$ can be used for $\\hat{f}$: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom, and:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\frac{\\hat{\\sigma}^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can use `seaborn.regplot` to plot the confidence interval for $f$. Below we also show the $95\\%$ confidence interval computed using the formula we just derived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b7a4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard error of f\n",
    "x_axis = np.linspace(0, 1, 1000)\n",
    "f_se = np.sqrt(var_est/m*(1 + (x_axis - x.mean())**2/np.var(x)))\n",
    "f_l = alpha + beta * x_axis - t * f_se\n",
    "f_u = alpha + beta * x_axis + t * f_se\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_axis, f_l, 'r--')\n",
    "plt.plot(x_axis, f_u, 'r--', label='Reproduced 95% confidence interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Plot using seaborn\n",
    "seaborn.regplot(x, y);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
