{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Consider the data set $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$, where $x$ is the independent variable and $y$ is the dependent variable. We will model this data set as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "What are the values of $\\alpha$ and $\\beta$ that provide the best fit to the data set? To answer this question, we minimize the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\alpha, \\beta) &= \\frac{1}{2m}\\sum_{i=1}^N (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "                 &= \\frac{1}{2m}\\sum_{i=1}^N (\\alpha + \\beta x^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "$J(\\alpha, \\beta)$ is called the *cost function*, or *objective function*. Let's minimize the cost function with respect to $\\alpha$ and $\\beta$. We will denote the optimal values by $\\hat{\\beta}$ and $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial\\alpha} = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)}) = 0 \\\\\n",
    "\\frac{\\partial J}{\\partial\\beta}  = 0 &\\to \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)} - y^{(i)})x^{(i)} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)} ,\\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y^{(i)} ,\\qquad\n",
    "S_x^2   = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 ,\\qquad\n",
    "C_{xy}  = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "These quantities should be familiar: $\\bar{x}$ is the sample mean of $x$, $\\bar{y}$ is the sample mean of $y$, $S_x^2$ is the (biased) sample variance of $x$, and $C_{xy}$ is the (biased) sample covariance of $x$ and $y$. Using these definitions, it can be shown after straightforward algebra that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{C_{xy}}{S_x^2}, \\qquad\n",
    "\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data using the model $f(x)=0.3 + 2x$ and Gaussian noise with $\\sigma=0.5$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116dfc290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x = np.linspace(0, 1, m)\n",
    "y = np.random.normal(loc=2.0*x+0.3, scale=0.5, size=m)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run regression using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   106.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 24 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>2.63e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:25:56</td>     <th>  Log-Likelihood:    </th> <td> -84.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   173.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   178.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3065</td> <td>    0.113</td> <td>    2.710</td> <td> 0.008</td> <td>    0.082</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.0140</td> <td>    0.195</td> <td>   10.306</td> <td> 0.000</td> <td>    1.626</td> <td>    2.402</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.753</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.252</td> <th>  Jarque-Bera (JB):  </th> <td>   1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.035</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    4.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.520\n",
       "Model:                            OLS   Adj. R-squared:                  0.515\n",
       "Method:                 Least Squares   F-statistic:                     106.2\n",
       "Date:                Sun, 24 Feb 2019   Prob (F-statistic):           2.63e-17\n",
       "Time:                        19:25:56   Log-Likelihood:                -84.642\n",
       "No. Observations:                 100   AIC:                             173.3\n",
       "Df Residuals:                      98   BIC:                             178.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3065      0.113      2.710      0.008       0.082       0.531\n",
       "x1             2.0140      0.195     10.306      0.000       1.626       2.402\n",
       "==============================================================================\n",
       "Omnibus:                        2.753   Durbin-Watson:                   1.975\n",
       "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                1.746\n",
       "Skew:                           0.035   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.356   Cond. No.                         4.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression.linear_model.OLS(y, np.column_stack([np.ones(len(x)), x])).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's see if the formulas we derived for $\\hat{\\beta}$ and $\\hat{\\alpha}$ match the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha = 0.306537621742\n",
      "Reproduced  Alpha = 0.306537621742\n",
      "\n",
      "statsmodels Beta  = 2.01403383001\n",
      "Reproduced  Beta  = 2.01403383001\n"
     ]
    }
   ],
   "source": [
    "xmean = np.mean(x)\n",
    "xvar = np.var(x)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "beta = np.cov(x, y, bias=True)[0, 1] / xvar\n",
    "alpha = ymean - beta * xmean\n",
    "print(\"statsmodels Alpha = {}\".format(model.params[0]))\n",
    "print(\"Reproduced  Alpha = {}\".format(alpha))\n",
    "print(\"\")\n",
    "print(\"statsmodels Beta  = {}\".format(model.params[1]))\n",
    "print(\"Reproduced  Beta  = {}\".format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$ are quite close to the actual model parameters $\\beta=2$ and $\\alpha=0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "R-squared (usually denoted $R^2$) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and $f^{(i)} = f(x^{(i)})$. R-squared compares the error in prediction $y^{(i)} - f^{(i)}$ to that of the trivial model $f=\\bar{y}$. The smaller the error of prediction, the closer $R^2$ is to $1$. In that sense, it measures the proportion of variance in the dependent variable $y$ that is predictable from the independent variable $x$ using the model $f$. Note, however, that this is an *in-sample* measure of goodness-of-fit, if the same dataset is used to fit the parameters of the model *and* compute $R^2$.\n",
    "\n",
    "Let's confirm $R^2$ for our linear regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.520089566727\n",
      "Reproduced  R-squared = 0.520089566727\n"
     ]
    }
   ],
   "source": [
    "f = alpha + beta * x\n",
    "e = f - y\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model $f$ is linear regression with an intercept term, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed! Let's prove this.\n",
    "\n",
    "Consider the following definitions:\n",
    "\n",
    "\\begin{equation}\n",
    "S_y^2   = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2 ,\\qquad\n",
    "r_{xy}  = \\frac{C_{xy}}{S_xS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where $S_y$ is the (biased) sample variance of $y$ and $r_{xy}$ is the sample correlation coefficient of $x$ and $y$. Using this, we can express $\\hat{\\beta}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = \\frac{r_{xy}S_y}{S_x}.\n",
    "\\end{equation}\n",
    "\n",
    "Also note that:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} - \\bar{y} = \\hat{\\alpha} + \\hat{\\beta}x^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x}),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$. The average of squared residuals can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - y^{(i)})^2\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m[(f^{(i)} - \\bar{y}) - (y^{(i)} - \\bar{y}) ]^2 \\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})^2 + \\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\frac{2}{m}\\sum_{i=1}^m(f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\frac{\\hat{\\beta}^2}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})^2 + S_y^2 - \\frac{\\hat{\\beta}}{m}\\sum_{i=1}^m(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) \\\\\n",
    "&= \\hat{\\beta}^2 S_x^2 + S_y^2 - 2\\hat{\\beta}C_{xy} \\\\\n",
    "&= S_y^2(1 - r_{xy}^2),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$, and the last one from substituting the value of $\\hat{\\beta}$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2} \\\\\n",
    "    &= 1 - \\frac{S_y^2(1 - r_{xy}^2)}{S_y^2} \\\\\n",
    "    &= r_{xy}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We proved that $R^2$ is given by the square of the sample correlation between $x$ and $y$, and not $f$ and $y$ as originally promised! First, let's numerically check what we just proved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200895667266315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(x, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's do the extra work and show that $r_{fy}^2 = r_{xy}^2$.\n",
    "\n",
    "To do this, we'll need one more result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m f^{(i)}\n",
    "    = \\frac{1}{m}\\sum_{i=1}^m (\\hat{\\alpha} + \\hat{\\beta}x^{(i)})\n",
    "    = \\hat{\\alpha} + \\hat{\\beta}\\bar{x}\n",
    "    = \\bar{y},\n",
    "\\end{equation}\n",
    "where the last line follows from $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\bar{x}$.\n",
    "\n",
    "Now we're ready:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{fy}\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\sum_{i=1}^m (f^{(i)} - \\bar{y})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (f^{(i)} - \\bar{y})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sqrt{\\hat{\\beta}^2\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2}} \\\\\n",
    "&= \\frac{\\hat{\\beta}}{|\\hat{\\beta}|}r_{xy},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\bar{f}=\\bar{y}$ and the third equality follows from $f^{(i)} - \\bar{y} = \\hat{\\beta}(x^{(i)}-\\bar{x})$. There we have it:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = r_{fy}^2 = r_{xy}^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "In most realistic cases, we cannot hope to explain all variability in $y$ using only $x$. This is also true for models which are more complex than linear regression. We should always expect a certain degree of randomness that our models cannot account for. However, we can hope to design models that make the correct predictions *on average*. This suggests a probabilistic approach for modelling. We can think about $y$ as a random variable with a certain probability distribution, whose mean is given by the model $f(x)$.\n",
    "\n",
    "For simplicity, let's assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing $(x^{(i)}, y^{(i)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P^{(i)} = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we're assuming all observations are independent, the probability of observing the whole dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\Pi_{i=1}^m P^{(i)} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\alpha - \\beta x^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\alpha, \\beta)}{\\sigma^2}\\right)},\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is the cost function defined earlier. This is interesting! The probability of observing the data set can be expressed in terms of the cost function. In fact, the parameters $\\alpha$ and $\\beta$ that minimize $J$ also maximize $P$. Picking the parameters of a model to maximize the probability of observing the dataset is called *maximum likelihood estimation*.\n",
    "\n",
    "*Log-likelihood* is given by $\\log P$, which in our case is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\alpha, \\beta, \\sigma^2) = -\\frac{m}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m(y^{(i)} - \\alpha - \\beta x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the model summary above from `statsmodels`, a number is quoted for Log-Likelihood. To reproduce it, let's compute $\\log P(\\hat{\\alpha}, \\hat{\\beta}, \\sigma^2)$. One thing, though, is that $\\log P$ depends on $\\sigma$, which `statsmodels` doesn't take as input. It can, however, esimate it from the sample standard deviation of the prediction errors $y^{(1)} - f^{(1)}, \\dots, y^{(m)} - f^{(m)}$. Let's give that a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -84.6424357588\n",
      "Reproduced  Log-Likelihood = -84.6424357588\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(len(x)/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "Given the probabilistic view, we now see that the optimal estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$ should be regarded as random variables, and not confused with the \"real\" underlying parameters $\\alpha$ and $\\beta$. A different dataset, for instance, would result in different values for $\\hat{\\alpha}$ and $\\hat{\\beta}$. Therefore, it's important to study their variability.\n",
    "\n",
    "Let's first check their means. Note that by assumption\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "and as a result\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\bar{y} \\rangle = \\frac{1}{m} \\sum_{i=1}^m \\langle y^{(i)} \\rangle = \\frac{1}{m} \\sum_{i=1}^m (\\alpha + \\beta x^{(i)}) = \\alpha + \\beta \\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Combining these:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle(y^{(i)} - \\bar{y})\\rangle = \\beta (x^{(i)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these results, we can compute the mean of $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta} \\rangle\n",
    "= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})\\langle(y^{(i)} - \\bar{y})\\rangle\n",
    "= \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(x^{(i)} - \\bar{x}) = \\beta,\n",
    "\\end{equation}\n",
    "\n",
    "and also that of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\alpha} \\rangle = \\langle \\bar{y} \\rangle - \\langle \\hat{\\beta} \\rangle \\bar{x} = \\alpha + \\beta \\bar{x} - \\beta \\bar{x} = \\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "So the mean of $\\hat{\\beta}$ and $\\hat{\\alpha}$ are $\\beta$ and $\\alpha$, respectively. Good, but no terribly surprising!\n",
    "\n",
    "Let's compute the variance of $\\hat{\\beta}$. First, we will express $\\hat{\\beta} - \\beta$ in a convenient way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} - \\beta\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y}) - \\frac{\\beta}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})[(y^{(i)} - \\beta x^{(i)} - \\alpha) - (\\bar{y} - \\beta \\bar{x} - \\alpha)] \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle) - \\frac{\\bar{y} - \\beta \\bar{x} - \\alpha}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "        &= \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle)\n",
    "\\end{align*}\n",
    "\n",
    "where the first equality follows from the definition of $S_x^2$, the second from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, and the third from $\\sum_{i=1}^m (x^{(i)} - \\bar{x})=0$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\\\\n",
    "&= \\frac{1}{m^2S_x^4}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x})\\sigma^2\\delta_{ij} \\\\\n",
    "&= \\frac{\\sigma^2}{m^2S_x^4}\\sum_{i}^m (x^{(i)} - \\bar{x})^2 \\\\\n",
    "&= \\frac{\\sigma^2}{mS_x^2},\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from the fact that $y^{(1)}, \\dots, y^{(m)}$ are (by assumption) independent and normally distributed with variance $\\sigma^2$.\n",
    "\n",
    "Let's check this against `statsmodels`. The estimated value of $\\sigma^2$ (since the real value is unknown to `statsmodels`), is stored in the `scale` attribute of `model`. Using that, we can reproduce the standard error of $\\hat{\\beta}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta std err = 0.195431582311\n",
      "Reproduced  Beta std err = 0.195431582311\n"
     ]
    }
   ],
   "source": [
    "beta_se = np.sqrt(model.scale / (m * xvar))\n",
    "print(\"statsmodels Beta std err = {}\".format(model.bse[1]))\n",
    "print(\"Reproduced  Beta std err = {}\".format(beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute $\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle$. First, note that we can express $\\hat{\\alpha} - \\alpha$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha\n",
    "    = \\bar{y} - (\\hat{\\beta} - \\beta)\\bar{x} - \\beta\\bar{x} - \\alpha\n",
    "    = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "To compute the variance of $\\hat{\\alpha} - \\alpha$, we will need the covariance between $\\hat{\\beta} - \\beta$ and $\\bar{y} - \\langle \\bar{y} \\rangle$. We can express $\\bar{y} - \\langle \\bar{y} \\rangle$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\bar{y} - \\beta \\bar{x} - \\alpha = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\beta x^{(j)} - \\alpha) = \\frac{1}{m}\\sum_{j=1}^m (y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\left\\langle (y^{(i)} - \\langle y^{(i)} \\rangle) (y^{(j)} - \\langle y^{(j)} \\rangle) \\right\\rangle \\\\\n",
    "    &= \\frac{1}{m S_x^2}\\sum_{i,j=1}^m (x^{(i)} - \\bar{x}) \\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x}) \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\langle(\\bar{y} - \\langle \\bar{y} \\rangle)^2\\right\\rangle\n",
    "=\\frac{1}{m^2}\\sum_{k,l=1}^m \\left\\langle(y^{(k)} - \\langle y^{(k)} \\rangle)(y^{(l)} - \\langle y^{(l)} \\rangle)\\right\\rangle\n",
    "= \\frac{1}{m^2}\\sum_{k,l=1}^m \\sigma^2\\delta_{kl}\n",
    "= \\frac{\\sigma^2}{m}.\n",
    "\\end{equation}\n",
    "This is exactly what we should've expected without doing any work, since $\\bar{y} - \\langle \\bar{y} \\rangle \\sim \\mathcal{N}(0, \\sigma^2/m)$ follows from the fact that $\\bar{y}$ is an average of independent and identical normally distributed random variables.\n",
    "\n",
    "Now we can compute the variance of $\\hat{\\alpha}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)^2 \\rangle =\n",
    "\\left\\langle (\\bar{y} - \\langle \\bar{y} \\rangle)^2 \\right\\rangle + \\bar{x}^2 \\left\\langle (\\hat{\\beta} - \\beta)^2 \\right\\rangle\n",
    "= \\frac{\\sigma^2}{m} + \\bar{x}^2\\frac{\\sigma^2}{mS_x^2}\n",
    "= \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Alpha std err = 0.113117048297\n",
      "Reproduced  Alpha std err = 0.113117048297\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(model.scale / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels Alpha std err = {}\".format(model.bse[0]))\n",
    "print(\"Reproduced  Alpha std err = {}\".format(alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our calculation above also makes it easy to derive the covariance between $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle (\\hat{\\alpha} - \\alpha)(\\hat{\\beta} - \\beta) \\rangle\n",
    "= \\left\\langle (\\hat{\\beta} - \\beta) (\\bar{y} - \\langle \\bar{y} \\rangle) \\right\\rangle - \\bar{x} \\langle (\\hat{\\beta} - \\beta)^2 \\rangle\n",
    "= 0 - \\bar{x} \\frac{\\sigma^2}{mS_x^2}\n",
    "= -\\frac{\\sigma^2\\bar{x}}{mS_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Again, we get the same number as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha-beta covariance = -0.0190967516823\n",
      "Reproduced  alpha-beta covariance = -0.0190967516823\n"
     ]
    }
   ],
   "source": [
    "cov_params = - model.scale * xmean/(m*xvar)\n",
    "print(\"statsmodels alpha-beta covariance = {}\".format(model.cov_params()[0, 1]))\n",
    "print(\"Reproduced  alpha-beta covariance = {}\".format(cov_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we glossed over was how `statsmodels` estimates $\\sigma^2$. Let's think about this a little. Since, by assumption, $y{(i)} \\sim \\mathcal{N}(\\alpha + \\beta x^{(i)}, \\sigma^2)$, it makes sense to use the sample variance of the prediction error $e_i=y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)}=y^{(i)} - f^{(i)}$ as an estimate. We can express $e_i$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "e_i &= y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\hat{\\alpha} - \\alpha) - (\\hat{\\beta} - \\beta)x^{(i)} \\\\\n",
    "    &= (y^{(i)} - \\langle y^{(i)} \\rangle) - (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)(x^{(i)} - \\bar{x}) \\\\\n",
    "    &= \\sum_{j=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)\\left[\\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}) \\right] \\\\\n",
    "    &= \\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\langle y^{(i)} \\rangle = \\alpha + \\beta x^{(i)}$, the third from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$, the fourth from our earlier expressions for $\\bar{y} - \\langle \\bar{y} \\rangle$ and $\\hat{\\beta} - \\beta$, and in the last equality we have introduced the matrix $A$ whose elements are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} \\equiv \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "$A$ has a few interesting properties. First, it's symmetric: $A_{ij} = A_{ij}$. Second, it's equal to its square: $A^2=A$. (This can be shown by carrying out the matrix multiplication between $A$ and itself. It's a bit tedious and not terribly interesting, so I'll leave it out.) Using these results, we can compute the sum of squared errors:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m} e_i^2\n",
    "    &= \\sum_{i=1}^{m}\\left[\\sum_{j=1}^{m}A_{ij}(y^{(j)} - \\langle y^{(j)} \\rangle)\\right]^2 \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\left[\\sum_{i=1}^{m}A_{ij}A_{ik}\\right] \\\\\n",
    "    &= \\sum_{j,k=1}^{m}(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle) A_{jk},\n",
    "\\end{align*}\n",
    "\n",
    "where the last equality follows from the fact that $A$ is symmetric and $A^2=A$. Taking the expectation value of both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle\\sum_{i=1}^{m} e_i^2 \\right\\rangle\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\left\\langle(y^{(j)} - \\langle y^{(j)} \\rangle)(y^{(k)} - \\langle y^{(k)} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{j,k=1}^{m} A_{jk}\\sigma^2\\delta_{ij} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(A),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{tr}(A)=\\sum_{i=1}^{m}A_{ii}$ denotes the trace of $A$. It's easy to show that $\\text{tr}(A)=m-2$, so the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m-2}\\sum_{i=1}^{m}e_i^2 = \\frac{1}{m-2}\\sum_{i=1}^m(y^{(i)} - \\hat{\\alpha} - \\hat{\\beta} x^{(i)})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.324709077426\n",
      "Reproduced  estimate of sigma^2 = 0.324709077426\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m-2)\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "We've worked out the variance (and covariance) of the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. It would also be nice to have confidence intervals for them, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta_L, \\beta_U)$ contains $\\beta$. We've done this sort of analysis for the sample mean when discussing the [t-distribution](https://github.com/siavashaslanbeigi/stats_notes/blob/master/t.ipynb). Can we do something similar for $\\hat{\\alpha}$ and $\\hat{\\beta}$? Yes, but it's a bit more involved than dealing with the sample mean.\n",
    "\n",
    "\n",
    "Above we showed that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2/(m S_x^2))$, or equivalently $\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma \\sim \\mathcal{N}(0, 1)$. This means $\\beta_L=\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x}$ and $\\beta_U=\\hat{\\beta}+z\\frac{\\sigma}{\\sqrt{m} S_x}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\hat{\\beta}-z\\frac{\\sigma}{\\sqrt{m} S_x} \\le \\beta \\le \\hat{\\beta}+z \\frac{\\sigma}{\\sqrt{m} S_x})\n",
    "= P(-z \\le \\frac{\\hat{\\beta} - \\beta}{\\sigma/(\\sqrt{m} S_x)} \\le z )\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\hat{\\sigma}^2 / (m S_x^2)}}\n",
    "    = \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)/\\sigma}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\sqrt{m} S_x(\\hat{\\beta} - \\beta)}{\\sigma}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\sum_{i=1}^{m}e_i^2.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. In what follows, we will show that $V$ has a chi-squared distribution with $m-2$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "We will make use (one of the formulations of) [Cochran's theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem), which states the following: suppose $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an $m$-dimensional multivariate normal random vector, and that $B^{(1)}, \\dots, B^{(k)}$ are symmetric $m \\times m$ matrices which satisfy the following properties:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{k}B^{(i)} = I, \\qquad\n",
    "\\sum_{i=1}^{k}r_i = m,\n",
    "\\end{equation}\n",
    "where $I$ is the $m \\times m$ identity matrix and $r_i$ denotes the rank of $B_i$. Then, the random variable $Q^{(i)}=Y^TB^{(i)}Y$ is distributed as $\\sigma^2 \\chi^2_{r_i}$, where $\\chi^2_{r_i}$ is the chi-squared distribution with $r_i$ degrees of freedom, and $Q^{(i)}$ and $Q^{(j)}$ are independent for all $i\\neq j$.\n",
    "\n",
    "\n",
    "It's not immediately obvious how this theorem will help us arrive at confidence intervals, but it will. First, note that if we define $Y$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i \\equiv y^{(i)} - \\langle y^{(i)} \\rangle.\n",
    "\\end{equation}\n",
    "then $Y \\sim \\mathcal{N}(0, \\sigma^2 I)$. With this definition, we can rewrite the sum of squared errors as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}e_i^2 = Y^TAY.\n",
    "\\end{equation}\n",
    "\n",
    "Now this looks like a $Q$ matrix from Cochran's theorem. What is the rank of $A$? Remember that the rank of a matrix is the dimension of the vector space spanned by its column vectors. It can be shown that this is equal to the number of non-zero eigenvalues. Because $A^2=A$, eigenvalues of $A$ are either $1$ or $0$. To see why, let $v$ denote an eigenvector of $A$ with eigenvalue $\\lambda$: $Av=\\lambda v$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda v = Av = A^2v=A(Av)=A(\\lambda v)=\\lambda Av=\\lambda^2v,\n",
    "\\end{equation}\n",
    "\n",
    "which implies $\\lambda^2=\\lambda$, which has the solutions $\\lambda=0$ and $\\lambda=1$. Therefore, for $A$, the number of non-zero eigenvalues is simply the sum of its eigenvalues, which is also its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Rank}(A) = \\text{tr}(A) = m - 2.\n",
    "\\end{equation}\n",
    "Let's summarize what we have so far\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(1)} \\equiv \\sum_{i=1}^{m}e_i^2 = Y^TB^{(1)}Y, \\qquad\n",
    "B^{(1)}_{ij} = \\delta_{ij} - \\frac{1}{m} - \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}), \\qquad\n",
    "r_1 = m - 2.\n",
    "\\end{equation}\n",
    "\n",
    "Next we'll turn our attention to $\\hat{\\beta} - \\beta$. We had previous shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} - \\beta = \\frac{1}{m S_x^2}\\sum_{i=1}^m (x^{(i)} - \\bar{x})(y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(2)} \\equiv m S_x^2(\\hat{\\beta} - \\beta)^2 = Y^TB^{(2)}Y, \\qquad\n",
    "B^{(2)}_{ij} = \\frac{1}{m S_x^2}(x^{(i)} - \\bar{x})(x^{(j)} - \\bar{x}).\n",
    "\\end{equation}\n",
    "\n",
    "Again, it can be checked that $B^{(2)}$ is equal to its own square, so its rank is given by its trace:\n",
    "\n",
    "\\begin{equation}\n",
    "r_2 = \\text{Rank}(B^{(2)}) = \\text{tr}(B^{(2)}) = \\sum_{i=1}^{m} B^{(2)}_{ii} = \\frac{1}{m S_x^2}\\sum_{i=1}^{m} (x^{(i)} - \\bar{x})^2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, as we showed in previous section\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} - \\langle \\bar{y} \\rangle = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\langle y^{(i)} \\rangle),\n",
    "\\end{equation}\n",
    "\n",
    "from which it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{(3)} \\equiv m (\\bar{y} - \\langle \\bar{y} \\rangle)^2 = Y^TB^{(3)}Y, \\qquad\n",
    "B^{(3)}_{ij} = \\frac{1}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Since all columns of $B^{(3)}$ are the same, its rank is $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "It's now easy to check that $B^{(1)}$, $B^{(2)}$, and $B^{(3)}$ satisfy the conditions of Cochran's theorem:\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(1)}_{ij} + B^{(2)}_{ij} + B^{(3)}_{ij} = \\delta_{ij}, \\qquad\n",
    "r_1 + r_2 + r_3 = m.\n",
    "\\end{equation}\n",
    "\n",
    "Cochran's theorem then implies the following:\n",
    "\n",
    "* $\\sum_{i=1}^{m}e_i^2 \\sim \\sigma^2 \\chi^2_{m-2}$.\n",
    "* $\\sum_{i=1}^{m}e_i^2$, $(\\hat{\\beta} - \\beta)^2$, and $(\\bar{y} - \\langle \\bar{y} \\rangle)^2$ are mutually independent, which implies $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are mutually independent.\n",
    "\n",
    "This in turn implies that $V$ has a $\\chi^2_{m-2}$ distribution and that $Z$ and $V$ are independent. Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\frac{Z}{\\sqrt{\\frac{V}{m-2}}} = \\frac{\\hat{\\beta} - \\beta}{s_{\\hat{\\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "has the t-distribution with $m-2$ degrees of freedom, where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\beta}}^2 = \\frac{\\hat{\\sigma}^2}{m S_x^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally: $\\beta_L=\\hat{\\beta}-ts_{\\hat{\\beta}}$ and $\\beta_U=\\hat{\\beta}+ts_{\\hat{\\beta}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels beta 95% confidence interval = (1.62620621534, 2.40186144467)\n",
      "Reproduced  beta 95% confidence interval = (1.62620621534, 2.40186144467)\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - 2)\n",
    "# Reproduced confidence interval for beta\n",
    "beta_se = np.sqrt(yvar / (m * np.var(x)))\n",
    "print(\"statsmodels beta 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[1, 0], statsmodels_ci[1, 1]))\n",
    "print(\"Reproduced  beta 95% confidence interval = ({0}, {1})\".format(beta - t*beta_se, beta + t*beta_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the confidence interval for $\\hat{\\alpha}$. Earlier we had shown that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} \\sim \\mathcal{N}\\left(\\alpha, \\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As before, we divide $\\hat{\\alpha} - \\alpha$ by its standard deviation and replace $\\sigma$ with $\\hat{\\sigma}$:\n",
    "\\begin{equation}\n",
    "T_{\\alpha}\n",
    "    \\equiv \\frac{\\hat{\\alpha} - \\alpha}{s_{\\hat{\\alpha}}}\n",
    "    = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2)}}\n",
    "    = \\frac{\\sqrt{m}(\\hat{\\alpha} - \\alpha)/\\sqrt{\\sigma^2(1+\\bar{x}^2/S_x^2)}}{\\sqrt{\\frac{1}{(m-2)\\sigma^2}\\sum_{i=1}^{m}e_i^2}}\n",
    "    = \\frac{Z_{\\alpha}}{\\sqrt{\\frac{V}{m-2}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{\\alpha}} \\equiv \\frac{\\hat{\\sigma}^2}{m}(1+\\bar{x}^2/S_x^2), \\qquad\n",
    "Z_{\\alpha} = \\frac{\\hat{\\alpha} - \\alpha}{\\sqrt{\\frac{\\sigma^2}{m}(1+\\bar{x}^2/S_x^2)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that $Z_{\\alpha} \\sim \\mathcal{N}(0, 1)$ and $V \\sim \\chi^2_{m-2}$. If $Z_{\\alpha}$ and $V$ are independent, then $T_{\\alpha}$ also has a t-distribution with $m-2$ degrees of freedom. We had shown earlier that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\sum_{i=1}^{m}e_i^2$, $\\hat{\\beta} - \\beta$, and $\\bar{y} - \\langle \\bar{y} \\rangle$ are independent, it follows that $\\hat{\\alpha} - \\alpha$ and $\\sum_{i=1}^{m}e_i^2$, and in turn $Z_{\\alpha}$ and $V$, are independent. As a result, $\\alpha_L=\\hat{\\alpha}-ts_{\\hat{\\alpha}}$ and $\\alpha_U=\\hat{\\alpha}+ts_{\\hat{\\alpha}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n",
      "Reproduced  alpha 95% confidence interval = (0.082060520856, 0.531014722628)\n"
     ]
    }
   ],
   "source": [
    "alpha_se = np.sqrt(yvar / m * (1 + xmean**2/xvar))\n",
    "print(\"statsmodels alpha 95% confidence interval = ({0}, {1})\".format(statsmodels_ci[0, 0], statsmodels_ci[0, 1]))\n",
    "print(\"Reproduced  alpha 95% confidence interval = ({0}, {1})\".format(alpha - t*alpha_se, alpha + t*alpha_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using $\\hat{\\alpha}$ and $\\hat{\\beta}$, so our predictions are also subject to statistical noise. Let's compute the variance of $\\hat{f}=\\hat{\\alpha} + \\hat{\\beta} x$. First, its mean is what we expect it to be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{f} \\rangle = \\langle\\hat{\\alpha}\\rangle + \\langle\\hat{\\beta}\\rangle x = \\alpha + \\beta x.\n",
    "\\end{equation}\n",
    "\n",
    "As a result:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f} - \\langle \\hat{f} \\rangle\n",
    "    &= (\\hat{\\alpha} - \\alpha) + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}  + (\\hat{\\beta} - \\beta)x \\\\\n",
    "    &= (\\bar{y} - \\langle \\bar{y} \\rangle) + (\\hat{\\beta} - \\beta)(x - \\bar{x}),\n",
    "\\end{align*}\n",
    "\n",
    "where the second equality follows from $\\hat{\\alpha} - \\alpha = (\\bar{y} - \\langle \\bar{y} \\rangle) - (\\hat{\\beta} - \\beta)\\bar{x}$. Since $\\bar{y}$ and $\\hat{\\beta}$ are independent, the variance of $\\hat{f}$ is given simply by:\n",
    "\n",
    "The variance is given by:\n",
    "\\begin{align*}\n",
    "\\left\\langle (\\hat{f} - \\langle \\hat{f} \\rangle)^2 \\right\\rangle\n",
    "    &= \\langle (\\bar{y} - \\langle \\bar{y}\\rangle)^2 + (x - \\bar{x})^2 \\langle(\\hat{\\beta} - \\beta)^2 \\rangle \\\\\n",
    "    &= \\frac{\\sigma^2}{m} + (x - \\bar{x})^2 \\frac{\\sigma^2}{m S_x^2} \\\\\n",
    "    &= \\frac{\\sigma^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "The same argument we used for deriving the confidence interval of $\\hat{\\alpha}$ can be used for $\\hat{f}$: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-2$ degrees of freedom, and:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\frac{\\hat{\\sigma}^2}{m}\\left[1+\\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can use `seaborn.regplot` to plot the confidence interval for $f$. Below we also show the $95\\%$ confidence interval computed using the formula we just derived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e2bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard error of f\n",
    "x_axis = np.linspace(0, 1, 1000)\n",
    "f_se = np.sqrt(yvar/m*(1 + (x_axis - xmean)**2/xvar))\n",
    "f_l = alpha + beta * x_axis - t * f_se\n",
    "f_u = alpha + beta * x_axis + t * f_se\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_axis, f_l, 'r--')\n",
    "plt.plot(x_axis, f_u, 'r--', label='Reproduced 95% confidence interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Plot using seaborn\n",
    "seaborn.regplot(x, y);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
