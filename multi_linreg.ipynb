{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables\n",
    "\n",
    "The goal of this notebook is to generalize our work on [linear regression with one variable](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) to multiple variables. Consider the model\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2, \\dots, x_n) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n.\n",
    "\\end{equation}\n",
    "\n",
    "We now have multiple ($n$ to be precise) independent variables instead of just one, hence the term *multiple* linear regression. A single observation corresponds to a value for each $x_1, x_2, \\dots, x_n$ as well as the dependent variable $y$. We will assume there are $m$ such observations. Also, we will use the terms *feature* and *independent variable* interchangeably. For instance, we may call $x_2$ the second feature. This terminology comes from the machine learning world.\n",
    "\n",
    "\n",
    "Let's start by establishing some notation:\n",
    "\n",
    "* $y^{(i)}$: value of the dependent variable for the $i^{\\text{th}}$ observation.\n",
    "* $x^{(i)}_{j}$: value of the $j^{\\text{th}}$ feature for the $i^{\\text{th}}$ observation.\n",
    "* $x_0 \\equiv 1$. In other words, $x^{(i)}_{0} = 1$ for all $i=1,\\dots,m$.\n",
    "* $x=\\begin{pmatrix}x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features.\n",
    "* $x^{(i)}=\\begin{pmatrix}x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features for the $i^{\\text{th}}$ observation.\n",
    "* $\\beta=\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of parameters.\n",
    "* $X_{ij} \\equiv x^{(i)}_{j}$ for $i=1,\\dots,m$ and $j=0,\\dots,n$. $X$ is called the *design matrix*. It's an $m \\times (n+1)$ dimensional matrix whose rows correspond to the observations. Note that the first column is just a vector of $1$s, since by definition $x^{(i)}_{0} = 1$.\n",
    "* $Y_i \\equiv y^{(i)}$: $m$-dimensional vector created from $y^{(1)}, \\dots, y^{(m)}$. It's sometimes called the *target vector*.\n",
    "\n",
    "Expressing the data in vectorized form will make our life much easier. For instance, $f$ can now be expressed as a dot product:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "We will use the same cost function as we did for simple linear regression, i.e. the normalized sum of squared prediction errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^Tx^{(i)} = \\sum_{j=1}^{m} \\beta_j x^{(i)}_{j} = \\sum_{j=1}^{m} X_{ij}\\beta_j = [X\\beta]_{i},\n",
    "\\end{equation}\n",
    "\n",
    "which in term allows us to express $J$ in matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} ([X\\beta]_{i} - Y_i)^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} [X\\beta - Y]_{i}[X\\beta - Y]_{i} \\\\\n",
    "         &= \\frac{1}{2m}[X\\beta - Y]^T[X\\beta - Y].\n",
    "\\end{align*}\n",
    "\n",
    "To find the parameters $\\beta_0,\\beta_1, \\dots, \\beta_n$ that best fit the data set, we will minimize $J$ with respect to each one of them. To do that, first we need the gradient of $J$ with respect to each $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_j}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}(\\beta^Tx^{(i)} - y^{(i)})x^{(i)}_{j} \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[X\\theta - Y]_{i} \\\\\n",
    "    &= \\frac{1}{m} [X^T(X\\beta - Y)]_j.\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradients to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J = 0 \\to X^T(X\\hat{\\beta} - Y) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we find that the optimal parameters $\\hat{\\beta}$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY.\n",
    "\\end{equation}\n",
    "\n",
    "This is called the *normal equation*. Does it reproduce the formulas we found for $\\hat{\\alpha}$ and $\\hat{\\beta}$ [simple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb)? Let's check. First note the mapping between the variables:\n",
    "\n",
    "| Multiple LR     | Simple LR       |\n",
    "|-----------------|-----------------|\n",
    "| $x^{(i)}_1$     | $x^{(i)}$       |\n",
    "| $\\hat{\\beta}_0$ | $\\hat{\\alpha}$  |\n",
    "| $\\hat{\\beta}_1$ | $\\hat{\\beta}$   |\n",
    "\n",
    "Using this, it can be shown after straightforward algebra that (see [this](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) notebook for undefined quantities below)\n",
    "\\begin{equation}\n",
    "X^TX = m\n",
    "\\begin{pmatrix}\n",
    "    1       & \\bar{x} \\\\\n",
    "    \\bar{x} & S_x^2 + \\bar{x}^2\n",
    "\\end{pmatrix}, \\qquad\n",
    "X^TY = m\n",
    "\\begin{pmatrix}\n",
    "    \\bar{y} \\\\\n",
    "    C_{xy} + \\bar{x}\\bar{y}.\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The inverse of $X^TX$ is given by\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{mS_x^2}\n",
    "\\begin{pmatrix}\n",
    "    S_x^2 + \\bar{x}^2       & -\\bar{x} \\\\\n",
    "    -\\bar{x} & 1\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "using which we can compute $\\hat{\\beta}$: \n",
    "\\begin{equation}\n",
    "\\hat{\\beta}\n",
    "    = (X^TX)^{-1}X^TY\n",
    "    = \\frac{1}{S_x^2}\n",
    "        \\begin{pmatrix} S_x^2 + \\bar{x}^2 & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{pmatrix}\n",
    "        \\begin{pmatrix} \\bar{y} \\\\ C_{xy} + \\bar{x}\\bar{y} \\end{pmatrix}\n",
    "    = \\frac{1}{S_x^2}\n",
    "    \\begin{pmatrix} \\bar{y}S_x^2 -\\bar{x}C_{xy} \\\\ C_{xy} \\end{pmatrix}\n",
    "    = \\begin{pmatrix} \\bar{y} -\\bar{x}\\frac{C_{xy}}{S_x^2} \\\\ \\frac{C_{xy}}{S_x^2} \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "That's exactly what we had derived previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data using the model $f(x)=0.3 + 2x_1 + 5 x_2$, and by adding Gaussian noise with $\\sigma=0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(123)\n",
    "\n",
    "m = 100\n",
    "x1 = np.random.uniform(size=m)\n",
    "x2 = np.random.uniform(size=m)\n",
    "y = np.random.normal(loc=0.3 + 2.0*x1 + 5.0*x2, scale=0.5, size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   468.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>1.37e-50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:16:33</td>     <th>  Log-Likelihood:    </th> <td> -65.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   136.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   144.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3851</td> <td>    0.142</td> <td>    2.706</td> <td> 0.008</td> <td>    0.103</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.7702</td> <td>    0.193</td> <td>    9.150</td> <td> 0.000</td> <td>    1.386</td> <td>    2.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9949</td> <td>    0.167</td> <td>   29.947</td> <td> 0.000</td> <td>    4.664</td> <td>    5.326</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.909</td> <th>  Durbin-Watson:     </th> <td>   1.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.032</td> <th>  Jarque-Bera (JB):  </th> <td>   9.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td> 0.00973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.366</td> <th>  Cond. No.          </th> <td>    6.17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.906\n",
       "Model:                            OLS   Adj. R-squared:                  0.904\n",
       "Method:                 Least Squares   F-statistic:                     468.9\n",
       "Date:                Tue, 05 Mar 2019   Prob (F-statistic):           1.37e-50\n",
       "Time:                        15:16:33   Log-Likelihood:                -65.432\n",
       "No. Observations:                 100   AIC:                             136.9\n",
       "Df Residuals:                      97   BIC:                             144.7\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3851      0.142      2.706      0.008       0.103       0.668\n",
       "x1             1.7702      0.193      9.150      0.000       1.386       2.154\n",
       "x2             4.9949      0.167     29.947      0.000       4.664       5.326\n",
       "==============================================================================\n",
       "Omnibus:                        6.909   Durbin-Watson:                   1.675\n",
       "Prob(Omnibus):                  0.032   Jarque-Bera (JB):                9.264\n",
       "Skew:                          -0.299   Prob(JB):                      0.00973\n",
       "Kurtosis:                       4.366   Cond. No.                         6.17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(m), x1, x2])\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's compare the normal equation to the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 = 0.385073881382\n",
      "Reproduced  Beta_0 = 0.385073881382\n",
      "\n",
      "statsmodels Beta_1 = 1.77016602447\n",
      "Reproduced  Beta_1 = 1.77016602447\n",
      "\n",
      "statsmodels Beta_2 = 4.99486380715\n",
      "Reproduced  Beta_2 = 4.99486380715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    print(\"statsmodels Beta_{0} = {1}\".format(i, model.params[i]))\n",
    "    print(\"Reproduced  Beta_{0} = {1}\".format(i, beta[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "We introduced $R^2$ [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb). It's defined by\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = f(x^{(i)}), \\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "In the case of multiple linear regression:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = \\hat{\\beta}^Tx^{(i)} = [X\\hat{\\beta}]_i.\n",
    "\\end{equation}\n",
    "\n",
    "If we let $\\hat{f}$ define the vector of all preditions on input data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} \\equiv \\begin{pmatrix}f^{(1)} \\\\ f^{(2)} \\\\ \\vdots \\\\ f^{(m)} \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} = X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "Let $e$ denote the vector of all prediction errors on input data\n",
    "\n",
    "\\begin{equation}\n",
    "e^{(i)} = y^{(i)} - f^{(i)}, \\qquad\n",
    "\\hat{e} \\equiv \\begin{pmatrix}e^{(1)} \\\\ e^{(2)} \\\\ \\vdots \\\\ e^{(m)} \\end{pmatrix} = Y - \\hat{f} = Y - X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "We're ready to compute $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.906264829007\n",
      "Reproduced  R-squared = 0.906264829007\n"
     ]
    }
   ],
   "source": [
    "ymean = y.mean()\n",
    "f = X.dot(beta)\n",
    "e = f - y\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like simple linear regression, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062648290073423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generalize the proof we had to multiple variables. Remember that minimizing the cost function gave us the following equation: $X^T(X\\hat{\\beta} - Y)=0$, which can also be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "X^T\\hat{e} = 0.\n",
    "\\end{equation}.\n",
    "\n",
    "Looking at the first component of this equation:\n",
    "\n",
    "\\begin{equation}\n",
    "[X^T\\hat{e}]_0 = \\sum_{i=1}^{m}X_{i0}e^{(i)} = \\sum_{i=1}^{m}x^{(i)}_0e^{(i)} = \\sum_{i=1}^{m}e^{(i)} = 0,\n",
    "\\end{equation}.\n",
    "\n",
    "we see that the sum of prediction errors is zero (recall that by definition $x^{(i)}_0=1$). Since $e^{(i)}=y^{(i)} - f^{(i)}$, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}y^{(i)} = \\sum_{i=1}^{m} f^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\bar{y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\frac{1}{m} \\sum_{i=1}^{m} f^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, if we apply $\\hat{\\beta}$ to both sides of $X^T\\hat{e} = 0$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^TX^T\\hat{e} = (X\\hat{\\beta})^T\\hat{e} = \\hat{f}^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Alright, on to computing $R^2$. Let's start with the denominator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2\n",
    "    &= \\sum_{i=1}^{m}[(f^{(i)} - \\bar{y}) + (y^{(i)} - f^{(i)})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + \\sum_{i=1}^{m}(y^{(i)} - f^{(i)})^2 + 2\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})(y^{(i)} - f^{(i)}).\n",
    "\\end{align*}\n",
    "\n",
    "The last term turns out to be zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})(y^{(i)} - f^{(i)})\n",
    "    = \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})e^{(i)}\n",
    "    = \\sum_{i=1}^{m}f^{(i)}e^{(i)} - \\bar{y}\\sum_{i=1}^{m}e^{(i)}\n",
    "    = 0,\n",
    "\\end{equation}\n",
    "\n",
    "where the last equality follow from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 = \\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2 + \\sum_{i=1}^{m}(y^{(i)} - f^{(i)})^2,\n",
    "\\end{equation}\n",
    "\n",
    "where we've used $\\bar{y}=\\bar{f}$. Plugging this back into the definition of $R^2$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}\n",
    "    = \\frac{\\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The correlation $r_{fy}$ between $f$ and $y$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy} = \\frac{C_{fy}}{S_fS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "S_y = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2, \\qquad\n",
    "S_f = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2, \\qquad\n",
    "C_{fy} = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these definitions, we can also rewrite $R^2$ more simply as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{S_f^2}{S_y^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The covariance between $f$ and $y$ can be simplified:\n",
    "\n",
    "\\begin{align*}\n",
    "C_{fy}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(e^{(i)} + f^{(i)} - \\bar{f}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})^2 + \\frac{1}{m}\\sum_{i=1}^{m} f^{(i)}e^{(i)} - \\frac{\\bar{f}}{m}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= S_f^2,\n",
    "\\end{align*}\n",
    "\n",
    "where the second line uses the definition of $e^{(i)}$ and the fact that $\\bar{y}=\\bar{f}$, and the last equality follows from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$.  Plugging this back in $r_{fy}$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy}^2 = \\frac{C_{fy}^2}{S_f^2S_y^2} = \\frac{S_f^4}{S_f^2S_y^2} = \\frac{S_f^2}{S_y^2} = R^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view\n",
    "\n",
    "The probabilistic view is much the same as before. We will assume $y^{(i)}$ are independent normally-distributed samples with standard deviation $\\sigma$: \n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} \\sim \\mathcal{N}(\\beta^Tx^{(i)}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "The probabiliy of observing the dataset $(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})$ is\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\beta, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left[-\\sum_{i=1}^m\\frac{(y^{(i)} - \\beta^Tx^{(i)})^2}{2\\sigma^2}\\right]} = \\frac{1}{(2\\pi \\sigma^2)^{m/2}}\\exp{\\left(-\\frac{mJ(\\beta)}{\\sigma^2}\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "As before, the parameters $\\hat{\\beta}$ that maximize $P$ are precisely those that minimize the cost function $J(\\beta)$. Also, we can reproduce `statsmodels`'s computation of log-likelihood by evaluating $\\log P(\\hat{\\beta}, \\tilde{\\sigma}^2)$, where $\\hat{\\sigma}^2$ is the sample estimate of prediction erorrs:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\sigma}^2 = \\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\beta^Tx^{(i)})^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Log-Likelihood = -65.4317634738\n",
      "Reproduced  Log-Likelihood = -65.4317634738\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = -(m/2.0)*np.log(2*np.pi*np.var(e)) - m/2.0\n",
    "print(\"statsmodels Log-Likelihood = {}\".format(model.llf))\n",
    "print(\"Reproduced  Log-Likelihood = {}\".format(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error of estimates\n",
    "\n",
    "What is the probability distribution of $\\hat{\\beta}$? By definition, $Y$ has a multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I),\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the $m \\times m$ identity matrix and $\\mu_y$. Also, $\\hat{\\beta}$ is a linear transformation of $Y$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}=AY\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is the following $(n+1)\\times m$ dimensional matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "A \\equiv (X^TX)^{-1}X^T.\n",
    "\\end{equation}\n",
    "\n",
    "Because of this, $\\hat{\\beta}$ itself has a multivariate Gaussian distribution. This is a general result which we won't prove here. Instead, let's compute the mean and covariance of $\\hat{\\beta}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta}_j \\rangle\n",
    "    = \\sum_{i=1}^{m}A_{ji}\\langle y^{(i)} \\rangle\n",
    "    = \\sum_{i=1}^{m}A_{ji}\\beta^Tx^{(i)}\n",
    "    = \\sum_{i=1}^{m}A_{ji}[X\\beta]_i\n",
    "    = [AX\\beta]_j\n",
    "    = [(X^TX)^{-1}X^TX\\beta]_j\n",
    "    =\\beta_j.\n",
    "\\end{equation}\n",
    "\n",
    "Good! Of course, we should've expected this. Let's move on the covariance. Note that because of the first equality above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_j - \\beta_j = \\sum_{i=1}^{m}A_{ji} (y^{(i)} - \\langle y^{(i)} \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "We can now use this to compute the covariance:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle (\\hat{\\beta}_j - \\beta_j)(\\hat{\\beta}_{j'} - \\beta_{j'}) \\rangle\n",
    "    &= \\sum_{i,i'=1}^{m}A_{ji}A_{j'i'}\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{i,i'=1}^{m}A_{ji}A_{j'i'}\\sigma^2\\delta_{ii'} \\\\\n",
    "    &= \\sigma^2\\sum_{i}^{m}A_{ji}A_{j'i} \\\\\n",
    "    &= \\sigma^2[AA^T]_{jj'} \\\\\n",
    "    &= \\sigma^2[(X^TX)^{-1}X^TX(X^TX)^{-1}]_{jj'} \\\\\n",
    "    &= \\sigma^2[(X^TX)^{-1}]_{jj'}.\n",
    "\\end{align*}\n",
    "\n",
    "What an elegant expression! For simple linear regression we had to do a lot more work to compute the the variance of $\\hat{\\alpha}$, $\\hat{\\beta}$, and also their covariance. Above we computed $(X^TX)^{-1}$ for univariate linear regression. Using that, it's easy to check that the formulas derived [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) are consistent with the covariance matrix $\\sigma^2[(X^TX)^{-1}]$.\n",
    "\n",
    "To summarize, $\\hat{\\beta}$ has a multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2\\Sigma),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Sigma$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma = (X^TX)^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Let's check this against `statsmodels`: (we will use `statsmodels` estimate of $\\sigma^2$ by using the `scale` attribute of `model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "Reproduced  Beta_0-Beta_0 covariance = 0.0202562993414\n",
      "\n",
      "statsmodels Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "Reproduced  Beta_0-Beta_1 covariance = -0.0202764718329\n",
      "\n",
      "statsmodels Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "Reproduced  Beta_0-Beta_2 covariance = -0.0155511241659\n",
      "\n",
      "statsmodels Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "Reproduced  Beta_1-Beta_1 covariance = 0.0374243151515\n",
      "\n",
      "statsmodels Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "Reproduced  Beta_1-Beta_2 covariance = 0.00299109336668\n",
      "\n",
      "statsmodels Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "Reproduced  Beta_2-Beta_2 covariance = 0.0278181525471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta_cov = model.scale * np.linalg.inv(X.T.dot(X))\n",
    "statsmodels_cov = model.cov_params()\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    for j in range(i, len(beta)):\n",
    "        print(\"statsmodels Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, statsmodels_cov[i, j]))\n",
    "        print(\"Reproduced  Beta_{0}-Beta_{1} covariance = {2}\".format(i, j, beta_cov[i, j]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the unbiased esimate of $\\sigma^2$? Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e}\n",
    "    = Y - X\\hat{\\beta}\n",
    "    = Y - X(X^TX)^{-1}X^TY\n",
    "    = (I - X(X^TX)^{-1}X^T)Y,\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the $m \\times m$ dimensional identity matrix. Let \n",
    "\n",
    "\\begin{equation}\n",
    "M \\equiv I - X(X^TX)^{-1}X^T,\n",
    "\\end{equation}\n",
    "\n",
    "so that \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e} = MY.\n",
    "\\end{equation}\n",
    "\n",
    "It's easy to check that $M$ is symmetric and also equal to its own square: $M^2=M$. Let $\\langle Y \\rangle$ denote\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle Y \\rangle\n",
    "    \\equiv\n",
    "    \\begin{pmatrix}\n",
    "        \\langle y^{(1)} \\rangle \\\\\n",
    "        \\langle y^{(2)} \\rangle \\\\\n",
    "        \\vdots \\\\\n",
    "        \\langle y^{(m)} \\rangle\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        \\beta^Tx^{(1)} \\\\\n",
    "        \\beta^Tx^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta^Tx^{(m)}\n",
    "    \\end{pmatrix}\n",
    "    = X\\beta\n",
    "\\end{equation}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation}\n",
    "M\\langle Y \\rangle = (I - X(X^TX)^{-1}X^T)X\\beta = X\\beta - X\\beta = 0,\n",
    "\\end{equation}\n",
    "\n",
    "so we can also write $\\hat{e}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{e} = M(Y - \\langle Y \\rangle).\n",
    "\\end{equation}\n",
    "\n",
    "The sum of squared prediction errors is given by the norm of $\\hat{e}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{e}^T\\hat{e}\n",
    "    &= (Y - \\langle Y \\rangle)^TM^TM(Y - \\langle Y \\rangle) \\\\\n",
    "    &= (Y - \\langle Y \\rangle)^TM(Y - \\langle Y \\rangle).\n",
    "\\end{align*}\n",
    "\n",
    "Taking the expectation value we find\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle\\hat{e}^T\\hat{e}\\rangle\n",
    "    &= \\left\\langle(Y - \\langle Y \\rangle)^TM(Y - \\langle Y \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{ii'}M_{ii'}\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{ii'}M_{ii'}\\sigma^2\\delta_{ii'} \\\\\n",
    "    &= \\sigma^2 \\text{tr}(M),\n",
    "\\end{align*}\n",
    "\n",
    "where the third equality follow from $\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle = \\sigma^2 \\delta_{ii'}$. We had arrived at the exact same formula for simple linear regression as well. Next, we need to find the trace of $M$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{tr}(M)\n",
    "    = \\text{tr}(I) - \\text{tr}(X(X^TX)^{-1}X^T)\n",
    "    = m - \\text{tr}(X^TX(X^TX)^{-1})\n",
    "    = m - (n+1),\n",
    "\\end{equation}\n",
    "\n",
    "where the second equality uses the linear algebra identity $\\text{tr}(AB)=\\text{tr}(BA)$. Therefore, the unbiased estimator $\\hat{\\sigma}^2$ of $\\sigma^2$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{m - n - 1}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "For simple linear regression $n=1$ and we recover the $m-2$ factor in the denominator.\n",
    "\n",
    "Let's see if this reproduces the estimate of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels estimate of sigma^2 = 0.223401966576\n",
      "Reproduced  estimate of sigma^2 = 0.223401966576\n"
     ]
    }
   ],
   "source": [
    "yvar = np.dot(e, e)/(m-len(beta))\n",
    "print(\"statsmodels estimate of sigma^2 = {}\".format(model.scale))\n",
    "print(\"Reproduced  estimate of sigma^2 = {}\".format(yvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Let's work our confidence intervals for $\\hat{\\beta}_j$, so we can make statements like: there's a $95\\%$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. We've already shown that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}_j \\sim \\mathcal{N}(\\beta_j, \\sigma^2\\Sigma^{-1}_{jj}),\n",
    "\\end{equation}\n",
    "\n",
    "This means $\\beta^{(L)}_j=\\hat{\\beta}_j-z\\sqrt{\\sigma^2\\Sigma^{-1}_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma^{-1}_{jj}}$ define a valid confidence interval for confidence level $1 - \\gamma$, where $z=\\Phi^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the standard normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(\\hat{\\beta}_j-z \\sqrt{\\sigma^2\\Sigma^{-1}_{jj}} \\le \\beta_j \\le \\hat{\\beta}_j+z\\sqrt{\\sigma^2\\Sigma^{-1}_{jj}}\\right)\n",
    "= P\\left(-z \\le \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma^{-1}_{jj}}} \\le z \\right)\n",
    "= 2\\Phi(z)-1\n",
    "= 2\\Phi(\\Phi^{-1}(1-\\gamma/2))-1\n",
    "= 1-\\gamma.\n",
    "\\end{equation}\n",
    "\n",
    "In other words, there's a $1-\\gamma$ chance that the interval $(\\beta^{(L)}_j, \\beta^{(U)}_j)$ contains $\\beta_j$. The problem, though, is that we do not know $\\sigma^2$. What if we replace it with the estimate $\\hat{\\sigma}^2$? Then we'll need to know the distribution of\n",
    "\n",
    "\\begin{equation}\n",
    "T \\equiv\n",
    "    \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2\\Sigma^{-1}_{jj}}}\n",
    "    = \\frac{(\\hat{\\beta}_j - \\beta_j)/\\sqrt{\\sigma^2\\Sigma_{jj}}}{\\sqrt{\\frac{1}{(m - n - 1)\\sigma^2}\\hat{e}^T\\hat{e}}}\n",
    "    = \\frac{Z}{\\sqrt{\\frac{V}{m-n-1}}},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2\\Sigma^{-1}_{jj}}}, \\qquad\n",
    "V \\equiv \\frac{1}{\\sigma^2}\\hat{e}^T\\hat{e}.\n",
    "\\end{equation}\n",
    "\n",
    "We already know that $Z \\sim \\mathcal{N}(0, 1)$. It can also be shown that $V$ has a chi-squared distribution with $m-n-1$ degrees of freedom and that $Z$ and $V$ are independent. Therefore, $T$ has a t-distribution with $m-n-1$ degrees of freedom.\n",
    "\n",
    "This means that $\\beta^{(L)}_j=\\hat{\\beta}_j-t\\sqrt{\\hat{\\sigma}^2\\Sigma^{-1}_{jj}}$ and $\\beta^{(U)}_j=\\hat{\\beta}_j+t\\sqrt{\\hat{\\sigma}^2\\Sigma^{-1}_{jj}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$ and $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom. This gives us the same confidence interval as `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "Reproduced  Beta_0 95% confidence interval = (0.102598895008, 0.667548867756)\n",
      "\n",
      "statsmodels Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "Reproduced  Beta_1 95% confidence interval = (1.38621407759, 2.15411797136)\n",
      "\n",
      "statsmodels Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "Reproduced  Beta_2 95% confidence interval = (4.66383629856, 5.32589131574)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# statsmodel confidence interval\n",
    "statsmodels_ci = model.conf_int()\n",
    "t = stats.t.ppf(1 - 0.05/2., m - len(beta))\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    beta_se = np.sqrt(beta_cov[i, i])\n",
    "    print(\"statsmodels Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, statsmodels_ci[i, 0], statsmodels_ci[i, 1]))\n",
    "    print(\"Reproduced  Beta_{0} 95% confidence interval = ({1}, {2})\".format(i, beta[i] - t*beta_se, beta[i] + t*beta_se))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unseen values of the independent variables $x$, we predict using $\\hat{\\beta}$. Therefore, our predictions are also subject to statistical noise. Let's work out the variance of $\\hat{\\f}(x) = \\hat{\\beta}^Tx$, starting with the mean:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{f}(x) \\rangle = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The variance is then given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\langle \\left(\\hat{f}(x) - \\langle \\hat{f}(x) \\rangle\\right)^2 \\right\\rangle\n",
    "    &= \\sum_{j,j'=0}^{n} x_jx_{j'}\\langle (\\hat{\\beta}_j - \\beta_j)(\\hat{\\beta}_{j'} - \\beta_{j'}) \\rangle \\\\\n",
    "    &= \\sum_{j,j'=0}^{n} x_jx_{j'}\\sigma^2[(X^TX)^{-1}]_{jj'} \\\\\n",
    "    &= \\sigma^2 x^T (X^TX)^{-1}x.\n",
    "\\end{align*}\n",
    "\n",
    "Does this match the expression we derived for simple linear regression? Let's check:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2 x^T (X^TX)^{-1}x\n",
    "    &= \\frac{\\sigma^2}{m S_x^2}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}^T\n",
    "    \\begin{pmatrix} S_x^2 + \\bar{x}^2 && -\\bar{x} \\\\ -\\bar{x} && 1 \\end{pmatrix}\n",
    "    \\begin{pmatrix} 1 \\\\ x \\end{pmatrix} \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + \\bar{x}^2 - 2x\\bar{x} +x^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m S_x^2} (S_x^2 + (x - \\bar{x})^2) \\\\\n",
    "    &= \\frac{\\sigma^2}{m} \\left[1 + \\frac{(x - \\bar{x})^2}{S_x^2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Indeed! To get confidence intervals for our predictions, we will proceed as before: $f_L=\\hat{f}-ts_{\\hat{f}}$ and $f_U=\\hat{f}+ts_{\\hat{f}}$ define a valid confidence interval for confidence level $1-\\gamma$ ($0\\le \\gamma \\le 1$), where $t=\\Phi_t^{-1}(1-\\gamma/2)$, $\\Phi_t^{-1}$ is the inverse CDF of the t-distribution with $m-n-1$ degrees of freedom, and:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{\\hat{f}}^2 = \\hat{\\sigma}^2 x^T (X^TX)^{-1}x.\n",
    "\\end{equation}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
