{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables\n",
    "\n",
    "The goal of this notebook is to generalize our work on [linear regression with one variable](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) to multiple variables. Consider the model\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2, \\dots, x_n) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n.\n",
    "\\end{equation}\n",
    "\n",
    "We now have multiple ($n$ to be precise) independent variables instead of just one, hence the term *multiple* linear regression. A single observation corresponds to a value for each $x_1, x_2, \\dots, x_n$ as well as the dependent variable $y$. We will assume there are $m$ such observations. Also, we will use the terms *feature* and *independent variable* interchangeably. For instance, we may call $x_2$ the second feature. This terminology comes from the machine learning world.\n",
    "\n",
    "\n",
    "Let's start by establishing some notation:\n",
    "\n",
    "* $y^{(i)}$: value of the dependent variable for the $i^{\\text{th}}$ observation.\n",
    "* $x^{(i)}_{j}$: value of the $j^{\\text{th}}$ feature for the $i^{\\text{th}}$ observation.\n",
    "* $x_0 \\equiv 1$. In other words, $x^{(i)}_{0} = 1$ for all $i=1,\\dots,m$.\n",
    "* $x=\\begin{pmatrix}x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features.\n",
    "* $x^{(i)}=\\begin{pmatrix}x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of features for the $i^{\\text{th}}$ observation.\n",
    "* $\\beta=\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_n\\end{pmatrix} \\in \\mathbb{R}^{n+1}$ denotes the vector of parameters.\n",
    "* $X_{ij} \\equiv x^{(i)}_{j}$ for $i=1,\\dots,m$ and $j=0,\\dots,n$. $X$ is called the *design matrix*. It's an $m \\times (n+1)$ dimensional matrix whose rows correspond to the observations. Note that the first column is just a vector of $1$s, since by definition $x^{(i)}_{0} = 1$.\n",
    "* $Y_i \\equiv y^{(i)}$: $m$-dimensional vector created from $y^{(1)}, \\dots, y^{(m)}$. It's sometimes called the *target vector*.\n",
    "\n",
    "Expressing the data in vectorized form will make our life much easier. For instance, $f$ can now be expressed as a dot product:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "We will use the same cost function as we did for simple linear regression, i.e. the normalized sum of squared prediction errors:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2.\n",
    "\\end{align*}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^Tx^{(i)} = \\sum_{j=1}^{m} \\beta_j x^{(i)}_{j} = \\sum_{j=1}^{m} X_{ij}\\beta_j = [X\\beta]_{i},\n",
    "\\end{equation}\n",
    "\n",
    "which in term allows us to express $J$ in matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta) &= \\frac{1}{2m}\\sum_{i=1}^{m} (\\beta^Tx^{(i)} - y^{(i)})^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} ([X\\beta]_{i} - Y_i)^2 \\\\\n",
    "         &= \\frac{1}{2m}\\sum_{i=1}^{m} [X\\beta - Y]_{i}[X\\beta - Y]_{i} \\\\\n",
    "         &= \\frac{1}{2m}[X\\beta - Y]^T[X\\beta - Y].\n",
    "\\end{align*}\n",
    "\n",
    "To find the parameters $\\beta_0,\\beta_1, \\dots, \\beta_n$ that best fit the data set, we will minimize $J$ with respect to each one of them. To do that, first we need the gradient of $J$ with respect to each $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_j}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}(\\beta^Tx^{(i)} - y^{(i)})x^{(i)}_{j} \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m}X_{ij}[X\\theta - Y]_{i} \\\\\n",
    "    &= \\frac{1}{m} [X^T(X\\beta - Y)]_j.\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradients to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J = 0 \\to X^T(X\\hat{\\beta} - Y) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "we find that the optimal parameters $\\hat{\\beta}$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY.\n",
    "\\end{equation}\n",
    "\n",
    "This is called the *normal equation*. Does it reproduce the formulas we found for $\\hat{\\alpha}$ and $\\hat{\\beta}$ [simple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb)? Let's check. First note the mapping between the variables:\n",
    "\n",
    "| Multiple LR     | Simple LR       |\n",
    "|-----------------|-----------------|\n",
    "| $x^{(i)}_1$     | $x^{(i)}$       |\n",
    "| $\\hat{\\beta}_0$ | $\\hat{\\alpha}$  |\n",
    "| $\\hat{\\beta}_1$ | $\\hat{\\beta}$   |\n",
    "\n",
    "Using this, it can be shown after straightforward algebra that (see [this](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb) notebook for undefined quantities below)\n",
    "\\begin{equation}\n",
    "X^TX = m\n",
    "\\begin{pmatrix}\n",
    "    1       & \\bar{x} \\\\\n",
    "    \\bar{x} & S_x^2 + \\bar{x}^2\n",
    "\\end{pmatrix}, \\qquad\n",
    "X^TY = m\n",
    "\\begin{pmatrix}\n",
    "    \\bar{y} \\\\\n",
    "    C_{xy} + \\bar{x}\\bar{y}.\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The inverse of $X^TX$ is given by\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{mS_x^2}\n",
    "\\begin{pmatrix}\n",
    "    S_x^2 + \\bar{x}^2       & -\\bar{x} \\\\\n",
    "    -\\bar{x} & 1\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "using which we can compute $\\hat{\\beta}$: \n",
    "\\begin{equation}\n",
    "\\hat{\\beta}\n",
    "    = (X^TX)^{-1}X^TY\n",
    "    = \\frac{1}{S_x^2}\n",
    "        \\begin{pmatrix} S_x^2 + \\bar{x}^2 & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{pmatrix}\n",
    "        \\begin{pmatrix} \\bar{y} \\\\ C_{xy} + \\bar{x}\\bar{y} \\end{pmatrix}\n",
    "    = \\frac{1}{S_x^2}\n",
    "    \\begin{pmatrix} \\bar{y}S_x^2 -\\bar{x}C_{xy} \\\\ C_{xy} \\end{pmatrix}\n",
    "    = \\begin{pmatrix} \\bar{y} -\\bar{x}\\frac{C_{xy}}{S_x^2} \\\\ \\frac{C_{xy}}{S_x^2} \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "That's exactly what we had derived previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with `statsmodels`\n",
    "\n",
    "Below we will create a fake dataset, run linear regression on it using `statsmodels`, and try to reproduce the various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\siaas\\.virtualenvs\\ml\\lib\\site-packages\\pandas\\__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected zd from C header, got zd from PyObject\n",
      "  from pandas._libs import (hashtable as _hashtable,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels import regression\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data using the model $f(x)=0.3 + 2x_1 + 5 x_2$, and by adding Gaussian noise with $\\sigma=0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "x1 = np.random.uniform(size=m)\n",
    "x2 = np.random.uniform(size=m)\n",
    "y = np.random.normal(loc=0.3 + 2.0*x1 + 5.0*x2, scale=0.5, size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data using `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   474.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 03 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>8.31e-51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:31:04</td>     <th>  Log-Likelihood:    </th> <td> -59.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   124.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   132.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.5760</td> <td>    0.129</td> <td>    4.469</td> <td> 0.000</td> <td>    0.320</td> <td>    0.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.7903</td> <td>    0.166</td> <td>   10.814</td> <td> 0.000</td> <td>    1.462</td> <td>    2.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.6690</td> <td>    0.157</td> <td>   29.795</td> <td> 0.000</td> <td>    4.358</td> <td>    4.980</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.657</td> <th>  Durbin-Watson:     </th> <td>   1.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.437</td> <th>  Jarque-Bera (JB):  </th> <td>   1.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.037</td> <th>  Prob(JB):          </th> <td>   0.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.448</td> <th>  Cond. No.          </th> <td>    5.85</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.907\n",
       "Model:                            OLS   Adj. R-squared:                  0.905\n",
       "Method:                 Least Squares   F-statistic:                     474.3\n",
       "Date:                Sun, 03 Mar 2019   Prob (F-statistic):           8.31e-51\n",
       "Time:                        15:31:04   Log-Likelihood:                -59.294\n",
       "No. Observations:                 100   AIC:                             124.6\n",
       "Df Residuals:                      97   BIC:                             132.4\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.5760      0.129      4.469      0.000       0.320       0.832\n",
       "x1             1.7903      0.166     10.814      0.000       1.462       2.119\n",
       "x2             4.6690      0.157     29.795      0.000       4.358       4.980\n",
       "==============================================================================\n",
       "Omnibus:                        1.657   Durbin-Watson:                   1.939\n",
       "Prob(Omnibus):                  0.437   Jarque-Bera (JB):                1.291\n",
       "Skew:                          -0.037   Prob(JB):                        0.524\n",
       "Kurtosis:                       2.448   Cond. No.                         5.85\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack([np.ones(m), x1, x2])\n",
    "model = regression.linear_model.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal parameters\n",
    "\n",
    "Let's compare the normal equation to the output of `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels Beta_0 = 0.575953682309\n",
      "Reproduced  Beta_0 = 0.575953682309\n",
      "\n",
      "statsmodels Beta_1 = 1.7902644927\n",
      "Reproduced  Beta_1 = 1.7902644927\n",
      "\n",
      "statsmodels Beta_2 = 4.66903258592\n",
      "Reproduced  Beta_2 = 4.66903258592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "for i in range(len(beta)):\n",
    "    print(\"statsmodels Beta_{0} = {1}\".format(i, model.params[i]))\n",
    "    print(\"Reproduced  Beta_{0} = {1}\".format(i, beta[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared\n",
    "\n",
    "We introduced $R^2$ [here](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb). It's defined by\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $f(x)$ is any model of the data and\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = f(x^{(i)}), \\qquad\n",
    "\\bar{y} = \\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "In the case of multiple linear regression:\n",
    "\n",
    "\\begin{equation}\n",
    "f^{(i)} = \\hat{\\beta}^Tx^{(i)} = [X\\hat{\\beta}]_i.\n",
    "\\end{equation}\n",
    "\n",
    "If we let $\\hat{f}$ define the vector of all preditions on input data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} \\equiv \\begin{pmatrix}f^{(1)} \\\\ f^{(2)} \\\\ \\vdots \\\\ f^{(m)} \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} = X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "Let $e$ denote the vector of all prediction errors on input data\n",
    "\n",
    "\\begin{equation}\n",
    "e^{(i)} = y^{(i)} - f^{(i)}, \\qquad\n",
    "\\hat{e} \\equiv \\begin{pmatrix}e^{(1)} \\\\ e^{(2)} \\\\ \\vdots \\\\ e^{(m)} \\end{pmatrix} = Y - \\hat{f} = Y - X\\hat{\\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "We're ready to compute $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels R-squared = 0.907229595044\n",
      "Reproduced  R-squared = 0.907229595044\n"
     ]
    }
   ],
   "source": [
    "ymean = y.mean()\n",
    "f = X.dot(beta)\n",
    "e = f - y\n",
    "rsquared = 1 - np.dot(e, e) / np.dot(y - ymean, y - ymean)\n",
    "print(\"statsmodels R-squared = {}\".format(model.rsquared))\n",
    "print(\"Reproduced  R-squared = {}\".format(rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like simple linear regression, $R^2$ is equal to the square of the sample correlation between $f$ and $y$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9072295950436563"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(f, y)[0, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generalize the proof we had to multiple variables. Remember that minimizing the cost function gave us the following equation: $X^T(X\\hat{\\beta} - Y)=0$, which can also be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "X^T\\hat{e} = 0.\n",
    "\\end{equation}.\n",
    "\n",
    "Looking at the first component of this equation:\n",
    "\n",
    "\\begin{equation}\n",
    "[X^T\\hat{e}]_0 = \\sum_{i=1}^{m}X_{i0}e^{(i)} = \\sum_{i=1}^{m}x^{(i)}_0e^{(i)} = \\sum_{i=1}^{m}e^{(i)} = 0,\n",
    "\\end{equation}.\n",
    "\n",
    "we see that the sum of prediction errors is zero (recall that by definition $x^{(i)}_0=1$). Since $e^{(i)}=y^{(i)} - f^{(i)}$, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}y^{(i)} = \\sum_{i=1}^{m} f^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\bar{y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{f} = \\frac{1}{m} \\sum_{i=1}^{m} f^{(i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, if we apply $\\hat{\\beta}$ to both sides of $X^T\\hat{e} = 0$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^TX^T\\hat{e} = (X\\hat{\\beta})^T\\hat{e} = \\hat{f}^T\\hat{e} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Alright, on to computing $R^2$. Let's start with the denominator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2\n",
    "    &= \\sum_{i=1}^{m}[(f^{(i)} - \\bar{y}) + (y^{(i)} - f^{(i)})]^2 \\\\\n",
    "    &= \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})^2 + \\sum_{i=1}^{m}(y^{(i)} - f^{(i)})^2 + 2\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})(y^{(i)} - f^{(i)}).\n",
    "\\end{align*}\n",
    "\n",
    "The last term turns out to be zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(f^{(i)} - \\bar{y})(y^{(i)} - f^{(i)})\n",
    "    = \\sum_{i=1}^{m}(f^{(i)} - \\bar{y})e^{(i)}\n",
    "    = \\sum_{i=1}^{m}f^{(i)}e^{(i)} - \\bar{y}\\sum_{i=1}^{m}e^{(i)}\n",
    "    = 0,\n",
    "\\end{equation}\n",
    "\n",
    "where the last equality follow from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$. Then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{m}(y^{(i)} - \\bar{y})^2 = \\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2 + \\sum_{i=1}^{m}(y^{(i)} - f^{(i)})^2,\n",
    "\\end{equation}\n",
    "\n",
    "where we've used $\\bar{y}=\\bar{f}$. Plugging this back into the definition of $R^2$, we find:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2 - \\sum_{i=1}^m(y^{(i)} - f^{(i)})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}\n",
    "    = \\frac{\\sum_{i=1}^{m}(f^{(i)} - \\bar{f})^2}{\\sum_{i=1}^m(y^{(i)} - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The correlation $r_{fy}$ between $f$ and $y$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy} = \\frac{C_{fy}}{S_fS_y},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "S_y = \\frac{1}{m}\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2, \\qquad\n",
    "S_f = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})^2, \\qquad\n",
    "C_{fy} = \\frac{1}{m}\\sum_{i=1}^m (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}).\n",
    "\\end{equation}\n",
    "\n",
    "Given these definitions, we can also rewrite $R^2$ more simply as\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{S_f^2}{S_y^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The covariance between $f$ and $y$ can be simplified:\n",
    "\n",
    "\\begin{align*}\n",
    "C_{fy}\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(y^{(i)} - \\bar{y}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})(e^{(i)} + f^{(i)} - \\bar{f}) \\\\\n",
    "    &= \\frac{1}{m}\\sum_{i=1}^{m} (f^{(i)} - \\bar{f})^2 + \\frac{1}{m}\\sum_{i=1}^{m} f^{(i)}e^{(i)} - \\frac{\\bar{f}}{m}\\sum_{i=1}^{m}e^{(i)} \\\\\n",
    "    &= S_f^2,\n",
    "\\end{align*}\n",
    "\n",
    "where the second line uses the definition of $e^{(i)}$ and the fact that $\\bar{y}=\\bar{f}$, and the last equality follows from $\\sum_{i=1}^{m}e^{(i)}=0$ and $\\hat{f}^T\\hat{e}=0$.  Plugging this back in $r_{fy}$:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{fy}^2 = \\frac{C_{fy}^2}{S_f^2S_y^2} = \\frac{S_f^4}{S_f^2S_y^2} = \\frac{S_f^2}{S_y^2} = R^2.\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
