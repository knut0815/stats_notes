{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression assumptions\n",
    "\n",
    "In this notebook, we will review the assumptions underlying our analysis of [linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/multi_linreg.ipynb), and what happens when those assumptions are invalid. What we've covered so far goes under the umbrella of Ordinary Least Squares (OLS).\n",
    "\n",
    "### Prerequisites\n",
    "* [Simple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/linreg.ipynb)\n",
    "* [Multiple linear regression](https://nbviewer.jupyter.org/github/siavashaslanbeigi/stats_notes/blob/master/multi_linreg.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-normal noise\n",
    "\n",
    "### BLUE\n",
    "\n",
    "We derived the OLS estimates $\\hat{\\beta}_j$ in two ways: (i) minimizing a cost function which is constructed from the sum of squared prediction errors, and (ii) maximizing the probability of observing the data set under the following assumptions: $y^{(i)}$ are normally distributed as $\\mathcal{N}(\\beta^T{x^{(i)}}, \\sigma^2)$ and independent for all $i \\neq j$.\n",
    "\n",
    "In the first method, we never spoke of the normal distribution, but came up with the cost function heuristically. In the second method, we did assume errors are normally distributed. So what's the deal? Are the OLS estimates $\\hat{\\beta}_j$ only valid if the residuals are normally distributed? To answer this question, consider the following model:\n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} = \\beta^Tx^{(i)} + \\epsilon^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon^{(1)}, \\dots, \\epsilon^{(m)}$ are *any* random variables satisfying:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\epsilon^{(i)} \\rangle = 0, \\qquad\n",
    "\\langle \\epsilon^{(i)}\\epsilon^{(j)} \\rangle = \\sigma^2\\delta_{ij}.\n",
    "\\end{equation}\n",
    "\n",
    "Given that $\\epsilon^{(i)}$ could be non-normal, are the OLS estimates still unbiased? Recall that the OLS esimtaes $\\hat{\\beta}$ are given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}=(X^TX)^{-1}X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "Let's compute the expectation value of $\\hat{\\beta}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\hat{\\beta} \\rangle\n",
    "    = (X^TX)^{-1}X^T \\langle Y \\rangle\n",
    "    = (X^TX)^{-1}X^T (X\\beta + \\langle \\epsilon \\rangle)\n",
    "    = (X^TX)^{-1}X^TX\\beta\n",
    "    = \\beta,\n",
    "\\end{equation}\n",
    "\n",
    "where we've introduced the vectors\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle Y \\rangle\n",
    "    \\equiv\n",
    "    \\begin{pmatrix}\n",
    "        \\langle y^{(1)} \\rangle \\\\\n",
    "        \\langle y^{(2)} \\rangle \\\\\n",
    "        \\vdots \\\\\n",
    "        \\langle y^{(m)} \\rangle\n",
    "    \\end{pmatrix} =\n",
    "    \\begin{pmatrix}\n",
    "        \\beta^Tx^{(1)} \\\\\n",
    "        \\beta^Tx^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta^Tx^{(m)}\n",
    "    \\end{pmatrix}\n",
    "    = X\\beta, \\qquad\n",
    "\\langle \\epsilon \\rangle\n",
    "    \\equiv\n",
    "    \\begin{pmatrix}\n",
    "        \\langle \\epsilon^{(1)} \\rangle \\\\\n",
    "        \\langle \\epsilon^{(2)} \\rangle \\\\\n",
    "        \\vdots \\\\\n",
    "        \\langle \\epsilon^{(m)} \\rangle\n",
    "    \\end{pmatrix} =\n",
    "    \\begin{pmatrix}\n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        \\vdots \\\\\n",
    "        0\n",
    "    \\end{pmatrix}\n",
    "    = 0\n",
    "\\end{equation}\n",
    "\n",
    "Nice! All we need for $\\hat{\\beta}$ to be unbiased is $\\langle \\epsilon \\rangle = 0$.\n",
    "\n",
    "What about the variance of $\\hat{\\beta}$? Could it be that there is some other linear estimator that is unbiased but has lower variance? Consider some other linear estimator $\\tilde{\\beta}$. It needs to be linear in $Y$, so we can express it as\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\beta} = CY, \\qquad\n",
    "C \\equiv [(X^TX)^{-1}X^T + D],\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ is any (for now) $(n+1)\\times m$ dimensional matrix. We want $\\tilde{\\beta}$ to be unbiased:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\tilde{\\beta} \\rangle\n",
    "    = [(X^TX)^{-1}X^T + D] \\langle Y \\rangle\n",
    "    = [(X^TX)^{-1}X^T + D] X\\beta\n",
    "    = [I_{n+1} + DX]\\beta,\n",
    "\\end{equation}\n",
    "\n",
    "so we must have that\n",
    "\n",
    "\\begin{equation}\n",
    "DX = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Let's compute the variance of $\\tilde{\\beta}_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "Var(\\tilde{\\beta}_j)\n",
    "    &= \\left\\langle \\left(\\tilde{\\beta}_j - \\langle \\tilde{\\beta}_j \\rangle\\right)^2 \\right\\rangle \\\\\n",
    "    &= \\sum_{i,i'=1}^{m}C_{ji}C_{ji'}\\left\\langle(y^{(i)} - \\langle y^{(i)} \\rangle)(y^{(i')} - \\langle y^{(i')} \\rangle)\\right\\rangle \\\\\n",
    "    &= \\sum_{i,i'=1}^{m}C_{ji}C_{ji'}\\langle \\epsilon^{(i)}\\epsilon^{(i')} \\rangle \\\\\n",
    "    &= \\sum_{i,i'=1}^{m}C_{ji}C_{ji'}\\sigma^2\\delta_{ii'} \\\\\n",
    "    &= \\sigma^2\\sum_{i=1}^{m}C_{ji}C_{ji} \\\\\n",
    "    &= \\sigma^2[CC^T]_{jj}.\n",
    "\\end{align*}\n",
    "\n",
    "We need to compute $CC^T$:\n",
    "\n",
    "\\begin{equation}\n",
    "CC^T\n",
    "    = [(X^TX)^{-1}X^T + D][X(X^TX)^{-1} + D^T]\n",
    "    = (X^TX)^{-1} + (X^TX)^{-1}(DX)^T + (DX)(X^TX)^{-1} + DD^T\n",
    "    = (X^TX)^{-1} + DD^T,\n",
    "\\end{equation}\n",
    "\n",
    "where the last equality follows from $DX=0$. Finally:\n",
    "\n",
    "\\begin{align*}\n",
    "Var(\\tilde{\\beta}_j)\n",
    "    &= \\sigma^2[CC^T]_{jj} \\\\\n",
    "    &= \\sigma^2[(X^TX)^{-1}]_{jj} + \\sigma^2[DD^T]_{jj} \\\\\n",
    "    &= Var(\\hat{\\beta}_j) + \\sigma^2[DD^T]_{jj} \\\\\n",
    "    &\\ge Var(\\hat{\\beta}_j).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the OLS estimator is the Best Linear Unbiased Estimator (BLUE), where best is in the sense of having the lowest variance. This remarkable result goes by the name of [Gaussâ€“Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem).\n",
    "\n",
    "Let's remember what we assumed: (i) errors have zero mean, (ii) errors are uncorrelated, (iii) errors have the same finite variance. If these assumptions are true, the OLS estimator is the best in town. Note that we don't even have to assume *independence*; the errors just have to be uncorrelated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
